{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF From Scratch - Module Notebook\n",
        "\n",
        "This notebook contains the complete implementation of TF-IDF from scratch and related utilities as used in the LearnMateAI project. You can run cells interactively to see and test the logic.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports used in TF-IDF from scratch\n",
        "import re\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    PLOTTING_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PLOTTING_AVAILABLE = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TFIDFVectorizer Implementation\n",
        "This class builds vocabulary, computes document frequency, IDF, and creates TF-IDF vectors from scratch. No scikit-learn TF-IDF vectorizer is used here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TFIDFVectorizer:\n",
        "    \"\"\"\n",
        "    TF-IDF Vectorizer implemented from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, max_features: int = None, min_df: int = 1, max_df: float = 1.0, \n",
        "                 stop_words: Set[str] = None, lowercase: bool = True):\n",
        "        self.max_features = max_features\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.stop_words = stop_words or set()\n",
        "        self.lowercase = lowercase\n",
        "        self.vocabulary_ = {}\n",
        "        self.idf_ = {}\n",
        "        self.feature_names_ = []\n",
        "        self.n_documents_ = 0\n",
        "    \n",
        "    def _preprocess_text(self, text: str) -> List[str]:\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens\n",
        "    \n",
        "    def _build_vocabulary(self, documents: List[str]) -> Dict[str, int]:\n",
        "        doc_freq = defaultdict(int)\n",
        "        all_terms = []\n",
        "        for doc in documents:\n",
        "            tokens = self._preprocess_text(doc)\n",
        "            unique_tokens = set(tokens)\n",
        "            for token in unique_tokens:\n",
        "                doc_freq[token] += 1\n",
        "            all_terms.extend(tokens)\n",
        "        self.n_documents_ = len(documents)\n",
        "        min_doc_freq = self.min_df if isinstance(self.min_df, int) else int(self.min_df * self.n_documents_)\n",
        "        max_doc_freq = self.max_df if isinstance(self.max_df, int) else int(self.max_df * self.n_documents_)\n",
        "        filtered_vocab = {}\n",
        "        for term, df in doc_freq.items():\n",
        "            if min_doc_freq <= df <= max_doc_freq:\n",
        "                filtered_vocab[term] = df\n",
        "        sorted_vocab = sorted(filtered_vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "        if self.max_features:\n",
        "            sorted_vocab = sorted_vocab[:self.max_features]\n",
        "        vocabulary = {term: idx for idx, (term, _) in enumerate(sorted_vocab)}\n",
        "        self.feature_names_ = [term for term, _ in sorted_vocab]\n",
        "        return vocabulary\n",
        "    \n",
        "    def _calculate_idf(self, documents: List[str]) -> Dict[str, float]:\n",
        "        doc_freq = defaultdict(int)\n",
        "        for doc in documents:\n",
        "            tokens = self._preprocess_text(doc)\n",
        "            unique_tokens = set(tokens)\n",
        "            for token in unique_tokens:\n",
        "                if token in self.vocabulary_:\n",
        "                    doc_freq[token] += 1\n",
        "        idf = {}\n",
        "        for term in self.vocabulary_:\n",
        "            df = doc_freq.get(term, 1)\n",
        "            idf[term] = math.log(self.n_documents_ / (df + 1))\n",
        "        return idf\n",
        "    \n",
        "    def fit(self, documents: List[str]):\n",
        "        self.vocabulary_ = self._build_vocabulary(documents)\n",
        "        self.idf_ = self._calculate_idf(documents)\n",
        "        return self\n",
        "    \n",
        "    def _calculate_tf(self, tokens: List[str]) -> Dict[str, float]:\n",
        "        term_counts = Counter(tokens)\n",
        "        total_terms = len(tokens)\n",
        "        if total_terms == 0:\n",
        "            return {}\n",
        "        tf = {}\n",
        "        for term, count in term_counts.items():\n",
        "            if term in self.vocabulary_:\n",
        "                tf[term] = count / total_terms\n",
        "        return tf\n",
        "    \n",
        "    def transform(self, documents: List[str]) -> np.ndarray:\n",
        "        vectors = []\n",
        "        for idx, doc in enumerate(documents):\n",
        "            tokens = self._preprocess_text(doc)\n",
        "            tf = self._calculate_tf(tokens)\n",
        "            vector = np.zeros(len(self.vocabulary_))\n",
        "            for term, tf_val in tf.items():\n",
        "                term_idx = self.vocabulary_[term]\n",
        "                idf_val = self.idf_[term]\n",
        "                vector[term_idx] = tf_val * idf_val\n",
        "            vectors.append(vector)\n",
        "        return np.array(vectors)\n",
        "    \n",
        "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Using TFIDFVectorizer\n",
        "Now let's use the above class on a sample set of documents and view the resulting TF-IDF vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample test of TFIDFVectorizer\n",
        "sample_docs = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\",\n",
        "    \"Artificial intelligence and machine learning are related fields.\"\n",
        "]\n",
        "\n",
        "vectorizer = TFIDFVectorizer()\n",
        "vectorizer.fit(sample_docs)\n",
        "tfidf_matrix = vectorizer.transform(sample_docs)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Continue copying important classes/methods from your module in this notebook. You can now run and demonstrate the TF-IDF algorithm step-by-step. If you want the full notebook for utils.py or additional classes, let me know!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes Text Classifier\n",
        "This class implements a simple Naive Bayes classifier for text classification using custom TF-IDF vectors as input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    \"\"\"\n",
        "    Naive Bayes Classifier for text classification using TF-IDF features\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha: float = 1.0):\n",
        "        self.alpha = alpha\n",
        "        self.classes_ = None\n",
        "        self.class_priors_ = {}\n",
        "        self.feature_probs_ = {}  # {class: {feature_idx: probability}}\n",
        "        self.n_features_ = 0\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_features_ = X.shape[1]\n",
        "        n_samples = X.shape[0]\n",
        "        for class_label in self.classes_:\n",
        "            class_mask = y == class_label\n",
        "            self.class_priors_[class_label] = np.sum(class_mask) / n_samples\n",
        "        self.feature_probs_ = {class_label: {} for class_label in self.classes_}\n",
        "        for epoch in range(epochs):\n",
        "            for class_label in self.classes_:\n",
        "                class_mask = y == class_label\n",
        "                class_samples = X[class_mask]\n",
        "                feature_sums = np.sum(class_samples, axis=0)\n",
        "                total_sum = np.sum(feature_sums) + self.alpha * self.n_features_\n",
        "                for feature_idx in range(self.n_features_):\n",
        "                    feature_sum = feature_sums[feature_idx]\n",
        "                    prob = (feature_sum + self.alpha) / total_sum\n",
        "                    self.feature_probs_[class_label][feature_idx] = prob\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes_)\n",
        "        probabilities = np.zeros((n_samples, n_classes))\n",
        "        for i, sample in enumerate(X):\n",
        "            class_probs = []\n",
        "            for class_label in self.classes_:\n",
        "                log_prob = math.log(self.class_priors_[class_label])\n",
        "                for feature_idx in range(self.n_features_):\n",
        "                    feature_value = sample[feature_idx]\n",
        "                    if feature_value > 0:\n",
        "                        feature_prob = self.feature_probs_[class_label].get(feature_idx, self.alpha)\n",
        "                        log_prob += feature_value * math.log(feature_prob + 1e-10)\n",
        "                class_probs.append(log_prob)\n",
        "            class_probs = np.array(class_probs)\n",
        "            class_probs = class_probs - np.max(class_probs)\n",
        "            class_probs = np.exp(class_probs)\n",
        "            probabilities[i] = class_probs / np.sum(class_probs)\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probabilities = self.predict_proba(X)\n",
        "        predictions = self.classes_[np.argmax(probabilities, axis=1)]\n",
        "        return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the Naive Bayes Classifier with TF-IDF\n",
        "Below is an example of how to use the Naive Bayes classifier on a toy dataset with TF-IDF features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents and labels\n",
        "sample_docs = [\n",
        "    \"Python is a powerful programming language.\",\n",
        "    \"Machine learning uses algorithms for data analysis.\",\n",
        "    \"Artificial intelligence and data science are closely related.\",\n",
        "    \"Programming in Python is fun.\",\n",
        "    \"Data analysis is a key part of data science.\"\n",
        "]\n",
        "sample_labels = [\"Programming\", \"ML\", \"ML\", \"Programming\", \"ML\"]\n",
        "\n",
        "vectorizer = TFIDFVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(sample_docs)\n",
        "y = np.array(sample_labels)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.4, random_state=42, stratify=y)\n",
        "\n",
        "# Training Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "train_pred = classifier.predict(X_train)\n",
        "test_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"TRAINING PREDICTION:\", train_pred)\n",
        "print(\"TEST PREDICTION:\", test_pred)\n",
        "print(\"TRAIN ACCURACY:\", accuracy_score(y_train, train_pred))\n",
        "print(\"TEST ACCURACY:\", accuracy_score(y_test, test_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions: Data Analysis and Confusion Matrix\n",
        "These utilities help analyze datasets and visualize results, useful for both demo and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_data(documents, labels):\n",
        "    \"\"\"Analyzes class distribution and document lengths.\"\"\"\n",
        "    from collections import Counter\n",
        "    print(\"Class distribution:\")\n",
        "    print(Counter(labels))\n",
        "    doc_lens = [len(d.split()) for d in documents]\n",
        "    print(\"Avg words per doc:\", np.mean(doc_lens))\n",
        "    print(\"Min:\", np.min(doc_lens), \"Max:\", np.max(doc_lens))\n",
        "    print(\"Stddev:\", np.std(doc_lens))\n",
        "    print(\"Vocabulary size:\", len(set(' '.join(documents).split())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import matplotlib.pyplot as plt\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function: Default English Stopwords\n",
        "You can optionally use this for stopword removal in your vectorizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_default_stopwords():\n",
        "    return {\n",
        "        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
        "        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
        "        'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',\n",
        "        'have', 'had', 'what', 'said', 'each', 'which', 'their', 'if',\n",
        "        'up', 'out', 'many', 'then', 'them', 'these', 'so', 'some', 'her',\n",
        "        'would', 'make', 'like', 'into', 'him', 'has', 'two', 'more',\n",
        "        'very', 'after', 'words', 'long', 'than', 'first', 'been', 'call',\n",
        "        'who', 'oil', 'sit', 'now', 'find', 'down', 'day', 'did', 'get',\n",
        "        'come', 'made', 'may', 'part'}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
