text
"We compute spin-orbit effects in the equations of motion, binding energy and energy loss of binary systems of compact objects at the next-to-leading order in the post-Newtonian (PN) approximation in the effective field theory (EFT) framework. We then use these quantities to compute the evolution of the orbital frequency and accumulated orbital phase including spin-orbit effects beyond the dominant order. To obtain the results presented in this paper, we make use of known ingredients in the EFT literature, such as the potential and the multipole moments with spin effects at next-to-leading order, and which are given in the linearized harmonic gauge and with the spins in the locally flat frame. We also obtain the correction to the center-of-mass frame caused by spin-orbit effects at next-to-leading order. We demonstrate the equivalence between our EFT results and those which were obtained elsewhere using different formalisms. The results presented in this paper provide us with the final ingredients for the construction of theoretical templates for gravitational waves including next-to-leading order spin-orbit effects, which will be presented in a future publication."
"In this study we tried to emphasize the role parton shower plays in event generation, and in the physics of high energy event generation. We achieved this task by comparing the next-to-leading order and next-to-next-to-leading order results from EERAD3 with the next-to-leading order plus parton shower results from Vincia, and comparing both with real data."
"Universal quantum computation encoded over continuous variables can be achieved via Gaussian measurements acting on entangled non-Gaussian states. However, due to the weakness of available nonlinearities, generally these states can only be prepared conditionally, potentially with low probability. Here we show how universal quantum computation could be implemented unconditionally using an integrated platform able to sustain both linear and quadratic optomechanical-like interactions. Specifically, considering cavity opto- and electro-mechanical systems, we propose a realisation of a driven-dissipative dynamics that deterministically prepares the required non-Gaussian cluster states --- entangled squeezed states of multiple mechanical oscillators suitably interspersed with cubic-phase states. We next demonstrate how arbitrary Gaussian measurements on the cluster nodes can be performed by continuously monitoring the output cavity field. Finally, the feasibility requirements of this approach are analysed in detail, suggesting that its building blocks are within reach of current technology."
"Novel-View Human Action Synthesis aims to synthesize the movement of a body from a virtual viewpoint, given a video from a real viewpoint. We present a novel 3D reasoning to synthesize the target viewpoint. We first estimate the 3D mesh of the target body and transfer the rough textures from the 2D images to the mesh. As this transfer may generate sparse textures on the mesh due to frame resolution or occlusions. We produce a semi-dense textured mesh by propagating the transferred textures both locally, within local geodesic neighborhoods, and globally, across symmetric semantic parts. Next, we introduce a context-based generator to learn how to correct and complete the residual appearance information. This allows the network to independently focus on learning the foreground and background synthesis tasks. We validate the proposed solution on the public NTU RGB+D dataset. The code and resources are available at https://bit.ly/36u3h4K."
"We introduce a learning-based approach for room navigation using semantic maps. Our proposed architecture learns to predict top-down belief maps of regions that lie beyond the agent's field of view while modeling architectural and stylistic regularities in houses. First, we train a model to generate amodal semantic top-down maps indicating beliefs of location, size, and shape of rooms by learning the underlying architectural patterns in houses. Next, we use these maps to predict a point that lies in the target room and train a policy to navigate to the point. We empirically demonstrate that by predicting semantic maps, the model learns common correlations found in houses and generalizes to novel environments. We also demonstrate that reducing the task of room navigation to point navigation improves the performance further."
"The multi-source data generated by distributed systems, provide a holistic description of the system. Harnessing the joint distribution of the different modalities by a learning model can be beneficial for critical applications for maintenance of the distributed systems. One such important task is the task of anomaly detection where we are interested in detecting the deviation of the current behaviour of the system from the theoretically expected. In this work, we utilize the joint representation from the distributed traces and system log data for the task of anomaly detection in distributed systems. We demonstrate that the joint utilization of traces and logs produced better results compared to the single modality anomaly detection methods. Furthermore, we formalize a learning task - next template prediction NTP, that is used as a generalization for anomaly detection for both logs and distributed trace. Finally, we demonstrate that this formalization allows for the learning of template embedding for both the traces and logs. The joint embeddings can be reused in other applications as good initialization for spans and logs."
"This paper reports on the derivation and implementation of a shape optimization procedure for the minimization of hemolysis induction in biomedical devices. Hemolysis is a blood damaging phenomenon that may occur in mechanical blood-processing applications where large velocity gradients are found. An increased level of damaged blood can lead to deterioration of the immune system and quality of life. It is, thus, important to minimize flow-induced hemolysis by improving the design of next-generation biomedical machinery. Emphasis is given to the formulation of a continuous adjoint complement to a power-law hemolysis prediction model dedicated to efficiently identifying the shape sensitivity to hemolysis. The computational approach is verified against the analytical solutions of a benchmark problem and computed sensitivity derivatives are validated by a finite differences study on a generic 2D stenosed geometry. The application included addresses a 3D ducted geometry which features typical characteristics of biomedical devices. An optimized shape, leading to a potential improvement in hemolysis induction up to 22%, is identified. It is shown, that the improvement persists for different, literature-reported hemolysis-evaluation parameters."
"Since myocardial fibers drive the electric signal propagation throughout the myocardium, accurately modeling their arrangement is essential for simulating heart electrophysiology (EP). Rule-Based-Methods (RBMs) represent a commonly used strategy to include cardiac fibers in computational models. A particular class of such methods is known as Laplace-Dirichlet-Rule-Based-Methods (LDRBMs) since they rely on the solution of Laplace problems. In this work we provide a unified framework, based on LDRBMs, for generating full heart muscle fibers. First, we review existing ventricular LDRBMs providing a communal mathematical description and introducing also some modeling improvements with respect to the existing literature. We then carry out a systematic comparison of LDRBMs based on meaningful biomarkers produced by numerical EP simulations. Next we propose, for the first time, a LDRBM to be used for generating atrial fibers. The new method, tested both on idealized and realistic atrial models, can be applied to any arbitrary geometries. Finally, we present numerical results obtained in a realistic whole heart where fibers are included for all the four chambers using the discussed LDRBMs."
"At the heart of the standard deep learning training loop is a greedy gradient step minimizing a given loss. We propose to add a second step to maximize training generalization. To do this, we optimize the loss of the next training step. While computing the gradient for this generally is very expensive and many interesting applications consider non-differentiable parameters (e.g. due to hard samples), we present a cheap-to-compute and memory-saving reward, the gradient-alignment reward (GAR), that can guide the optimization. We use this reward to optimize multiple distributions during model training. First, we present the application of GAR to choosing the data distribution as a mixture of multiple dataset splits in a small scale setting. Second, we show that it can successfully guide learning augmentation strategies competitive with state-of-the-art augmentation strategies on CIFAR-10 and CIFAR-100."
"Semi-Markov model is one of the most general models for stochastic dynamic systems. This paper deals with a two-person zero-sum game for semi-Markov processes. We focus on the expected discounted payoff criterion with state-action-dependent discount factors. The state and action spaces are both Polish spaces, and the payoff function is $\omega$-bounded. We first construct a fairly general model of semi-Markov games under a given semi-Markov kernel and a pair of strategies. Next, based on the standard regularity condition and the continuity-compactness condition for semi-Markov games, we derive a ""drift condition"" on the semi-Markov kernel and suppose that the discount factors have a positive lower bound, under which the existence of the value function and a pair of optimal stationary strategies of our semi-Markov game are proved by using the Shapley equation. Moreover, when the state and action spaces are both finite, a value iteration-type algorithm for computing the value function and $\varepsilon$-Nash equilibrium of the game is developed. The convergence of the algorithm is also proved. Finally, we conduct numerical examples to demonstrate our main results."
"Removing undesired reflections from a photo taken in front of glass is of great importance for enhancing visual computing systems' efficiency. Previous learning-based approaches have produced visually plausible results for some reflections type, however, failed to generalize against other reflection types. There is a dearth of literature for efficient methods concerning single image reflection removal, which can generalize well in large-scale reflection types. In this study, we proposed an iterative gradient encoding network for single image reflection removal. Next, to further supervise the network in learning the correlation between the transmission layer features, we proposed a feature co-occurrence loss. Extensive experiments on the public benchmark dataset of SIR$^2$ demonstrated that our method can remove reflection favorably against the existing state-of-the-art method on all imaging settings, including diverse backgrounds. Moreover, as the reflection strength increases, our method can still remove reflection even where other state of the art methods failed."
"In this paper we propose a method to compute a freeform reflector system for collimating and shaping a beam from a point source. We construct these reflectors such that the radiant intensity of the source is converted into a desired target. An important generalization in our approach compared to previous research is that the output beam can be in an arbitrary direction. The design problem is approached by using a generalized Monge-Amp\`ere equation. This equation is solved using a least-squares algorithm for non-quadratic cost functions. This algorithm calculates the optical map, from which we can then compute the surfaces. We test our algorithm on two cases. First we consider a uniform source and target distribution. Next, we use the model of a laser diode light source and a ring-shaped target distribution."
"As progressive densification of cells, deployment of Cloud-RAN and \ac{MEC} are coming into reality to support the ultra-low latency with high reliability in 5G and beyond, it generates mesh traffic pattern across fronthaul network. This led to evolution of PON architectural enhancements with virtualization in order to support such mesh traffic pattern. However, allocation of virtual PON slices dynamically over such mesh-PON based fronthaul transport is becoming a research challenge. In this paper, we provide a mixed analytical-iterative model to compute optimal virtual PON slice allocation, providing mesh access connectivity with ultra-low end-to-end latency in next-generation MEC-based Cloud-RAN. Our proposed method can compute optimal virtual PON slice allocation in timescales compatible with real-time or near real-time operations."
"This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modelling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future."
"In this paper, we introduce and motivate the studies of Quantum Weyl Gravity (also known as Conformal Gravity). We discuss some appealing features of this theory both on classical and quantum level. The construction of the quantum theory is described in detail to the one-loop level. To facilitate computations we use only physical degrees of freedom, which are singled out through the York decomposition. At the one-loop level we compute the partition function around a general Einstein space. Next, the functional renormalization group of couplings in Quantum Weyl Gravity is investigated. We reproduce completely previous results obtained on maximally symmetric and Ricci-flat backgrounds. Finally, we comment on further directions and on the issue of conformal anomaly."
"We review the detectability of gravitational waves generated by oscillations excited during a phase transition from hadronic matter to deconfined quark-gluon matter in the core of a neutron star. Neutron star properties were computed using a Boguta and Bodmer's based model and the MIT bag model. The maximum energy available to excite mechanical oscillations into the star is estimated by energy difference between the configurations with and without a quark-gluon matter core. On basis of the planned sensitivity of present laser interferometers (VIRGO or LIGO I) and those of the next generation (LIGO II), the maximum volume to be proped by these experiments is determined. These results are used as an indication of the potential detectability of neutron stars as sources of gravitational waves. Our results indicate that the maximum distance probed by the detectors of the first generation is well beyond M31, whereas the second generation detectors will probably see phase transitions events at distances two times longer, but certanly not yet attaining the Virgo cluster."
"Over the last 10-15 years a general understanding of the chemical reaction of protein folding has emerged from statistical mechanics. The lessons learned from protein folding kinetics based on energy landscape ideas have benefited protein structure prediction, in particular the development of coarse grained models. We survey results from blind structure prediction. We explore how second generation prediction energy functions can be developed by introducing information from an ensemble of previously simulated structures. This procedure relies on the assumption of a funnelled energy landscape keeping with the principle of minimal frustration. First generation simulated structures provide an improved input for associative memory energy functions in comparison to the experimental protein structures chosen on the basis of sequence alignment."
"We explain how to generalize Nekrasov's microscopic approach to N=2 gauge theories to the N=1 case, focusing on the typical example of the U(N) theory with one adjoint chiral multiplet X and an arbitrary polynomial tree-level superpotential Tr W(X). We provide a detailed analysis of the generalized glueball operators and a non-perturbative discussion of the Dijkgraaf-Vafa matrix model and of the generalized Konishi anomaly equations. We compute in particular the non-trivial quantum corrections to the Virasoro operators and algebra that generate these equations. We have performed explicit calculations up to two instantons, that involve the next-to-leading order corrections in Nekrasov's Omega-background."
"We compute the fourth order action in perturbation theory for scalar and second order tensor perturbations for a minimally coupled single field inflationary model, where the inflaton's lagrangian is a general function of the field's value and its kinetic energy. We obtain the fourth order action in two gauges, the comoving gauge and the uniform curvature gauge. Using the comoving gauge action we calculate the trispectrum at leading order in slow-roll, finding agreement with a previously known result in the literature. We point out that in general to obtain the correct leading order trispectrum one cannot ignore second order tensor perturbations as previously done by others. The next-to-leading order corrections may become detectable depending on the shape and we provide the necessary formalism to calculate them."
"We compute the Hamiltonian for spherically symmetric scalar field collapse in Einstein-Gauss-Bonnet gravity in D dimensions using slicings that are regular across future horizons. We first reduce the Lagrangian to two dimensions using spherical symmetry. We then show that choosing the spatial coordinate to be a function of the areal radius leads to a relatively simple Hamiltonian constraint whose gravitational part is the gradient of the generalized mass function. Next we complete the gauge fixing such that the metric is the Einstein-Gauss-Bonnet generalization of non-static Painleve-Gullstrand coordinates. Finally, we derive the resultant reduced equations of motion for the scalar field. These equations are suitable for use in numerical simulations of spherically symmetric scalar field collapse in Gauss-Bonnet gravity and can readily be generalized to other matter fields minimally coupled to gravity."
"In the era of the next generation of gravitational wave experiments a stochastic background from cusps of cosmic (super)strings is expected to be probed and, if not detected, to be significantly constrained. A popcorn-like background can be, for part of the parameter space, as pronounced as the (Gaussian) continuous contribution from unresolved sources that overlap in frequency and time. We study both contributions from unresolved cosmic string cusps over a range of frequencies relevant to ground based interferometers, such as LIGO/Virgo second generation (AdLV) and Einstein Telescope (ET) third generation detectors, the space antenna LISA and Pulsar Timing Arrays (PTA). We compute the sensitivity (at $2 \sigma$ level) in the parameter space for AdLV, ET, LISA and PTA. We conclude that the popcorn regime is complementary to the continuous background. Its detection could therefore enhance confidence in a stochastic background detection and possibly help determine fundamental string parameters such as the string tension and the reconnection probability."
"Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary."
"Construction of graphs with equal eigenvalues (co-spectral graphs) is an interesting problem in spectral graph theory. Seidel switching is a well-known method for generating co-spectral graphs. From a matrix theoretic point of view, Seidel switching is a combined action of a number of unitary operations on graphs. Recent works [1] and [2] have shown significant connections between graph and quantum information theories. Corresponding to Laplacian matrices of any graph there are quantum states useful in quantum computing. From this point of view, graph theoretical problems are meaningful in the context of quantum information. This work describes Seidel switching from a quantum perspective. Here, we generalize Seidel switching to weighted directed graphs. We use it to construct graphs with equal Laplacian and signless Laplacian spectra and consider density matrices corresponding to them. Hence Seidel switching is a technique to generate cospectral density matrices. Next, we show that all the unitary operators used in Seidel switching are global unitary operators. Global unitary operators can be used to generate entanglement, a benchmark phenomena in quantum information processing."
"Deep learning approaches to breast cancer detection in mammograms have recently shown promising results. However, such models are constrained by the limited size of publicly available mammography datasets, in large part due to privacy concerns and the high cost of generating expert annotations. Limited dataset size is further exacerbated by substantial class imbalance since ""normal"" images dramatically outnumber those with findings. Given the rapid progress of generative models in synthesizing realistic images, and the known effectiveness of simple data augmentation techniques (e.g. horizontal flipping), we ask if it is possible to synthetically augment mammogram datasets using generative adversarial networks (GANs). We train a class-conditional GAN to perform contextual in-filling, which we then use to synthesize lesions onto healthy screening mammograms. First, we show that GANs are capable of generating high-resolution synthetic mammogram patches. Next, we experimentally evaluate using the augmented dataset to improve breast cancer classification performance. We observe that a ResNet-50 classifier trained with GAN-augmented training data produces a higher AUROC compared to the same model trained only on traditionally augmented data, demonstrating the potential of our approach."
"We present AdaFrame, a framework that adaptively selects relevant frames on a per-input basis for fast video recognition. AdaFrame contains a Long Short-Term Memory network augmented with a global memory that provides context information for searching which frames to use over time. Trained with policy gradient methods, AdaFrame generates a prediction, determines which frame to observe next, and computes the utility, i.e., expected future rewards, of seeing more frames at each time step. At testing time, AdaFrame exploits predicted utilities to achieve adaptive lookahead inference such that the overall computational costs are reduced without incurring a decrease in accuracy. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet. AdaFrame matches the performance of using all frames with only 8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We further qualitatively demonstrate learned frame usage can indicate the difficulty of making classification decisions; easier samples need fewer frames while harder ones require more, both at instance-level within the same class and at class-level among different categories."
"Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of ""abstract meaning"", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance."
"Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster."
"A facility based on a next-generation, high-flux D-D neutron generator has been commissioned and it is now operational at the University of California, Berkeley. The current generator design produces near monoenergetic 2.45 MeV neutrons at outputs of 10^8 n/s. Calculations provided show that future conditioning at higher currents and voltages will allow for a production rate over 10^10 n/s. A significant problem encountered was beam-induced electron backstreaming, that needed to be resolved to achieve meaningful beam currents. Two methods of suppressing secondary electrons resulting from the deuterium beam striking the target were tested: the application of static electric and magnetic fields. Computational simulations of both techniques were done using a finite element analysis in COMSOL Multiphysics. Experimental tests verified these simulation results. The most reliable suppression was achieved via the implementation of an electrostatic shroud with a voltage offset of -800 V relative to the target."
"Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them."
"We propose a new architecture and training methodology for generative adversarial networks. Current approaches attempt to learn the transformation from a noise sample to a generated data sample in one shot. Our proposed generator architecture, called $\textit{ChainGAN}$, uses a two-step process. It first attempts to transform a noise vector into a crude sample, similar to a traditional generator. Next, a chain of networks, called $\textit{editors}$, attempt to sequentially enhance this sample. We train each of these units independently, instead of with end-to-end backpropagation on the entire chain. Our model is robust, efficient, and flexible as we can apply it to various network architectures. We provide rationale for our choices and experimentally evaluate our model, achieving competitive results on several datasets."
"Anatomical landmark segmentation and pathology localization are important steps in automated analysis of medical images. They are particularly challenging when the anatomy or pathology is small, as in retinal images and cardiac MRI, or when the image is of low quality due to device acquisition parameters as in magnetic resonance (MR) scanners. We propose an image super-resolution method using progressive generative adversarial networks (P-GAN) that can take as input a low-resolution image and generate a high resolution image of desired scaling factor. The super resolved images can be used for more accurate detection of landmarks and pathology. Our primary contribution is in proposing a multistage model where the output image quality of one stage is progressively improved in the next stage by using a triplet loss function. The triplet loss enables stepwise image quality improvement by using the output of the previous stage as the baseline. This facilitates generation of super resolved images of high scaling factor while maintaining good image quality. Experimental results for image super-resolution show that our proposed multistage P-GAN outperforms competing methods and baseline GAN."
"Given the collection of timestamped web documents related to the evolving topic, timeline summarization (TS) highlights its most important events in the form of relevant summaries to represent the development of a topic over time. Most of the previous work focuses on fully-observable ranking models and depends on hand-designed features or complex mechanisms that may not generalize well. We present a novel dynamic framework for evolutionary timeline generation leveraging distributed representations, which dynamically finds the most likely sequence of evolutionary summaries in the timeline, called the Viterbi timeline, and reduces the impact of events that irrelevant or repeated to the topic. The assumptions of the coherence and the global view run through our model. We explore adjacent relevance to constrain timeline coherence and make sure the events evolve on the same topic with a global view. Experimental results demonstrate that our framework is feasible to extract summaries for timeline generation, outperforms various competitive baselines, and achieves the state-of-the-art performance as an unsupervised approach."
"We put forward new general criteria to design successor rules that generate binary de Bruijn sequences. Prior fast algorithms based on successor rules in the literature are then shown to be special instances. We implemented the criteria to join the cycles generated by a number of simple feedback shift registers (FSRs) of order $n$. These include the pure cycling register (PCR) and the pure summing register (PSR). For the PCR, we define a transitive relation on its cycles, based on their weights. We also extend the choices of conjugate states by using shift operations. For the PSR, we define three distinct transitive relations on its cycles, namely a run order, a necklace order, and a mixed order. Using the new orders, we propose numerous classes of successor rules. Each class efficiently generates a number, exponential in $n$, of binary de Bruijn sequences. Producing the next bit in each such sequence takes $O(n)$ memory and $O(n)$ time. We implemented computational routines to confirm the claims."
"Scientific image tampering is a problem that affects not only authors but also the general perception of the research community. Although previous researchers have developed methods to identify tampering in natural images, these methods may not thrive under the scientific setting as scientific images have different statistics, format, quality, and intentions. Therefore, we propose a scientific-image specific tampering detection method based on noise inconsistencies, which is capable of learning and generalizing to different fields of science. We train and test our method on a new dataset of manipulated western blot and microscopy imagery, which aims at emulating problematic images in science. The test results show that our method can detect various types of image manipulation in different scenarios robustly, and it outperforms existing general-purpose image tampering detection schemes. We discuss applications beyond these two types of images and suggest next steps for making detection of problematic images a systematic step in peer review and science in general."
In this paper we present a novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous driving scenarios. The proposed architecture uses a middle-fusion approach to fuse the radar point clouds and RGB images. Our radar object proposal network uses radar point clouds to generate 3D proposals from a set of 3D prior boxes. These proposals are mapped to the image and fed into a Radar Proposal Refinement (RPR) network for objectness score prediction and box refinement. The RPR network utilizes both radar information and image feature maps to generate accurate object proposals and distance estimations. The radar-based proposals are combined with image-based proposals generated by a modified Region Proposal Network (RPN). The RPN has a distance regression layer for estimating distance for every generated proposal. The radar-based and image-based proposals are merged and used in the next stage for object classification. Experiments on the challenging nuScenes dataset show our method outperforms other existing radar-camera fusion methods in the 2D object detection task while at the same time accurately estimates objects' distances.
"The NEURON simulator has been developed over the past three decades and is widely used by neuroscientists to model the electrical activity of neuronal networks. Large network simulation projects using NEURON have supercomputer allocations that individually measure in the millions of core hours. Supercomputer centers are transitioning to next generation architectures and the work accomplished per core hour for these simulations could be improved by an order of magnitude if NEURON was able to better utilize those new hardware capabilities. In order to adapt NEURON to evolving computer architectures, the compute engine of the NEURON simulator has been extracted and has been optimized as a library called CoreNEURON. This paper presents the design, implementation and optimizations of CoreNEURON. We describe how CoreNEURON can be used as a library with NEURON and then compare performance of different network models on multiple architectures including IBM BlueGene/Q, Intel Skylake, Intel MIC and NVIDIA GPU. We show how CoreNEURON can simulate existing NEURON network models with 4-7x less memory usage and 2-7x less execution time while maintaining binary result compatibility with NEURON."
"We report a GPT-based multi-sentence language model for dialogue generation and document understanding. First, we propose a hierarchical GPT which consists of three blocks, i.e., a sentence encoding block, a sentence generating block, and a sentence decoding block. The sentence encoding and decoding blocks are basically the encoder-decoder blocks of the standard Transformers, which work on each sentence independently. The sentence generating block is inserted between the encoding and decoding blocks, and generates the next sentence embedding vector from the previous sentence embedding vectors. We believe it is the way human make conversation and understand paragraphs and documents. Since each sentence may consist of fewer words, the sentence encoding and decoding Transformers can use much smaller dimensional embedding vectors. Secondly, we note the attention in the Transformers utilizes the inner-product similarity measure. Therefore, to compare the two vectors in the same space, we set the transform matrices for queries and keys to be the same. Otherwise, the similarity concept is incongruent. We report experimental results to show that these two modifications increase the language model performance for tasks with multiple sentences."
"We propose Dynamic Blocking, a decoding algorithm which enables large-scale pretrained autoregressive models (such as BART, T5, GPT-2 and XLNet) to generate high-quality paraphrases in an unsupervised setting. In order to obtain an alternative surface form, whenever the language model emits a token that is present in the source sequence, we prevent the model from generating the subsequent source token for the next time step. We show that our approach achieves state-of-the-art results on benchmark datasets when compared to previous unsupervised approaches, and is even comparable with strong supervised, in-domain models. We also propose a new automatic metric based on self-BLEU and BERTscore which not only discourages the model from copying the input through, but also evaluates text similarity based on distributed representations, hence avoiding reliance on exact keyword matching. In addition, we demonstrate that our model generalizes across languages without any additional training."
"In recent years, unmanned aircraft systems (UASs) are frequently used in many different applications of photogrammetry such as building damage monitoring, archaeological mapping and vegetation monitoring. In this paper, a new state-of-the-art vertical take-off and landing fixed-wing UAS is proposed to robust photogrammetry missions, called SAMA-VTOL. In this study, the capability of SAMA-VTOL is investigated for generating orthophoto. The major stages are including designing, building and experimental scenario. First, a brief description of design and build is introduced. Next, an experiment was done to generate accurate orthophoto with minimum ground control points requirements. The processing step, which includes automatic aerial triangulation with camera calibration and model generation. In this regard, the Pix4Dmapper software was used to orientate the images, produce point clouds, creating digital surface model and generating orthophoto mosaic. Experimental results based on the test area covering 26.3 hectares indicate that our SAMA-VTOL performs well in the orthophoto mosaic task."
"Frequency entangled photon sources are in high demand in a variety of optical quantum technologies, including quantum key distribution, cluster state quantum computation and quantum metrology. In the recent decade, chip-scale entangled photon sources have been developed using silicon platforms, offering robustness, large scalability and CMOS technology compatibility. Here, we report the generation of frequency correlated photon pairs using a 150-GHz silicon nitride ring cavity. First, the device is characterized for studying the phase matching condition during spontaneous four-wave mixing. Next, we evaluate the joint spectrum intensity of the generated photons and confirm the photon pair generation in a total of42 correlated frequency mode pairs, corresponding to a bandwidth of 51.25 nm. Finally, the experimental results are analyzed and the joint spectral intensity is quantified in terms of the phase matching condition."
"Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https://github.com/GT-SALT/IDBR."
"In this paper, we tackle the problem of training with multiple source domains with the aim to generalize to new domains at test time without an adaptation step. This is known as domain generalization (DG). Previous works on DG assume identical categories or label space across the source domains. In the case of category shift among the source domains, previous methods on DG are vulnerable to negative transfer due to the large mismatch among label spaces, decreasing the target classification accuracy. To tackle the aforementioned problem, we introduce an end-to-end feature-norm network (FNN) which is robust to negative transfer as it does not need to match the feature distribution among the source domains. We also introduce a collaborative feature-norm network (CFNN) to further improve the generalization capability of FNN. The CFNN matches the predictions of the next most likely categories for each training sample which increases each network's posterior entropy. We apply the proposed FNN and CFNN networks to the problem of DG for image classification tasks and demonstrate significant improvement over the state-of-the-art."
"Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state-of-the-art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed some light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures."
"Book covers are intentionally designed and provide an introduction to a book. However, they typically require professional skills to design and produce the cover images. Thus, we propose a generative neural network that can produce book covers based on an easy-to-use layout graph. The layout graph contains objects such as text, natural scene objects, and solid color spaces. This layout graph is embedded using a graph convolutional neural network and then used with a mask proposal generator and a bounding-box generator and filled using an object proposal generator. Next, the objects are compiled into a single image and the entire network is trained using a combination of adversarial training, perceptual training, and reconstruction. Finally, a Style Retention Network (SRNet) is used to transfer the learned font style onto the desired text. Using the proposed method allows for easily controlled and unique book covers."
"Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model's behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model's predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis."
"For the past six years, researchers in genetic programming and other program synthesis disciplines have used the General Program Synthesis Benchmark Suite to benchmark many aspects of automatic program synthesis systems. These problems have been used to make notable progress toward the goal of general program synthesis: automatically creating the types of software that human programmers code. Many of the systems that have attempted the problems in the original benchmark suite have used it to demonstrate performance improvements granted through new techniques. Over time, the suite has gradually become outdated, hindering the accurate measurement of further improvements. The field needs a new set of more difficult benchmark problems to move beyond what was previously possible.   In this paper, we describe the 25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite. These problems are curated from a variety of sources, including programming katas and college courses. We selected these problems to be more difficult than those in the original suite, and give results using PushGP showing this increase in difficulty. These new problems give plenty of room for improvement, pointing the way for the next six or more years of general program synthesis research."
"Learning to solve diagrammatic reasoning (DR) can be a challenging but interesting problem to the computer vision research community. It is believed that next generation pattern recognition applications should be able to simulate human brain to understand and analyze reasoning of images. However, due to the lack of benchmarks of diagrammatic reasoning, the present research primarily focuses on visual reasoning that can be applied to real-world objects. In this paper, we present a diagrammatic reasoning dataset that provides a large variety of DR problems. In addition, we also propose a Knowledge-based Long Short Term Memory (KLSTM) to solve diagrammatic reasoning problems. Our proposed analysis is arguably the first work in this research area. Several state-of-the-art learning frameworks have been used to compare with the proposed KLSTM framework in the present context. Preliminary results indicate that the domain is highly related to computer vision and pattern recognition research with several challenging avenues."
"In prediction problems, it is common to model the data-generating process and then use a model-based procedure, such as a Bayesian predictive distribution, to quantify uncertainty about the next observation. However, if the posited model is misspecified, then its predictions may not be calibrated -- that is, the predictive distribution's quantiles may not be nominal frequentist prediction upper limits, even asymptotically. Rather than abandoning the comfort of a model-based formulation for a more complicated non-model-based approach, here we propose a strategy in which the data itself helps determine if the assumed model-based solution should be adjusted to account for model misspecification. This is achieved through a generalized Bayes formulation where a learning rate parameter is tuned, via the proposed generalized predictive calibration (GPrC) algorithm, to make the predictive distribution calibrated, even under model misspecification. Extensive numerical experiments are presented, under a variety of settings, demonstrating the proposed GPrC algorithm's validity, efficiency, and robustness."
"There exist two canonical approaches to describe open quantum systems by a time-evolution equation: the Nakajima-Zwanzig quantum master equation, featuring a time-nonlocal memory kernel $\mathcal{K}$, and the time-convolutionless equation with a time-local generator $\mathcal{G}$. These key quantities have recently been shown to be connected by an exact fixed-point relation [Phys. Rev. X 11, 021041 (2021)]. Here we show that this implies a recursive relation between their perturbative expansions, allowing a series for the kernel $\mathcal{K}$ to be translated directly into a corresponding series for the more complicated generator $\mathcal{G}$. This leads to an elegant way of computing the generator using well-developed, standard memory-kernel techniques for strongly interacting open systems. Moreover, it allows for an unbiased comparison of time-local and time-nonlocal approaches independent of the particular technique chosen to calculate expansions of $\mathcal{K}$ and $\mathcal{G}$ (Nakajima-Zwanzig projections, real-time diagrams, etc.). We illustrate this for leading and next-to-leading order calculations of $\mathcal{K}$ and $\mathcal{G}$ for the single impurity Anderson model using both the bare expansion in the system-environment coupling and a more advanced renormalized series. We compare the different expansions obtained, quantify the legitimacy of the generated dynamics (complete positivity) and benchmark with the exact result in the non-interacting limit."
"We propose a novel universal detector for detecting images generated by using CNNs. In this paper, properties of checkerboard artifacts in CNN-generated images are considered, and the spectrum of images is enhanced in accordance with the properties. Next, a classifier is trained by using the enhanced spectrums to judge a query image to be a CNN-generated ones or not. In addition, an ensemble of the proposed detector with emphasized spectrums and a conventional detector is proposed to improve the performance of these methods. In an experiment, the proposed ensemble is demonstrated to outperform a state-of-the-art method under some conditions."
"The next generation of scientific experiments and studies, popularly called as e-Science, is carried out by large collaborations of researchers distributed around the world engaged in analysis of huge collections of data generated by scientific instruments. Grid computing has emerged as an enabler for e-Science as it permits the creation of virtual organizations that bring together communities with common objectives. Within a community, data collections are stored or replicated on distributed resources to enhance storage capability or efficiency of access. In such an environment, scientists need to have the ability to carry out their studies by transparently accessing distributed data and computational resources. In this paper, we propose and develop a Grid broker that mediates access to distributed resources by (a) discovering suitable data sources for a given analysis scenario, (b) suitable computational resources, (c) optimally mapping analysis jobs to resources, (d) deploying and monitoring job execution on selected resources, (e) accessing data from local or remote data source during job execution and (f) collating and presenting results. The broker supports a declarative and dynamic parametric programming model for creating grid applications. We have used this model in grid-enabling a high energy physics analysis application (Belle Analysis Software Framework). The broker has been used in deploying Belle experiment data analysis jobs on a grid testbed, called Belle Analysis Data Grid, having resources distributed across Australia interconnected through GrangeNet."
"The Eureqa symbolic regression program has recently received extensive press praise. A representative quote is   ""There are very clever 'thinking machines' in existence today, such as Watson, the IBM computer that conquered Jeopardy! last year. But next to Eureqa, Watson is merely a glorified search engine.""   The program was designed to work with noisy experimental data. However, if the data is generated from an expression for which there exists more concise equivalent expressions, sometimes some of the Eureqa results are one or more of those more concise equivalents. If not, perhaps one or more of the returned Eureqa results might be a sufficiently accurate approximation that is more concise than the given expression. Moreover, when there is no known closed form expression, the data points can be generated by numerical methods, enabling Eureqa to find expressions that concisely fit those data points with sufficient accuracy. In contrast to typical regression software, the user does not have to explicitly or implicitly provide a specific expression or class of expressions containiing unknown constants for the software to determine.   Is Eureqa useful enough in these regards to provide an additional tool for experimental mathematics, computer algebra users and numerical analysis? Yes if used carefully. Can computer algebra and numerical methods help Eureqa? Definitely."
"Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth information meaning. Our goals are: to translate written text from any language to ASL animation; to model maximum raw information using machine learning and computational techniques; and to produce a more adapted and expressive form to natural looking and understandable ASL animations. Our methods include linguistic annotation of initial text and semantic orientation to generate the facial expression. We use the genetic algorithms coupled to learning/recognized systems to produce the most natural form. To detect emotion we are based on fuzzy logic to produce the degree of interpolation between facial expressions. Roughly, we present a new expressive language Text Adapted Sign Modeling Language TASML that describes all maximum aspects related to a natural sign language interpretation. This paper is organized as follow: the next section is devoted to present the comprehension effect of using Space/Time/SVO form in ASL animation based on experimentation. In section 3, we describe our technical considerations. We present the general approach we adopted to develop our tool in section 4. Finally, we give some perspectives and future works."
"Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used."
"Index Modulations, in the form of Spatial Modulation or Polarized Modulation, are gaining traction for both satellite and terrestrial next generation communication systems. Adaptive Index Modulation based links are needed to fully exploit the transmission capacity of time-variant channels. The adaptation of code and/or modulation requires a real-time evaluation of the channel achievable rates. Some existing results in the literature present a computational complexity which scales quadratically with the number of transmit antennas and the constellation order. Moreover, the accuracy of these approximations is low and it can lead to wrong Modulation and Coding Scheme selection. In this work we apply a Multilayer Feedforward Neural Network to compute the achievable rate of a generic Index Modulation link. The case of two antennas/polarizations is analyzed throughly showing the neural network not only a one-hundred fold decrement of the Mean Square Error in the estimation of the capacity compared with existing analytical approximations, but it also reduces fifty times the computational complexity. Moreover, the extension to an arbitrary number of antennas is explained and supported with simulations. More generally, neural networks can be considered as promising candidates for the practical estimation of complex metrics in communication related settings."
"A general introduction is given in chapter 1. Chapter 2 outlines the main features of the Standard Model (SM) of particle physics and the theoretical motivations for going beyond it. We subsequently provide brief descriptions of a few popular models that aim to solve the issues that arise within the SM. In chapter 3 we describe the general Monte Carlo method and show how it can be used to construct a class of computational tools called Monte Carlo event generators. We describe the main generic features of event generators and how these are implemented in the HERWIG++ event generator. By applying resummation techniques, we provide, in chapter 4, analytical calculations of two types of hadron collider observables: global inclusive variables and the transverse energy of the QCD initial state radiation (ET), associated with the either Drell-Yan gauge boson production or Higgs boson production. In both cases we provide comparisons to results obtained from Monte Carlo event generators. In chapter 5 we examine two well-motivated models for new physics: one of new heavy charged vector bosons (W prime) and a model motivated by strong dynamics electroweak symmetry breaking that contains new resonances, leptoquarks, that couple primarily to quarks and leptons of the third generation. In the prior model, we improve the current treatment of the W' by considering interference effects with the SM W and construct an event generator accurate to next-to-leading order which we use to conduct a phenomenological analysis. For the leptoquark model, we provide an implementation in the HERWIG++ event generator and use it to form a strategy for mass reconstruction."
"It is safe to assume that, for the foreseeable future, machine learning, especially deep learning will remain both data- and computation-hungry. In this paper, we ask: Can we build a global exchange where everyone can contribute computation and data to train the next generation of machine learning applications?   We present an early, but running prototype of DataBright, a system that turns the creation of training examples and the sharing of computation into an investment mechanism. Unlike most crowdsourcing platforms, where the contributor gets paid when they submit their data, DataBright pays dividends whenever a contributor's data or hardware is used by someone to train a machine learning model. The contributor becomes a shareholder in the dataset they created. To enable the measurement of usage, a computation platform that contributors can trust is also necessary. DataBright thus merges both a data market and a trusted computation market.   We illustrate that trusted computation can enable the creation of an AI market, where each data point has an exact value that should be paid to its creator. DataBright allows data creators to retain ownership of their contribution and attaches to it a measurable value. The value of the data is given by its utility in subsequent distributed computation done on the DataBright computation market. The computation market allocates tasks and subsequent payments to pooled hardware. This leads to the creation of a decentralized AI cloud. Our experiments show that trusted hardware such as Intel SGX can be added to the usual ML pipeline with no additional costs. We use this setting to orchestrate distributed computation that enables the creation of a computation market. DataBright is available for download at https://github.com/ds3lab/databright."
"Deep neural networks (DNNs) have been demonstrated as effective prognostic models across various domains, e.g. natural language processing, computer vision, and genomics. However, modern-day DNNs demand high compute and memory storage for executing any reasonably complex task. To optimize the inference time and alleviate the power consumption of these networks, DNN accelerators with low-precision representations of data and DNN parameters are being actively studied. An interesting research question is in how low-precision networks can be ported to edge-devices with similar performance as high-precision networks. In this work, we employ the fixed-point, floating point, and posit numerical formats at $\leq$8-bit precision within a DNN accelerator, Deep Positron, with exact multiply-and-accumulate (EMAC) units for inference. A unified analysis quantifies the trade-offs between overall network efficiency and performance across five classification tasks. Our results indicate that posits are a natural fit for DNN inference, outperforming at $\leq$8-bit precision, and can be realized with competitive resource requirements relative to those of floating point."
"We recently completed a calculation of the process $e^+e^- \to Q\bar{Q}+X$, where Q is a heavy quark, at order $O(\alpha_s^2)$. As a first application of this calculation we compute the momentum correlations of $b\bar{b}$ pairs at next-to-leading-order. This quantity is interesting since it may affect the determination of $R_b$ as measured in $Z^0$ decays. We find that the next-to-leading corrections are of moderate size, thus confirming the conclusions that can be drawn from the leading-order calculation."
"We compute the next-to-leading, O(m alpha^5), contribution to the hyperfine splitting in positronium within the framework of NRQED. When applied to the ground state, our calculation reproduces known results, providing a further test of NRQED techniques. Besides providing a very simple method of calculation of the standard result, we also obtain new expressions for excited states of positronium with negligible additional effort. Our calculation requires the complete next-to-leading matching of the lowest-dimension NRQED four-fermi couplings, which we publish here for the first time."
"We compute the virtual next-to-leading corrections to the Lipatov vertex in the helicity-amplitude formalism. These agree with previous results by Fadin and collaborators, in the conventional dimensional-regularization scheme. We discuss the choice of reggeization scale in order to minimize its impact on the next-to-leading-logarithmic corrections to the BFKL equation."
"We compute the complete leading-log terms of the next-to-next-to-next-to-leading-order corrections to potential NRQCD. As a by-product we obtain the leading logs at $O(m\alpha_s^5)$ in the heavy quarkonium spectrum. These leading logs, when $\Lambda_{QCD} \ll m\alpha_s^2$, give the complete $O(m\alpha_s^5 \ln \alpha_s)$ corrections to the heavy quarkonium spectrum."
"I present results of a next-to-leading order calculation of three jet production at hadron colliders. This calculation will have many applications. In addition to computing three-jet observables (spectra, mass distributions), this calculation permits the first next-to-leading order studies (at hadron colliders) of jet and event shape variables."
"We present results of a next-to-leading order calculation of three jet production at hadron colliders. This calculation will have many applications. In addition to computing such three-jet observables as spectra, mass distributions, this calculation permits the first next-to-leading order studies at hadron colliders of jet and event shape variables."
"We study the effect of resumming large logarithms in the determination of the bottom quark mass through a non-relativistic sum rule analysis. Our result is complete at next-to-leading-logarithmic accuracy and includes some known contributions at next-to-next-to-leading logarithmic accuracy. Compared to finite order computations, the reliability of the theoretical evaluation is greatly improved, resulting in a substantially reduced scale dependence and a faster convergent perturbative series. This allows us to significantly improve over previous determinations of the $\MS$ bottom quark mass, $\bar{m}_b$, from non-relativistic sum rules. Our final figure reads $\bar{m}_b(\bar{m}_b)=4.19\pm 0.06$ GeV."
We compute the third-order correction to electromagnetic   S-wave quarkonium production and annihilation rates due to the emission and absorption of an ultrasoft gluon. Our result completes the analysis of the non-relativistic quarkonium bound-state dynamics in the next-to-next-to-next-to-leading order. The impact of the ultrasoft correction on the Upsilon(1S) leptonic width and the top quark-antiquark threshold production cross section is estimated.
"We compute the next-to-next-to-leading order (NNLO) QCD corrections to the thrust distribution in electron-positron annihilation. The corrections turn out to be sizable, enhancing the previously known next-to-leading order prediction by about 15%. Inclusion of the NNLO corrections significantly reduces the theoretical renormalisation scale uncertainty on the prediction of the thrust distribution."
We compute the static energy of QCD at short distances at next-to-next-to-next-to leading-logarithmic accuracy in terms of the three-loop singlet potential. By comparing our results with lattice data we extract the value of the unknown piece of the three-loop singlet potential.
"We critically assess mainstream accounting and finance research applying methods from computational linguistics (CL) to study financial discourse. We also review common themes and innovations in the literature and assess the incremental contributions of work applying CL methods over manual content analysis. Key conclusions emerging from our analysis are: (a) accounting and finance research is behind the curve in terms of CL methods generally and word sense disambiguation in particular; (b) implementation issues mean the proposed benefits of CL are often less pronounced than proponents suggest; (c) structural issues limit practical relevance; and (d) CL methods and high quality manual analysis represent complementary approaches to analyzing financial discourse. We describe four CL tools that have yet to gain traction in mainstream AF research but which we believe offer promising ways to enhance the study of meaning in financial discourse. The four tools are named entity recognition (NER), summarization, semantics and corpus linguistics."
"A numerical program for the evaluation of the inclusive cross section for associated Higgs production with a massive weak gauge boson at hadron colliders is described, sigma(pp/pbar p -> HV), V=W,Z. The calculation is performed in the framework of the Standard Model and includes next-to-next-to-leading order QCD as well as next-to-leading order electro-weak effects."
We compute the next-to-leading order QCD corrections to the ``direct'' part of the spin-dependent cross section for hadron-pair photoproduction. The calculation is performed using largely analytical methods. We present a brief phenomenological study of our results focussing on the $K$-factors and scale dependence of the next-to-leading order cross sections. This process is relevant for the extraction of the gluon polarization in present and future spin-dependent lepton-nucleon scattering experiments.
"Solitons of a discrete nonlinear Schr\""{o}dinger equation which includes the next-nearest-neighbor interactions are studied by means of a variational approximation and numerical computations. A large family of multi-humped solutions, including those with a nontrivial phase structure which are a feature particular to the next-nearest-neighbor interaction model, are accurately predicted by the variational approximation. Bifurcations linking solutions with the trivial and nontrivial phase structures are also captured remarkably well, including a prediction of critical parameter values."
We compute the energy levels of some of the lower-lying heavy quarkonium states perturbatively up to O(alpha_s^5*m) and O(alpha_s^5*m*log[alpha_s]). Stability of the predictions depends crucially on the unknown 4-loop pole-MSbar mass relation. We discuss the current status of the predictions with respect to the observed bottomonium spectrum.
"We introduce bisimulations for the logic $ITL^e$ with `next', `until' and `release', an intuitionistic temporal logic based on structures equipped with a partial order used to interpret intuitionistic implication and a monotone function used to interpret the temporal modalities. Our main results are that `eventually', which is definable in terms of `until', cannot be defined in terms of `next' and `henceforth', and similarly that `henceforth', definable in terms of `release', cannot be defined in terms of `next' and `until', even over the smaller class of here-and-there models."
"We compute the master integrals required for the calculation of the double-real emission contributions to the matching coefficients of 0-jettiness beam functions at next-to-next-to-next-to-leading order in perturbative QCD. As an application, we combine these integrals and derive the double-real gluon emission contribution to the matching coefficient $I_{qq}(t,z)$ of the quark beam function."
We present precise predictions for the Higgs boson rapidity distribution at the LHC in the gluon fusion production mode. Our approach relies on the fully analytic computation of six terms in a systematic expansion of the partonic differential cross section around the production threshold of the Higgs boson at next-to-next-to-next-to leading order (N$^3$LO) in QCD perturbation theory. We observe a mild correction compared to the previous perturbative order and a significant reduction of the dependence of the cross section on the perturbative scale throughout the entire rapidity range.
"We compute the three-loop master integrals required for the calculation of the triple-real contribution to the N$^3$LO quark beam function due to the splitting of a quark into a virtual quark and three collinear gluons, $q \to q^*+ggg$. This provides an important ingredient for the calculation of the leading-color contribution to the quark beam function at N$^3$LO."
"We compute the real-radiation corrections to Higgs boson pair production at   next-to-next-to-leading order in QCD, in an expansion for large top quark mass. We   concentrate on the radiative corrections to the interference contribution   from the next-to-leading order one-particle reducible and the leading order   amplitudes. This is a well defined and gauge invariant subset of the full   real-virtual corrections to the inclusive cross section. We obtain analytic   results for all phase-space master integrals both as an expansion around the   threshold and in an exact manner in terms of Goncharov polylogarithms."
"In this talk we examine how one-loop soft and collinear splitting functions occur in the calculation of next-to-next-to-leading order (NNLO) corrections to production rates, and we present the one-loop gluon soft and splitting functions, computed to all orders in the dimensional regularization parameter $\epsilon$. We apply the one-loop gluon soft function to the calculation of the next-to-leading logarithmic corrections to the Lipatov vertex to all orders in $\epsilon$."
"We present our implementation autoCAS for fully automated multi-configurational calculations, which we also make available free of charge on our webpages. The graphical user interface of autoCAS connects a general electronic structure program with a density matrix renormalization group program to carry out our recently introduced automated active space selection protocol for multi-configurational calculations [J. Chem. Theory Comput., 2016, 12, 1760]. Next to this active space selection, autoCAS carries out several steps of multi-configurational calculations so that only a minimal input is required to start them, comparable to that of a standard Kohn-Sham density functional theory calculation, so that black-box multi-configurational calculations become feasible. Furthermore, we introduce a new extension to the selection algorithm that facilitates automated selections for molecules with large valence orbital spaces consisting of several hundred orbitals."
"We compute the full three-loop coefficient functions for the structure functions F_2 and F_L in massless perturbative QCD. The results for F_L complete the next-to-next-to-leading order description of unpolarized electromagnetic deep-inelastic scattering. The third-order coefficient functions for F_2 form, at not too small values of the Bjorken variable x, the dominant part of the next-to-next-to-next-to-leading order corrections, thus facilitating improved determinations of the strong coupling alpha_s from scaling violations. The three-loop corrections to F_L are larger than those for F_2. Especially for the latter quantity the expansion in powers of alpha_s is very stable, for photon virtualities Q^2 >> 1 GeV^2, over the full x-range accessible to fixed-target and collider measurements."
"In this paper, we compute the next-nearest-neighboring site percolation (Connections exist not only between nearest-neighboring sites, but also between next-nearest-neighboring sites.) probabilities Pc on the two-dimensional Sierpinski carpets, using the translational-dilation method and Monte Carlo technique. We obtain a relation among Pc, fractal dimensionality D and connectivity Q. For the family of carpets with central cutouts,, where, the critical percolation probability for the next-nearest-neighboring site problem on square lattice. As D reaches 2, which is in agreement with the critical percolation probability on 2-d square lattices with next-nearest-neighboring interactions."
"We compute production rates for two, three, four and five jets in electron-positron annihilation at the third order in the QCD coupling constant. At this order, three-jet production is described to next-to-next-to-leading order (NNLO) in perturbation theory while the two-jet rate is obtained at next-to-next-to-next-to-leading order (N$^3$LO). Our results yield an improved perturbative description of the dependence of jet multiplicity on the jet resolution parameter, $\ycut$, particularly at small values of $\ycut$."
"We consider the transverse-momentum (q_T) distribution of Standard Model Higgs bosons produced by gluon fusion in hadron collisions. At small q_T (q_T<<m_H, m_H being the mass of the Higgs boson), we resum the logarithmically-enhanced contributions due to multiple soft-gluon emission to all order in QCD perturbation theory. At intermediate and large values of q_T (q_T <~m_H), we consistently combine resummation with the known fixed-order results. We use the most advanced perturbative information that is available at present: next-to-next-to-leading logarithmic resummation combined with the next-to-leading fixed-order calculation. We extend previous results including exactly all the perturbative terms up to order alphas^4 in our computation and, after integration over q_T, we recover the known next-to-next-to-leading order result for the total cross section. We present numerical results at the Tevatron and the LHC, together with an estimate of the corresponding uncertainties. Our calculation is implemented in an updated version of the numerical code HqT."
"We compute next-to-next-to-leading order QCD corrections to the gluon-induced production cross section of Higgs boson pairs in the large top quark mass limit using the soft-virtual approximation. In the limit of infinitely-heavy top quark we confirm the results in the literature. We add two more expansion terms in the inverse top quark mass to the $M_t\to\infty$ result. Since the $1/M_t$ expansion converges poorly, we try to improve on it by factorizing the exact leading order cross section. We discuss two ways of doing that and conclude that the finite top quark mass effects shift the cross section at most by about 10\% at next-to-leading order and by about 5\% at next-to-next-to-leading order."
"We have developed a subtractive renormalization method with which we can evaluate nucleon-nucleon (NN) scattering phase shifts produced by the NN potential obtained at leading, next-to-leading, and next-to-next-to-leading order (NNLO) in chiral effective theory ($\chi$ET). In this method the low-energy constants associated with short-distance NN physics are eliminated from the Lippmann-Schwinger equation (LSE) for the NN t-matrix, in favor of physical observables. This allows us to straightforwardly compute scattering phase shifts for ultra-violet cutoffs of at least 10 GeV. We then perform detailed analyses of the maximum cutoff at which the use of a $\chi$ET NN potential in the LSE makes sense."
"The Wilson Coefficients for all 4-parton operators which arise in matching QCD to Soft-Collinear Effective Theory (SCET) are computed at 1-loop. Any dijet observable calculated in SCET beyond leading order will require these results. The Wilson coefficients are separated by spin and color, although most applications will involve only the spin-averaged hard functions. The anomalous dimensions for the Wilson coefficients are given to 2-loop order, and the renormalization group equations are solved explicitly. This will allow for analytical resummation of dijet observables to next-to-next-to-leading logarithmic accuracy. For each channel, there is a natural basis in which the evolution is diagonal in color space. The same basis also diagonalizes the color evolution for the soft function. Even though soft functions required for SCET calculations are observable dependent, it is shown that their renormalization group evolution is almost completely determined by a universal structure. With these results, it will be possible to calculate hadronic event shapes or other dijet observables to next-to-leading order with next-to-next-to-leading log resummation."
"We compute the next-to-next-to-leading order QCD corrections for Standard Model Higgs boson pair production inclusive cross section at hadron colliders within the large top-mass approximation. We provide numerical results for the LHC, finding that the corrections are large, resulting in an increase of ${\cal O}(20%)$ with respect to the next-to-leading order result at c.m. energy $\sqrt{s_H}=14\,\text{TeV}$. We observe a substantial reduction in the scale dependence, with overlap between the current and previous order prediction. All our results are normalized using the full top- and bottom-mass dependence at leading order. We also provide analytical expressions for the K factors as a function of $s_H$."
"We study master integrals needed to compute the Higgs boson production cross section via gluon fusion in the infinite top quark mass limit, using a canonical form of differential equations for master integrals, recently identified by Henn, which makes their solution possible in a straightforward algebraic way. We apply the known criteria to derive such a suitable basis for all the phase space master integrals in afore mentioned process at next-to-next-to-leading order in QCD and demonstrate that the method is applicable to next-to-next-to-next-to-leading order as well by solving a non-planar topology. Furthermore, we discuss in great detail how to find an adequate basis using practical examples. Special emphasis is devoted to master integrals which are coupled by their differential equations."
"We are interested in the transverse-momentum ($q_T$) distribution of a diphoton pair produced in hadron collisions. We resum the logarithmically-enhanced perturbative QCD contributions at small values of $q_T$ up to next-to-next-to-leading logarithmic accuracy. We consistently combine resummation with the known next-to-leading order perturbative result at intermediate and large values of $q_T$ . We include all perturbative terms up to order $\alpha_S^2$ in our computation which, after integration over $q_T$ , reproduces the known next-to-next-to-leading order result for the diphoton pair production total cross section. A comparison with LHC data is presented. We estimate the perturbative accuracy of the theoretical calculation by performing the corresponding variation of scales. We anticipate that the effect of the transverse momentum resummation is not only to recover the predictivity of the calculation at small $q_T$ , but also to improve substantially the agreement with the experimental data."
"The recently completed research project DEEP-ER has developed a variety of hardware and software technologies to improve the I/O capabilities of next generation high-performance computers, and to enable applications recovering from the larger hardware failure rates expected on these machines.   The heterogeneous Cluster-Booster architecture --first introduced in the predecessor DEEP project-- has been extended by a multi-level memory hierarchy employing non-volatile and network-attached memory devices. Based on this hardware infrastructure, an I/O and resiliency software stack has been implemented combining and extending well established libraries and software tools, and sticking to standard user-interfaces. Real-world scientific codes have tested the projects' developments and demonstrated the improvements achieved without compromising the portability of the applications."
"Hard processes in diffractive deep-inelastic scattering can be described by a factorisation into parton-level subprocesses and diffractive parton distributions. In this framework, cross sections for inclusive dijet production in diffractive deep-inelastic electron-proton scattering (DIS) are computed to next-to-next-to-leading order (NNLO) QCD accuracy and compared to a comprehensive selection of data. Predictions for the total cross sections, 39 single-differential and four double-differential distributions for six measurements at HERA by the H1 and ZEUS collaborations are calculated. In the studied kinematical range, the NNLO corrections are found to be sizeable and positive. The NNLO predictions typically exceed the data, while the kinematical shape of the data is described better at NNLO than at next-to-leading order (NLO). A significant reduction of the scale uncertainty is achieved in comparison to NLO predictions. Our results use the currently available NLO diffractive parton distributions, and the discrepancy in normalisation highlights the need for a consistent determination of these distributions at NNLO accuracy."
"We compute all helicity amplitudes for four-particle scattering in massless QCD with $n_f$ fermion flavours to next-to-next-to-leading order (NNLO) in perturbation theory. In particular, we consider all possible configurations of external quarks and gluons. We evaluate the amplitudes in terms of a Laurent series in the dimensional regulator to the order required for future next-to-next-to-next-to-leading order (N$^3$LO) calculations. The coefficients of the Laurent series are given in terms of harmonic polylogarithms that can readily be evaluated numerically. We present our findings in the conventional dimensional regularisation and in the t'Hooft-Veltman schemes."
"We present the next-to-leading power (NLP) factorization formula for the $q\bar{q}\to \gamma^*+X$ channel of the Drell-Yan production near the kinematic threshold limit. The formalism used for the computation of next-to-leading power corrections within soft-collinear effective field theory is introduced, we discuss the emergence of new objects, the NLP collinear functions, and define them through an operator matching equation. We review the leading power factorization before extending it to subleading powers. We also present the one-loop result for the newly introduced collinear function, and demonstrate explicitly conceptual issues in performing next-to-leading logarithmic resummation at next-to-leading power."
"Future precision measurements of Higgs boson decays will determine the branching fraction for the decay into two photons with a precision at the one percent level. To fully exploit such measurements, equally precise theoretical predictions need to be available. To this end we compute four-loop QCD corrections in the large top quark mass expansion to the Higgs boson--photon form factor, which enter the two-photon decay width at next-to-next-to-next-to-leading order. Furthermore we obtain corrections to the two-photon decay width stemming from the emission of additional gluons, which contribute for the first time at next-to-next-to-leading order. Finally, we combine our results with other available perturbative corrections and estimate the residual uncertainty due to missing higher-order contributions."
"With the compelling evidence for massive neutrinos from recent neutrino-oscillation experiments, one of the most fundamental tasks of particle physics over the next years will be the determination of the absolute mass scale of neutrinos. The absolute value of neutrino-masses will have crucial implications for cosmology, astrophysics and particle physics. We present the case for a next generation tritium beta decay experiment to perform a high precision direct measurement of the absolute mass of the electron neutrino with sub-eV sensitivity. We discuss the experimental requirements and technical challenges of the proposed Karlsruhe Tritium Neutrino experiment (KATRIN) and outline its physics potential."
"The second and third factorial moments of the multiplicity distributions of gluon and quark jets are calculated up to the next-to-next-to-next-to leading order in perturbative QCD, using the equations for generating functions. The results are confronted with experimental data. A general discussion on high order corrections revealed by such an approach is given. Other possible corrections and approaches are discussed as well."
"We consider QCD radiative corrections to WW pair production in hadron collisions. We perform a calculation that consistently combines next-to-leading order predictions with soft-gluon resummation valid at small transverse momenta ptWW of the WW pair. We present results for the ptWW distribution at the LHC up to (almost) next-to-next-to-leading logarithmic accuracy, and study the effect of resummation on the charged-lepton distributions. Soft-gluon effects are typically mild, but they can be strongly enhanced when hard cuts are applied. The relevant distributions are generally well described by the MC@NLO event generator."
"We point out that the chiral Lagrangian describing pseudo-Goldstone bosons in partially quenched QCD has one more four-derivative operator than that for unquenched QCD with three flavors. The new operator can be chosen to vanish in the unquenched sector of the partially quenched theory. Its contributions begin at next-to-leading order in the chiral expansion. At this order it contributes only to unphysical scattering processes, and we work out some examples. Its contributions to pseudo-Goldstone properties begin at next-to-next-to-leading order, and we determine their form. We also determine all the zero and two derivative operators in the $O(p^6)$ partially quenched chiral Lagrangian, finding three more than in unquenched QCD, and use these to give the general form of the analytic next-to-next-to-leading order contributions to the pseudo-Goldstone mass and decay constant. We discuss the general implications of such additional operators for the utility of partially quenched simulations"
"We extract the next-to-next-to-leading order results for spin-flip generalized polarizabilities (GPs) of the nucleon from the spin-dependent amplitudes for virtual Compton scattering (VCS) at ${\cal O}(p^4)$ in heavy baryon chiral perturbation theory. At this order, no unknown low energy constants enter the theory, allowing us to make absolute predictions for all spin-flip GPs. Furthermore, by using constraint equations between the GPs due to nucleon crossing combined with charge conjugation symmetry of the VCS amplitudes, we get a next-to-next-to-next-to-leading order prediction for one of the GPs. We provide estimates for forthcoming double polarization experiments which allow to access these spin-flip GPs of the nucleon."
"Which song will Smith listen to next? Which restaurant will Alice go to tomorrow? Which product will John click next? These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network (e.g. website links, geographic location). What users are doing now may be unrelated to what they will be doing in an hour from now. Mindful of these challenges we propose TribeFlow, a method designed to cope with the complex challenges of learning personalized predictive models of non-stationary, transient, and time-heterogeneous user trajectories. TribeFlow is a general method that can perform next product recommendation, next song recommendation, next location prediction, and general arbitrary-length user trajectory prediction without domain-specific knowledge. TribeFlow is more accurate and up to 413x faster than top competitors."
"Streamline-based quad meshing algorithms use smooth cross fields to partition surfaces into quadrilateral regions by tracing cross field separatrices. In practice, re-entrant corners and misalignment of singularities lead to small regions and limit cycles, negating some of the benefits a quad layout can provide in quad meshing. We introduce three novel methods to improve on a pipeline for coarse quad partitioning. First, we formulate an efficient method to compute high-quality cross fields on curved surfaces by extending the diffusion generated method from Viertel and Osting (SISC, 2019). Next, we introduce a method for accurately computing the trajectory of streamlines through singular triangles that prevents tangential crossings. Finally, we introduce a robust method to produce coarse quad layouts by simplifying the partitions obtained via naive separatrix tracing. Our methods are tested on a database of 100 objects and the results are analyzed. The algorithm performs well both in terms of efficiency and visual results on the database when compared to state-of-the-art methods."
"We derive a general expression for the threshold resummation of transverse momentum distributions for processes with a colorless final state, by suitably generalizing the renormalization-group based approach to threshold resummation previously pursued by two of us. The ensuing expression holds to all logarithmic orders, and it can be used to extend available results in the literature, which only hold up to the next-to-leading log (NLL) level. We check agreement of our result with the existing NLL result, as well as against the known fixed next-to-leading order results for the Higgs transverse momentum distribution in gluon fusion, and we provide explicit expressions at the next-to-next-to-leading log level."
"Multi-Access Edge Computing (MEC) is a paradigm for handling delay sensitive services that require ultra-low latency at the access network. With it, computing and communications are performed within one Base Station (BS) site, where the computation resources are in the form of Virtual Machines (VMs) (computer emulators) in the MEC server. MEC and Energy Harvesting (EH) BSs, i.e., BSs equipped with EH equipments, are foreseen as a key towards next-generation mobile networks. In fact, EH systems are expected to decrease the energy drained from the electricity grid and facilitate the deployment of BSs in remote places, extending network coverage and making energy self-sufficiency possible in remote/rural sites. In this paper, we propose an online optimization algorithm called ENergy Aware and Adaptive Management (ENAAM), for managing remote BS sites through foresighted control policies exploiting (short-term) traffic load and harvested energy forecasts. Our numerical results reveal that ENAAM achieves energy savings with respect to the case where no energy management is applied, ranging from 56% to 66% through the scaling of computing resources, and keeps the server utilization factor between 30% and 96% over time (with an average of 75%). Notable benefits are also found against heuristic energy management techniques."
"Deep learning has achieved state-of-the-art accuracies on several computer vision tasks. However, the computational and energy requirements associated with training such deep neural networks can be quite high. In this paper, we propose a cumulative training strategy with Net2Net transformation that achieves training computational efficiency without incurring large accuracy loss, in comparison to a model trained from scratch. We achieve this by first training a small network (with lesser parameters) on a small subset of the original dataset, and then gradually expanding the network using Net2Net transformation to train incrementally on larger subsets of the dataset. This incremental training strategy with Net2Net utilizes function-preserving transformations that transfers knowledge from each previous small network to the next larger network, thereby, reducing the overall training complexity. Our experiments demonstrate that compared with training from scratch, cumulative training yields ~2x reduction in computational complexity for training TinyImageNet using VGG19 at iso-accuracy. Besides training efficiency, a key advantage of our cumulative training strategy is that we can perform pruning during Net2Net expansion to obtain a final network with optimal configuration (~0.4x lower inference compute complexity) compared to conventional training from scratch. We also demonstrate that the final network obtained from cumulative training yields better generalization performance and noise robustness. Further, we show that mutual inference from all the networks created with cumulative Net2Net expansion enables improved adversarial input detection."
"With the increasing size of quantum processors, sub-modules that constitute the processor hardware will become too large to accurately simulate on a classical computer. Therefore, one would soon have to fabricate and test each new design primitive and parameter choice in time-consuming coordination between design, fabrication, and experimental validation. Here we show how one can design and test the performance of next-generation quantum hardware by using existing quantum computers. Focusing on superconducting transmon processors as a prominent hardware platform, we compute the static and dynamic properties of individual and coupled transmons. We show how the energy spectra of transmons can be obtained by variational hybrid quantum-classical algorithms that are well-suited for near-term noisy quantum computers. In addition, single- and two-qubit gate simulations are demonstrated via Suzuki-Trotter decomposition. Our methods pave a new way towards designing candidate quantum processors when the demands of calculating sub-module properties exceed the capabilities of classical computing resources."
"Today's deep learning models are primarily trained on CPUs and GPUs. Although these models tend to have low error, they consume high power and utilize large amount of memory owing to double precision floating point learning parameters. Beyond the Moore's law, a significant portion of deep learning tasks would run on edge computing systems, which will form an indispensable part of the entire computation fabric. Subsequently, training deep learning models for such systems will have to be tailored and adopted to generate models that have the following desirable characteristics: low error, low memory, and low power. We believe that deep neural networks (DNNs), where learning parameters are constrained to have a set of finite discrete values, running on neuromorphic computing systems would be instrumental for intelligent edge computing systems having these desirable characteristics. To this extent, we propose the Combinatorial Neural Network Training Algorithm (CoNNTrA), that leverages a coordinate gradient descent-based approach for training deep learning models with finite discrete learning parameters. Next, we elaborate on the theoretical underpinnings and evaluate the computational complexity of CoNNTrA. As a proof of concept, we use CoNNTrA to train deep learning models with ternary learning parameters on the MNIST, Iris and ImageNet data sets and compare their performance to the same models trained using Backpropagation. We use following performance metrics for the comparison: (i) Training error; (ii) Validation error; (iii) Memory usage; and (iv) Training time. Our results indicate that CoNNTrA models use 32x less memory and have errors at par with the Backpropagation models."
"While the capacity, feasibility and methods to obtain codes for network coding problems are well studied, the decoding procedure and complexity have not garnered much attention. In this work, we pose the decoding problem at a sink node in a network as a marginalize a product function (MPF) problem over a Boolean semiring and use the sum-product (SP) algorithm on a suitably constructed factor graph to perform iterative decoding. We use \textit{traceback} to reduce the number of operations required for SP decoding at sink node with general demands and obtain the number of operations required for decoding using SP algorithm with and without traceback. For sinks demanding all messages, we define \textit{fast decodability} of a network code and identify a sufficient condition for the same. Next, we consider the in-network function computation problem wherein the sink nodes do not demand the source messages, but are only interested in computing a function of the messages. We present an MPF formulation for function computation at the sink nodes in this setting and use the SP algorithm to obtain the value of the demanded function. The proposed method can be used for both linear and nonlinear as well as scalar and vector codes for both decoding of messages in a network coding problem and computing linear and nonlinear functions in an in-network function computation problem."
"The key cryptographic protocols used to secure the internet and financial transactions of today are all susceptible to attack by the development of a sufficiently large quantum computer. One particular area at risk are cryptocurrencies, a market currently worth over 150 billion USD. We investigate the risk of Bitcoin, and other cryptocurrencies, to attacks by quantum computers. We find that the proof-of-work used by Bitcoin is relatively resistant to substantial speedup by quantum computers in the next 10 years, mainly because specialized ASIC miners are extremely fast compared to the estimated clock speed of near-term quantum computers. On the other hand, the elliptic curve signature scheme used by Bitcoin is much more at risk, and could be completely broken by a quantum computer as early as 2027, by the most optimistic estimates. We analyze an alternative proof-of-work called Momentum, based on finding collisions in a hash function, that is even more resistant to speedup by a quantum computer. We also review the available post-quantum signature schemes to see which one would best meet the security and efficiency requirements of blockchain applications."
"Microsimulation models (MSMs) are used to predict population-level effects of health care policies by simulating individual-level outcomes. Simulated outcomes are governed by unknown parameters that are chosen so that the model accurately predicts specific targets, a process referred to as model calibration. Calibration targets can come from randomized controlled trials, observational studies, and expert opinion, and are typically summary statistics. A well calibrated model can reproduce a wide range of targets. MSM calibration generally involves searching a high dimensional parameter space and predicting many targets through model simulation. This requires efficient methods for exploring the parameter space and sufficient computational resources. We develop Incremental Mixture Approximate Bayesian Computation (IMABC) as a method for MSM calibration and implement it via a high-performance computing workflow, which provides the necessary computational scale. IMABC begins with a rejection-based approximate Bayesian computation (ABC) step, drawing a sample of parameters from the prior distribution and simulating calibration targets. Next, the sample is iteratively updated by drawing additional points from a mixture of multivariate normal distributions, centered at the points that yield simulated targets that are near observed targets. Posterior estimates are obtained by weighting sampled parameter vectors to account for the adaptive sampling scheme. We demonstrate IMABC by calibrating a MSM for the natural history of colorectal cancer to obtain simulated draws from the joint posterior distribution of model parameters."
"The N = 2, 4 superconformal symmetry constraints in d = 4 for four point functions of chiral primary 1/2-BPS operators are derived. The operators are described by symmetric traceless tensors of the internal R-symmetry group. A substantial simplification is achieved by introduction of null vectors. Two variable polynomials corresponding to different R-symmetry representations are constructed. The Ward identities for superconformal symmetry are obtained as simple differential equations. The general solution is presented in terms of a constant, a single variable function and a two variable function. An interpretation in terms of the operator product expansion is given for the case of fields of equal dimension and for the so called (next-to)extremal cases. The result is shown to accommodate long multiplets, semishort and short multiplets with protected dimension. Generically also non-unitary multiplets can appear. It is shown how to remove them to obtain a unitary theory. Implications of crossing symmetry for the four point functions studied are derived and discussed. It is shown that crossing symmetry fixes the single variable function in the general solution to be of free field form using singularity arguments. For a restricted set of next-to-extremal correlation functions with S3 symmetry amongst the first three fields it is shown that the amplitude is fixed up to normalization to free field form. We compute the conformal partial wave expansion of all representations in this amplitude and compute an averaged value of the anomalous dimensions for long multiplets given spin and twist in each relevant representation at first order in 1/N. Finally assuming the universal singularity structure we derive the general large N amplitude of four identical 1/2-BPS operators in the [0, p, 0] representation in terms of D functions. Explicit expressions for all coefficients are given."
"The fifth-generation mobile evolution enables several transformations on Next Generation Radio Access Networks (NG-RAN). The RAN protocol stack is splitting into eight possible disaggregated options combined into three network units, i.e., Central, Distributed, and Radio. Besides that, further advances allow the RAN software to be virtualized on top of general-purpose vendor-neutral hardware, dealing with the concept of virtualized RAN (vRAN). The disaggregated network units initiatives reach full interoperability based on the Open RAN (O-RAN). The combination of NG-RAN and vRAN results in vNG-RAN, enabling the management of disaggregated units and protocols as a set of radio functions. The placement of these functions is challenging since the best decision can be based on multiple constraints, such as the RAN protocol stack split, routing paths of transport networks with restricted bandwidth and latency requirements, different topologies and link capabilities, asymmetric computational resources, etc. This article proposes the first exact model for the placement optimization of radio functions for vNG-RAN planning, named PlaceRAN. The main objective is to minimize the computing resources and maximize the aggregation of radio functions. The PlaceRAN evaluation considered two realistic network topologies. Our results reveal that the PlaceRAN model achieves an optimized high-performance aggregation level, it is flexible for RAN deployment overcoming the network restrictions, and it is up to date with the most advanced vNG-RAN design and development."
"Normalized double-differential cross sections for top quark pair (t t-bar) production are measured in pp collisions at a centre-of-mass energy of 8 TeV with the CMS experiment at the LHC. The analyzed data correspond to an integrated luminosity of 19.7 inverse femtobarns. The measurement is performed in the dilepton e+/- mu-/+ final state. The t t-bar cross section is determined as a function of various pairs of observables characterizing the kinematics of the top quark and t t-bar system. The data are compared to calculations using perturbative quantum chromodynamics at next-to-leading and approximate next-to-next-to-leading orders. They are also compared to predictions of Monte Carlo event generators that complement fixed-order computations with parton showers, hadronization, and multiple-parton interactions. Overall agreement is observed with the predictions, which is improved when the latest global sets of proton parton distribution functions are used. The inclusion of the measured t t-bar cross sections in a fit of parametrized parton distribution functions is shown to have significant impact on the gluon distribution."
"With an ever-increasing number of mobile devices competing for our attention, quantifying when, how often, or for how long users visually attend to their devices has emerged as a core challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying visual attention during everyday mobile interactions. We identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head pose estimation, and the need for accurate gaze estimation. Based on this analysis, we propose future research directions and discuss how eye contact detection represents the foundation for exciting new applications towards next-generation pervasive attentive user interfaces."
"We compute the PPN parameters $\gamma$ and $\beta$ for general scalar-tensor theories in the Einstein frame, which we compare to the existing PPN formulation in the Jordan frame for alternative theories of gravity. This computation is important for scalar-tensor theories that are expressed in the Einstein frame, such as chameleon and symmetron theories, which can incorporate hiding mechanisms that predict environment-dependent PPN parameters. We introduce a general formalism for scalar-tensor theories and constrain it using the limit on $\gamma$ given by the Cassini experiment. In particular we discuss massive Brans-Dicke scalar fields for extended sources. Next, using a recently proposed Earth satellite experiment, in which atomic clocks are used for spacecraft tracking, we compute the observable perturbations in the redshift induced by PPN parameters deviating from their general relativistic values. Our estimates suggest that $|\gamma - 1| \sim |\beta -1| \sim 10^{-6}$ may be detectable by a satellite that carries a clock with fractional frequency uncertainty $\Delta f/f \sim 10^{-16}$ in an eccentric orbit around the Earth. Such space experiments are within reach of existing atomic clock technology. We discuss further the requirements necessary for such a mission to detect deviations from Einstein relativity."
"Unsupervised learning poses one of the most difficult challenges in computer vision today. The task has an immense practical value with many applications in artificial intelligence and emerging technologies, as large quantities of unlabeled videos can be collected at relatively low cost. In this paper, we address the unsupervised learning problem in the context of detecting the main foreground objects in single images. We train a student deep network to predict the output of a teacher pathway that performs unsupervised object discovery in videos or large image collections. Our approach is different from published methods on unsupervised object discovery. We move the unsupervised learning phase during training time, then at test time we apply the standard feed-forward processing along the student pathway. This strategy has the benefit of allowing increased generalization possibilities during training, while remaining fast at testing. Our unsupervised learning algorithm can run over several generations of student-teacher training. Thus, a group of student networks trained in the first generation collectively create the teacher at the next generation. In experiments our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation and saliency detection. At test time the proposed system is fast, being one to two orders of magnitude faster than published unsupervised methods."
"Owing to the development and advancement of artificial intelligence, numerous works were established in the human facial expression recognition system. Meanwhile, the detection and classification of micro-expressions are attracting attentions from various research communities in the recent few years. In this paper, we first review the processes of a conventional optical-flow-based recognition system, which comprised of facial landmarks annotations, optical flow guided images computation, features extraction and emotion class categorization. Secondly, a few approaches have been proposed to improve the feature extraction part, such as exploiting GAN to generate more image samples. Particularly, several variations of optical flow are computed in order to generate optimal images to lead to high recognition accuracy. Next, GAN, a combination of Generator and Discriminator, is utilized to generate new ""fake"" images to increase the sample size. Thirdly, a modified state-of-the-art Convolutional neural networks is proposed. To verify the effectiveness of the the proposed method, the results are evaluated on spontaneous micro-expression databases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy performance metrics are reported in this paper."
"Variational autoencoders (VAEs) are powerful generative models with the salient ability to perform inference. Here, we introduce a quantum variational autoencoder (QVAE): a VAE whose latent generative process is implemented as a quantum Boltzmann machine (QBM). We show that our model can be trained end-to-end by maximizing a well-defined loss-function: a 'quantum' lower-bound to a variational approximation of the log-likelihood. We use quantum Monte Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To achieve the best performance, we first create a VAE platform with discrete latent space generated by a restricted Boltzmann machine (RBM). Our model achieves state-of-the-art performance on the MNIST dataset when compared against similar approaches that only involve discrete variables in the generative process. We consider QVAEs with a smaller number of latent units to be able to perform QMC simulations, which are computationally expensive. We show that QVAEs can be trained effectively in regimes where quantum effects are relevant despite training via the quantum bound. Our findings open the way to the use of quantum computers to train QVAEs to achieve competitive performance for generative models. Placing a QBM in the latent space of a VAE leverages the full potential of current and next-generation quantum computers as sampling devices."
"The task of multi-image cued story generation, such as visual storytelling dataset (VIST) challenge, is to compose multiple coherent sentences from a given sequence of images. The main difficulty is how to generate image-specific sentences within the context of overall images. Here we propose a deep learning network model, GLAC Net, that generates visual stories by combining global-local (glocal) attention and context cascading mechanisms. The model incorporates two levels of attention, i.e., overall encoding level and image feature level, to construct image-dependent sentences. While standard attention configuration needs a large number of parameters, the GLAC Net implements them in a very simple way via hard connections from the outputs of encoders or image features onto the sentence generators. The coherency of the generated story is further improved by conveying (cascading) the information of the previous sentence to the next sentence serially. We evaluate the performance of the GLAC Net on the visual storytelling dataset (VIST) and achieve very competitive results compared to the state-of-the-art techniques. Our code and pre-trained models are available here."
"We consider the Generalized Trust Region Subproblem (GTRS) of minimizing a nonconvex quadratic objective over a nonconvex quadratic constraint. A lifting of this problem recasts the GTRS as minimizing a linear objective subject to two nonconvex quadratic constraints. Our first main contribution is structural: we give an explicit description of the convex hull of this nonconvex set in terms of the generalized eigenvalues of an associated matrix pencil. This result may be of interest in building relaxations for nonconvex quadratic programs. Moreover, this result allows us to reformulate the GTRS as the minimization of two convex quadratic functions in the original space. Our next set of contributions is algorithmic: we present an algorithm for solving the GTRS up to an epsilon additive error based on this reformulation. We carefully handle numerical issues that arise from inexact generalized eigenvalue and eigenvector computations and establish explicit running time guarantees for these algorithms. Notably, our algorithms run in linear (in the size of the input) time. Furthermore, our algorithm for computing an epsilon-optimal solution has a slightly-improved running time dependence on epsilon over the state-of-the-art algorithm. Our analysis shows that the dominant cost in solving the GTRS lies in solving a generalized eigenvalue problem -- establishing a natural connection between these problems. Finally, generalizations of our convex hull results allow us to apply our algorithms and their theoretical guarantees directly to equality-, interval-, and hollow- constrained variants of the GTRS. This gives the first linear-time algorithm in the literature for these variants of the GTRS."
"Methods for generating synthetic data have become of increasing importance to build large datasets required for Convolution Neural Networks (CNN) based deep learning techniques for a wide range of computer vision applications. In this work, we extend existing methodologies to show how 2D thermal facial data can be mapped to provide 3D facial models. For the proposed research work we have used tufts datasets for generating 3D varying face poses by using a single frontal face pose. The system works by refining the existing image quality by performing fusion based image preprocessing operations. The refined outputs have better contrast adjustments, decreased noise level and higher exposedness of the dark regions. It makes the facial landmarks and temperature patterns on the human face more discernible and visible when compared to original raw data. Different image quality metrics are used to compare the refined version of images with original images. In the next phase of the proposed study, the refined version of images is used to create 3D facial geometry structures by using Convolution Neural Networks (CNN). The generated outputs are then imported in blender software to finally extract the 3D thermal facial outputs of both males and females. The same technique is also used on our thermal face data acquired using prototype thermal camera (developed under Heliaus EU project) in an indoor lab environment which is then used for generating synthetic 3D face data along with varying yaw face angles and lastly facial depth map is generated."
"The next decade promises an observational revolution which will change cosmology forever. The precise measurement of the angular anisotropy of the cosmic microwave background should specify to a few percent all of the parameters of the cosmological model which effect astrophysics. The growth of structure will then be determined (but not yet observed) until gravitational collapse becomes highly non-linear and stars, galaxies and active galactic nuclei (AGN) form. These processes are hard to model with basic physics because they are complex and allow a rich variety of expression. Instead observations will determine when the first stars and quasars formed, and how and when galaxies assembled. If we can reconcile the numerous contradictions which characterize the subject today, cosmology will become a mature subject, founded on the agreement between detailed, inclusive and realistic models, which make precise predictions, and the wealth of new data which will come from a wide variety of observations, at all wavelengths. This is an ambitious schedule, but nothing less is worthy of the outstanding capabilities of the 8 -- 10 m telescopes, the next generation space telescope, the opportunities at millimeterto sub-millimeter wavelengths and advanced computer modeling. The ESO Very Large Telescope (VLT) should play a major role in this revolution."
"Given the extremely high output rate foreseen at LHC and the general-purpose nature of ATLAS experiment, an efficient and flexible way to select events in the High Level Trigger is needed. An extremely flexible solution is proposed that allows for early rejection of unwanted events and an easily configurable way to choose algorithms and to specify the criteria for trigger decisions. It is implemented in the standard ATLAS object-oriented software framework, Athena. The early rejection is achieved by breaking the decision process down into sequential steps. The configuration of each step defines sequences of algorithms which should be used to process the data, and 'trigger menus' that define which physics signatures must be satisfied to continue on to the next step, and ultimately to accept the event. A navigation system has been built on top of the standard Athena transient store (StoreGate) to link the event data together in a tree-like structure. This is fundamental to the seeding mechanism, by which data from one step is presented to the next. The design makes it straightforward to utilize existing off-line reconstruction data classes and algorithms when they are suitable"
"We compute non-perturbatively the renormalization constants of composite operators on a quenched $16^3 \times 28 $ lattice with lattice spacing $a$ = 0.20 fm for the overlap fermion by using the regularization independent (RI) scheme. The quenched gauge configurations were generated with the Iwasaki action. We test the relations $Z_A = Z_V$ and $ Z_S=Z_P$ and find that they agree well {(less than 1%)} above $\mu$ = 1.6 GeV. %even for our lattice with a coarse lattice spacing. We also perform a Renormalization Group (RG) analysis at the next-to-next-to-leading order and match the renormalization constants to the $\bar{\rm MS}$ scheme. The wave-function renormalization $Z_{\psi}$ is determined from the vertex function of the axial current and $Z_A$ from the chiral Ward identity. Finally, we examine the finite quark mass behavior for the renormalization factors of the quark bilinear operators. We find that the $(pa)^2$ errors of the vertex functions are small and the quark mass dependence of the renormalization factors to be quite weak."
"We present a computational screening of experimental structural repositories for fast Li-ion conductors, with the goal of finding new candidate materials for application as solid-state electrolytes in next-generation batteries. We start from ~1400 unique Li-containing materials, of which ~900 are insulators at the level of density-functional theory. For those, we calculate the diffusion coefficient in a highly automated fashion, using extensive molecular dynamics simulations on a potential energy surface (the recently published pinball model) fitted on first-principles forces. The ~130 most promising candidates are studied with full first-principles molecular dynamics, first at high temperature and then more extensively for the 78 most promising candidates. The results of the first-principles simulations of the candidate solid-state electrolytes found are discussed in detail."
"We compute the nonequilibrium real-time evolution of an O(N)-symmetric scalar quantum field theory from a systematic 1/N expansion of the 2PI effective action to next-to-leading order, which includes scattering and memory effects. In contrast to the standard 1/N expansion of the 1PI effective action, the next-to-leading order expansion in presence of a possible expectation value for the composite operator leads to a bounded time evolution where the truncation error may be controlled by higher powers in 1/N. We present a detailed comparison with the leading-order results and determine the range of validity of standard mean field type approximations.   We investigate ``quench'' and ``tsunami'' initial conditions frequently used to mimic idealized far-from-equilibrium pion dynamics in the context of heavy-ion collisions. For spatially homogeneous initial conditions we find three generic regimes, characterized by an early-time exponential damping, a parametrically slow (power-law) behavior at intermediate times, and a late-time exponential approach to thermal equilibrium. The different time scales are obtained from a numerical solution of the time-reversal invariant equations in 1+1 dimensions without further approximations. We discuss in detail the out-of-equilibrium behavior of the nontrivial n-point correlation functions as well as the evolution of a particle number distribution and inverse slope parameter."
"We believe the Babcock--Leighton process of poloidal field generation to be the main source of irregularity in the solar cycle. The random nature of this process may make the poloidal field in one hemisphere stronger than that in the other hemisphere at the end of a cycle. We expect this to induce an asymmetry in the next sunspot cycle. We look for evidence of this in the observational data and then model it theoretically with our dynamo code. Since actual polar field measurements exist only from 1970s, we use the polar faculae number data recorded by Sheeley (1991) as a proxy of the polar field and estimate the hemispheric asymmetry of the polar field in different solar minima during the major part of the twentieth century. This asymmetry is found to have a reasonable correlation with the asymmetry of the next cycle. We then run our dynamo code by feeding information about this asymmetry at the successive minima and compare with observational data. We find that the theoretically computed asymmetries of different cycles compare favourably with the observational data, the correlation coefficient being 0.73. Due to the coupling between the two hemispheres, any hemispheric asymmetry tends to get attenuated with time. The hemispheric asymmetry of a cycle either from observational data or from theoretical calculation statistically tends to be less than the asymmetry in the polar field (as inferred from the faculae data) in the preceding minimum. This reduction factor turns out to be 0.38 and 0.60 respectively in observational data and theoretical simulation."
"Under certain conditions, sound waves in a fluid may be governed by a Klein-Gordon equation on an `effective spacetime' determined by the background flow properties. Here we consider the draining bathtub: a circulating, draining flow whose effective spacetime shares key features with the rotating black hole (Kerr) spacetime. We present a complete investigation of the role of quasinormal (QN) mode and Regge pole (RP) resonances of this system. First, we simulate a perturbation in the time domain by applying a finite-difference method, to demonstrate the ubiquity of `QN ringing'. Next, we solve the wave equation in the frequency domain with the continued-fraction method, to compute QN and RP spectra numerically. We then explore the geometric link between (prograde and retrograde) null geodesic orbits on the spacetime, and the properties of the QN/RP spectra. We develop a `geodesic expansion' method which leads to asymptotic expressions (in inverse powers of mode number $m$) for the spectra, the radial functions and the residues. Next, the role of the Regge poles in scattering and absorption processes is revealed through application of the complex angular momentum method. We elucidate the link between the Regge poles and oscillations in the absorption cross section. Finally, we show that Regge poles provide a neat explanation for `orbiting' oscillations seen in the scattering cross section."
"The gravitational-wave signal from inspiralling neutron-star--neutron-star (or black-hole--neutron-star) binaries will be influenced by tidal coupling in the system. An important science goal in the gravitational-wave detection of these systems is to obtain information about the equation of state of neutron star matter via the measurement of the tidal polarizability parameters of neutron stars. To extract this piece of information will require to have accurate analytical descriptions of both the motion and the radiation of tidally interacting binaries. We improve the analytical description of the late inspiral dynamics by computing the next-to-next-to-leading order relativistic correction to the tidal interaction energy. Our calculation is based on an effective-action approach to tidal interactions, and on its transcription within the effective-one-body formalism. We find that second-order relativistic effects (quadratic in the relativistic gravitational potential $u=G(m_1 +m_2)/(c^2 r)$) significantly increase the effective tidal polarizability of neutron stars by a distance-dependent amplification factor of the form $1 + \alpha_1 \, u + \alpha_2 \, u^2 +...$ where, say for an equal-mass binary, $\alpha_1=5/4=1.25$ (as previously known) and $\alpha_2=85/14\simeq6.07143$ (as determined here for the first time). We argue that higher-order relativistic effects will lead to further amplification, and we suggest a Pad\'e-type way of resumming them. We recommend to test our results by comparing resolution-extrapolated numerical simulations of inspiralling-binary neutron stars to their effective one body description."
"Fast multipole methods have O(N) complexity, are compute bound, and require very little synchronization, which makes them a favorable algorithm on next-generation supercomputers. Their most common application is to accelerate N-body problems, but they can also be used to solve boundary integral equations. When the particle distribution is irregular and the tree structure is adaptive, load-balancing becomes a non-trivial question. A common strategy for load-balancing FMMs is to use the work load from the previous step as weights to statically repartition the next step. The authors discuss in the paper another approach based on data-driven execution to efficiently tackle this challenging load-balancing problem. The core idea consists of breaking the most time-consuming stages of the FMMs into smaller tasks. The algorithm can then be represented as a Directed Acyclic Graph (DAG) where nodes represent tasks, and edges represent dependencies among them. The execution of the algorithm is performed by asynchronously scheduling the tasks using the QUARK runtime environment, in a way such that data dependencies are not violated for numerical correctness purposes. This asynchronous scheduling results in an out-of-order execution. The performance results of the data-driven FMM execution outperform the previous strategy and show linear speedup on a quad-socket quad-core Intel Xeon system."
"We consider the relativistic scattering of unequal-mass scalar particles through graviton exchange in the small-angle high-energy regime. We show the self-consistency of expansion around the eikonal limit and compute the scattering amplitude up to the next-to-leading power correction of the light particle energy, including gravitational effects of the same order. The first power correction is suppressed by a single power of the ratio of momentum transfer to the energy of the light particle in the rest frame of the heavy particle, independent of the heavy particle mass. We find that only gravitational corrections contribute to the exponentiated phase in impact parameter space in four dimensions. For large enough heavy-particle mass, the saddle point for the impact parameter is modified compared to the leading order by a multiple of the Schwarzschild radius determined by the mass of the heavy particle, independent of the energy of the light particle."
"We describe an extension to the SOFTSUSY program that provides for the calculation of the sparticle spectrum in the Next-to-Minimal Supersymmetric Standard Model (NMSSM), where a chiral superfield that is a singlet of the Standard Model gauge group is added to the Minimal Supersymmetric Standard Model (MSSM) fields. Often, a $\mathbb{Z}_{3}$ symmetry is imposed upon the model. SOFTSUSY can calculate the spectrum in this case as well as the case where general $\mathbb{Z}_{3}$ violating (denoted as $\,\mathbf{\backslash}\mkern-11.0mu{\mathbb{Z}}_{3}$) terms are added to the soft supersymmetry breaking terms and the superpotential. The user provides a theoretical boundary condition for the couplings and mass terms of the singlet. Radiative electroweak symmetry breaking data along with electroweak and CKM matrix data are used as weak-scale boundary conditions. The renormalisation group equations are solved numerically between the weak scale and a high energy scale using a nested iterative algorithm. This paper serves as a manual to the NMSSM mode of the program, detailing the approximations and conventions used."
"In composite Higgs models the Higgs boson arises as a pseudo-Goldstone boson from a strongly-interacting sector. Fermion mass generation is possible through partial compositeness accompanied by the appearance of new heavy fermionic resonances. The Higgs couplings to the Standard Model (SM) particles and between the Higgs bosons themselves are modified with respect to the SM. Higgs pair production is sensitive to the trilinear Higgs self-coupling but also to anomalous couplings like the novel 2-Higgs-2-fermion coupling emerging in composite Higgs models. The QCD corrections to SM Higgs boson pair production are known to be large. In this paper we compute, in the limit of heavy loop particle masses, the next-to-leading order (NLO) QCD corrections to Higgs pair production in composite Higgs models without and with new heavy fermions. The relative QCD corrections are found to be almost insensitive both to the compositeness of the Higgs boson and to the details of the heavy fermion spectrum, since the leading order cross section dominantly factorizes. With the obtained results we investigate the question if, taking into account Higgs coupling constraints, new physics could first be seen in Higgs pair production. We find this to be the case in the high-luminosity option of the LHC for composite Higgs models with heavy fermions. We also investigate the invariant mass distributions at NLO QCD. While they are sensitive to the Higgs non-linearities and hence anomalous couplings, the influence of the heavy fermions is much less pronounced."
"A measurement of the double-differential inclusive jet cross section as a function of jet transverse momentum pT and absolute jet rapidity |y| is presented. The analysis is based on proton-proton collisions collected by the CMS experiment at the LHC at a centre-of-mass energy of 13 TeV. The data samples correspond to integrated luminosities of 71 and 44 inverse picobarns for |y| < 3 and 3.2 < |y| < 4.7, respectively. Jets are reconstructed with the anti-kt clustering algorithm for two jet sizes, R, of 0.7 and 0.4, in a phase space region covering jet pT up to 2 TeV and jet rapidity up to |y| = 4.7. Predictions of perturbative quantum chromodynamics at next-to-leading order precision, complemented with electroweak and nonperturbative corrections, are used to compute the absolute scale and the shape of the inclusive jet cross section. The cross section difference in R, when going to a smaller jet size of 0.4, is best described by Monte Carlo event generators with next-to-leading order predictions matched to parton showering, hadronisation, and multiparton interactions. In the phase space accessible with the new data, this measurement provides a first indication that jet physics is as well understood at sqrt(s) = 13 TeV as at smaller centre-of-mass energies."
"The results of phenomenological studies of top-quark pair production in proton-proton collisions are presented. Differential cross sections are calculated in perturbative QCD at approximate next-to-next-to-leading order ${\cal O}(\alpha_s^4)$ by using methods of threshold resummation beyond the leading logarithmic accuracy. Predictions for the single-particle inclusive kinematics are presented for transverse momentum and rapidity distributions of final-state top quarks. Uncertainties related to the description of proton structure, top-quark mass and strong coupling constant are investigated in detail. The results are compared to the recent measurements by the ATLAS and CMS collaborations at the LHC at the center of mass energy of 7 TeV. The calculation presented here is implemented in the computer code \textsc{Difftop} and can be applied to the general case of heavy-quark pair production at hadron-hadron colliders. For the first time, a fit of parton distribution functions at NNLO is performed by using the differential cross sections of top-quark pair production together with other data sets. The impact of the top-pair production on the precision of the gluon distribution at high scales is illustrated."
"Suppose a target is hidden in one of the vertices of an edge-weighted graph according to a known probability distribution. The expanding search problem asks for a search sequence of the vertices so as to minimize the expected time for finding the target, where the time for reaching the next vertex is determined by its distance to the region that was already searched. This problem has numerous applications, such as searching for hidden explosives, mining coal, and disaster relief. In this paper, we develop exact algorithms and heuristics, including a branch-and-cut procedure, a greedy algorithm with a constant-factor approximation guarantee, and a novel local search procedure based on a spanning tree neighborhood. Computational experiments show that our branch-and-cut procedure outperforms all existing methods for general instances and both heuristics compute near-optimal solutions with little computational effort."
"We present an algorithm for controlling and scheduling multiple linear time-invariant processes on a shared bandwidth limited communication network using adaptive sampling intervals. The controller is centralized and computes at every sampling instant not only the new control command for a process, but also decides the time interval to wait until taking the next sample. The approach relies on model predictive control ideas, where the cost function penalizes the state and control effort as well as the time interval until the next sample is taken. The latter is introduced in order to generate an adaptive sampling scheme for the overall system such that the sampling time increases as the norm of the system state goes to zero. The paper presents a method for synthesizing such a predictive controller and gives explicit sufficient conditions for when it is stabilizing. Further explicit conditions are given which guarantee conflict free transmissions on the network. It is shown that the optimization problem may be solved off-line and that the controller can be implemented as a lookup table of state feedback gains. Simulation studies which compare the proposed algorithm to periodic sampling illustrate potential performance gains."
"We perform the renormalization of different types of Two-Higgs-Doublet Models for the calculation of observables at next-to-leading order. In detail, we suggest four different renormalization schemes based on on-shell renormalization conditions as far as possible and on MSbar prescriptions for the remaining field-mixing parameters where no distinguished on-shell condition exists and make contact to existing schemes in the literature. In particular, we treat the tadpole diagrams in different ways and discuss issues of gauge independence and perturbative stability in the considered schemes. The renormalization group equations for the MSbar parameters are solved in each scheme, so that a consistent renormalization scale variation can be performed. We have implemented all Feynman rules including counterterms and the renormalization conditions into a FeynArts model file, so that amplitudes and squared matrix elements can be generated automatically. As an application we compute the decay of the light, CP-even Higgs boson of the Two-Higgs-Doublet Model into four fermions at next-to-leading order. The comparison of different schemes and the investigation of the renormalization scale dependence allows us to test the perturbative consistency in each of the renormalization schemes, and to get a better estimate of the theoretical uncertainty that arises due to the truncation of the perturbation series."
"An improved version of a recently developed stochastic cluster dynamics (SCD) method {[}Marian, J. and Bulatov, V. V., {\it J. Nucl. Mater.} \textbf{415} (2014) 84-95{]} is introduced as an alternative to rate theory (RT) methods for solving coupled ordinary differential equation (ODE) systems for irradiation damage simulations. SCD circumvents by design the curse of dimensionality of the variable space that renders traditional ODE-based RT approaches inefficient when handling complex defect population comprised of multiple (more than two) defect species. Several improvements introduced here enable efficient and accurate simulations of irradiated materials up to realistic (high) damage doses characteristic of next-generation nuclear systems. The first improvement is a procedure for efficiently updating the defect reaction-network and event selection in the context of a dynamically expanding reaction-network. Next is a novel implementation of the $\tau$-leaping method that speeds up SCD simulations by advancing the state of the reaction network in large time increments when appropriate. Lastly, a volume rescaling procedure is introduced to control the computational complexity of the expanding reaction-network through occasional reductions of the defect population while maintaining accurate statistics. The enhanced SCD method is then applied to model defect cluster accumulation in iron thin films subjected to triple ion-beam ($\text{Fe}^{3+}$, $\text{He}^{+}$ and $ $$\text{H\ensuremath{{}^{+}}}$$ $) irradiations, for which standard RT or spatially-resolved kinetic Monte Carlo simulations are prohibitively expensive."
"With most modern smartphones supporting wireless protocols such as Bluetooth Low Energy (BLE) or ANT+, the number of networks are growing rapidly. Therefore, collisions among multiple networks need to be considered for choosing the appropriate protocol parameters. With growing numbers of networks, simulations for estimating the collision rate become computationally very complex and lengthy. The large simulation times therefore constitute a major limitation in the analysis of complex cases. In this paper, we present a novel simulation technique which can speed up collision simulations by one order of magnitude in realistic situations. Whenever the transmission of a packet is simulated, the cyclic nature of protocols like BLE is exploited to predict the next packet that has a chance of colliding. All transmissions in between can be skipped without affecting the simulation results. Based on the transmission intervals of the networks, one can compute a certain shrinkage per cycle of the time offset between their packets. Using this shrinkage and the current offset between the starting times of any two packets, our proposed simulation model can accurately predict the next pair of packets that needs to be simulated. Whereas our proposed technique aims at the BLE protocol, the theory is generic and can be used for many other cyclic protocols such as ANT/ANT+ as well."
"In this work, we exploit the task of joint classification and weakly supervised localization of thoracic diseases from chest radiographs, with only image-level disease labels coupled with disease severity-level (DSL) information of a subset. A convolutional neural network (CNN) based attention-guided curriculum learning (AGCL) framework is presented, which leverages the severity-level attributes mined from radiology reports. Images in order of difficulty (grouped by different severity-levels) are fed to CNN to boost the learning gradually. In addition, highly confident samples (measured by classification probabilities) and their corresponding class-conditional heatmaps (generated by the CNN) are extracted and further fed into the AGCL framework to guide the learning of more distinctive convolutional features in the next iteration. A two-path network architecture is designed to regress the heatmaps from selected seed samples in addition to the original classification task. The joint learning scheme can improve the classification and localization performance along with more seed samples for the next iteration. We demonstrate the effectiveness of this iterative refinement framework via extensive experimental evaluations on the publicly available ChestXray14 dataset. AGCL achieves over 5.7\% (averaged over 14 diseases) increase in classification AUC and 7%/11% increases in Recall/Precision for the localization task compared to the state of the art."
"We use an effective field theory (EFT) approach to calculate the next to leading order (NLO) gravitational spin-orbit interaction between two spinning compact objects. The NLO spin-orbit interaction provides the most computationally complex sector of the NLO spin effects, previously derived within the EFT approach. In particular, it requires the inclusion of non-stationary cubic self-gravitational interaction, as well as the implementation of a spin supplementary condition (SSC) at higher orders. The EFT calculation is carried out in terms of the non-relativistic gravitational field parametrization, making the calculation more efficient with no need to rely on automated computations, and illustrating the coupling hierarchy of the different gravitational field components to the spin and mass sources. Finally, we show explicitly how to relate the EFT derived spin results to the canonical results obtained with the ADM Hamiltonian formalism. This is done using non-canonical transformations, required due to the implementation of covariant SSC, as well as canonical transformations at the level of the Hamiltonian, with no need to resort to the equations of motion or the Dirac brackets."
"We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the proposed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatiotemporal dynamics for pixel-level future prediction in natural videos."
"Deep Neural Networks (DNNs) have been shown to be vulnerable against adversarial examples, which are data points cleverly constructed to fool the classifier. Such attacks can be devastating in practice, especially as DNNs are being applied to ever increasing critical tasks like image recognition in autonomous driving. In this paper, we introduce a new perspective on the problem. We do so by first defining robustness of a classifier to adversarial exploitation. Next, we show that the problem of adversarial example generation can be posed as learning problem. We also categorize attacks in literature into high and low perturbation attacks; well-known attacks like fast-gradient sign method (FGSM) and our attack produce higher perturbation adversarial examples while the more potent but computationally inefficient Carlini-Wagner (CW) attack is low perturbation. Next, we show that the dual approach of the attack learning problem can be used as a defensive technique that is effective against high perturbation attacks. Finally, we show that a classifier masking method achieved by adding noise to the a neural network's logit output protects against low distortion attacks such as the CW attack. We also show that both our learning and masking defense can work simultaneously to protect against multiple attacks. We demonstrate the efficacy of our techniques by experimenting with the MNIST and CIFAR-10 datasets."
"The Normalized Relative Compression (NRC) is a recent dissimilarity measure, related to the Kolmogorov Complexity. It has been successfully used in different applications, like DNA sequences, images or even ECG (electrocardiographic) signal. It uses a compressor that compresses a target string using exclusively the information contained in a reference string. One possible approach is to use finite-context models (FCMs) to represent the strings. A finite-context model calculates the probability distribution of the next symbol, given the previous $k$ symbols. In this paper, we introduce a generalization of the FCMs, called extended-alphabet finite-context models (xaFCM), that calculates the probability of occurrence of the next $d$ symbols, given the previous $k$ symbols. We perform experiments on two different sample applications using the xaFCMs and the NRC measure: ECG biometric identification, using a publicly available database; estimation of the similarity between DNA sequences of two different, but related, species -- chromosome by chromosome. In both applications, we compare the results against those obtained by the FCMs. The results show that the xaFCMs use less memory and computational time to achieve the same or, in some cases, even more accurate results."
"In this paper, we propose a novel framework for performance optimization in Internet of Things (IoT)-based next-generation wireless sensor networks. In particular, a computationally-convenient system is presented to combat two major research problems in sensor networks. First is the conventionally-tackled resource optimization problem which triggers the drainage of battery at a faster rate within a network. Such drainage promotes inefficient resource usage thereby causing sudden death of the network. The second main bottleneck for such networks is that of data degradation. This is because the nodes in such networks communicate via a wireless channel, where the inevitable presence of noise corrupts the data making it unsuitable for practical applications. Therefore, we present a layer-adaptive method via 3-tier communication mechanism to ensure the efficient use of resources. This is supported with a mathematical coverage model that deals with the formation of coverage holes. We also present a transform-domain based robust algorithm to effectively remove the unwanted components from the data. Our proposed framework offers a handy algorithm that enjoys desirable complexity for real-time applications as shown by the extensive simulation results."
"Tasks related to Natural Language Processing (NLP) have recently been the focus of a large research endeavor by the machine learning community. The increased interest in this area is mainly due to the success of deep learning methods. Genetic Programming (GP), however, was not under the spotlight with respect to NLP tasks. Here, we propose a first proof-of-concept that combines GP with the well established NLP tool word2vec for the next word prediction task. The main idea is that, once words have been moved into a vector space, traditional GP operators can successfully work on vectors, thus producing meaningful words as the output. To assess the suitability of this approach, we perform an experimental evaluation on a set of existing newspaper headlines. Individuals resulting from this (pre-)training phase can be employed as the initial population in other NLP tasks, like sentence generation, which will be the focus of future investigations, possibly employing adversarial co-evolutionary approaches."
"Major chip manufacturers have all introduced multicore microprocessors. Multi-socket systems built from these processors are routinely used for running various server applications. Depending on the application that is run on the system, remote memory accesses can impact overall performance. This paper presents a new operating system (OS) scheduling optimization to reduce the impact of such remote memory accesses. By observing the pattern of local and remote DRAM accesses for every thread in each scheduling quantum and applying different algorithms, we come up with a new schedule of threads for the next quantum. This new schedule potentially cuts down remote DRAM accesses for the next scheduling quantum and improves overall performance. We present three such new algorithms of varying complexity followed by an algorithm which is an adaptation of Hungarian algorithm. We used three different synthetic workloads to evaluate the algorithm. We also performed sensitivity analysis with respect to varying DRAM latency. We show that these algorithms can cut down DRAM access latency by up to 55% depending on the algorithm used. The benefit gained from the algorithms is dependent upon their complexity. In general higher the complexity higher is the benefit. Hungarian algorithm results in an optimal solution. We find that two out of four algorithms provide a good trade-off between performance and complexity for the workloads we studied."
"We introduce Bayesian QuickNAT for the automated quality control of whole-brain segmentation on MRI T1 scans. Next to the Bayesian fully convolutional neural network, we also present inherent measures of segmentation uncertainty that allow for quality control per brain structure. For estimating model uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC) samples from the posterior distribution are generated by keeping the dropout layers active at test time. Entropy over the MC samples provides a voxel-wise model uncertainty map, whereas expectation over the MC predictions provides the final segmentation. Next to voxel-wise uncertainty, we introduce four metrics to quantify structure-wise uncertainty in segmentation for quality control. We report experiments on four out-of-sample datasets comprising of diverse age range, pathology and imaging artifacts. The proposed structure-wise uncertainty metrics are highly correlated with the Dice score estimated with manual annotation and therefore present an inherent measure of segmentation quality. In particular, the intersection over union over all the MC samples is a suitable proxy for the Dice score. In addition to quality control at scan-level, we propose to incorporate the structure-wise uncertainty as a measure of confidence to do reliable group analysis on large data repositories. We envisage that the introduced uncertainty metrics would help assess the fidelity of automated deep learning based segmentation methods for large-scale population studies, as they enable automated quality control and group analyses in processing large data repositories."
"Multiple-input multiple-output (MIMO) millimeter wave (mmWave) communication is a key technology for next generation wireless networks. One of the consequences of utilizing a large number of antennas with an increased bandwidth is that array steering vectors vary among different subcarriers. Due to this effect, known as beam squint, the conventional channel model is no longer applicable for mmWave massive MIMO systems. In this paper, we study channel estimation under the resulting non-standard model. To that aim, we first analyze the beam squint effect from an array signal processing perspective, resulting in a model which sheds light on the angle-delay sparsity of mmWave transmission. We next design a compressive sensing based channel estimation algorithm which utilizes the shift-invariant block-sparsity of this channel model. The proposed algorithm jointly computes the off-grid angles, the off-grid delays, and the complex gains of the multi-path channel. We show that the newly proposed scheme reflects the mmWave channel more accurately and results in improved performance compared to traditional approaches. We then demonstrate how this approach can be applied to recover both the uplink as well as the downlink channel in frequency division duplex (FDD) systems, by exploiting the angle-delay reciprocity of mmWave channels."
"A trustworthy estimate of the redshift distribution $n(z)$ is crucial for using weak gravitational lensing and large-scale structure of galaxy catalogs to study cosmology. Spectroscopic redshifts for the dim and numerous galaxies of next-generation weak-lensing surveys are expected to be unavailable, making photometric redshift (photo-$z$) probability density functions (PDFs) the next-best alternative for comprehensively encapsulating the nontrivial systematics affecting photo-$z$ point estimation. The established stacked estimator of $n(z)$ avoids reducing photo-$z$ PDFs to point estimates but yields a systematically biased estimate of $n(z)$ that worsens with decreasing signal-to-noise, the very regime where photo-$z$ PDFs are most necessary. We introduce Cosmological Hierarchical Inference with Probabilistic Photometric Redshifts (CHIPPR), a statistically rigorous probabilistic graphical model of redshift-dependent photometry, which correctly propagates the redshift uncertainty information beyond the best-fit estimator of $n(z)$ produced by traditional procedures and is provably the only self-consistent way to recover $n(z)$ from photo-$z$ PDFs. We present the $\texttt{chippr}$ prototype code, noting that the mathematically justifiable approach incurs computational expense. The CHIPPR approach is applicable to any one-point statistic of any random variable, provided the prior probability density used to produce the posteriors is explicitly known; if the prior is implicit, as may be the case for popular photo-$z$ techniques, then the resulting posterior PDFs cannot be used for scientific inference. We therefore recommend that the photo-$z$ community focus on developing methodologies that enable the recovery of photo-$z$ likelihoods with support over all redshifts, either directly or via a known prior probability density."
"The state-of-the-art in wind-farm flow-physics modeling is Large Eddy Simulation (LES) which makes accurate predictions of most relevant physics, but requires extensive computational resources. The next-fidelity model types are Reynolds-Averaged Navier-Stokes (RANS) which are two orders of magnitude cheaper, but resolve only mean quantities and model the effect of turbulence. They often fail to accurately predict key effects, such as the wake recovery rate. Custom RANS closures designed for wind-farm wakes exist, but so far do not generalize well: there is substantial room for improvement. In this article we present the first steps towards a systematic data-driven approach to deriving new RANS models in the wind-energy setting. Time-averaged LES data is used as ground-truth, and we first derive optimal corrective fields for the turbulence anisotropy tensor and turbulence kinetic energy (t.k.e.) production. These fields, when injected into the RANS equations (with a baseline $k-\epsilon$ model) reproduce the LES mean-quantities. Next we build a custom RANS closure from these corrective fields, using a deterministic symbolic regression method to infer algebraic correction as a function of the (resolved) mean-flow. The result is a new RANS closure, customized to the training data. The potential of the approach is demonstrated under neutral atmospheric conditions for multi-turbine constellations at wind-tunnel scale. The results show significantly improved predictions compared to the baseline closure, for both mean velocity and the t.k.e. fields."
"The United Nations Broadband Commission has committed the international community to accelerate universal broadband. However, the cost of meeting this objective, and the feasibility of doing so on a commercially viable basis, are not well understood. Using scenario analysis, this paper compares the global cost-effectiveness of different infrastructure strategies for the developing world to achieve universal 4G or 5G mobile broadband. Utilizing remote sensing and demand forecasting, least-cost network designs are developed for eight representative low and middle-income countries (Malawi, Uganda, Kenya, Senegal, Pakistan, Albania, Peru and Mexico), the results from which form the basis for aggregation to the global level. The cost of meeting a minimum 10 Mbps per user is estimated at USD 1.7 trillion using 5G Non-Standalone, approximately 0.6% of annual GDP for the developing world over the next decade. However, by creating a favorable regulatory environment, governments can bring down these costs by as much as three quarters, to USD 0.5 trillion (approximately 0.2% of annual GDP), and avoid the need for public subsidy. Providing governments make judicious choices, adopting fiscal and regulatory regimes conducive to lowering costs, universal broadband may be within reach of most developing countries over the next decade."
"To accommodate the unprecedented increase of commercial airlines over the next ten years, the Next Generation Air Transportation System (NextGen) has been implemented in the USA that records large-scale Air Traffic Management (ATM) data to make air travel safer, more efficient, and more economical. A key role of collaborative decision making for air traffic scheduling and airspace resource management is the accurate prediction of flight delay. There has been a lot of attempts to apply data-driven methods such as machine learning to forecast flight delay situation using air traffic data of departures and arrivals. However, most of them omit en-route spatial information of airlines and temporal correlation between serial flights which results in inaccuracy prediction. In this paper, we present a novel aviation delay prediction system based on stacked Long Short-Term Memory (LSTM) networks for commercial flights. The system learns from historical trajectories from automatic dependent surveillance-broadcast (ADS-B) messages and uses the correlative geolocations to collect indispensable features such as climatic elements, air traffic, airspace, and human factors data along posterior routes. These features are integrated and then are fed into our proposed regression model. The latent spatio-temporal patterns of data are abstracted and learned in the LSTM architecture. Compared with previous schemes, our approach is demonstrated to be more robust and accurate for large hub airports."
"In the case that user profiles are not available, the recommendation based on anonymous session is particularly important, which aims to predict the items that the user may click at the next moment based on the user's access sequence over a while. In recent years, with the development of recurrent neural network, attention mechanism, and graph neural network, the performance of session-based recommendation has been greatly improved. However, the previous methods did not comprehensively consider the context dependencies and short-term interest first of the session. Therefore, we propose a context-aware short-term interest first model (CASIF).The aim of this paper is improve the accuracy of recommendations by combining context and short-term interest. In CASIF, we dynamically construct a graph structure for session sequences and capture rich context dependencies via graph neural network (GNN), latent feature vectors are captured as inputs of the next step. Then we build the short-term interest first module, which can to capture the user's general interest from the session in the context of long-term memory, at the same time get the user's current interest from the item of the last click. In the end, the short-term and long-term interest are combined as the final interest and multiplied by the candidate vector to obtain the recommendation probability. Finally, a large number of experiments on two real-world datasets demonstrate the effectiveness of our proposed method."
"We present the analytic evaluation of the two-loop corrections to the amplitude for the scattering of four fermions in Quantum Electrodynamics, $f^- + f^+ + F^- + F^+ \to 0$, with $f$ and $F$ representing a massless and a massive lepton, respectively. Dimensional regularization is employed to evaluate the loop integrals. Ultraviolet divergences are removed by renormalizing the coupling constant in the ${\overline{\text{MS}}}$-scheme, and the lepton mass as well as the external fields in the on-shell scheme. The analytic result for the renormalized amplitude is expressed as Laurent series around $d=4$ space-time dimensions, and contains Generalized Polylogarithms with up to weight four. The structure of the residual infrared divergences of the virtual amplitude is in agreement with the prediction of the Soft Collinear Effective Theory. Our analytic results are an essential ingredient for the computation of the scattering cross section for massive fermion-pair production in massless fermion-pair annihilation, i.e. $f^- f^+ \to F^- F^+$, and crossing related processes such as the elastic scattering $f F \to f F$, with up to Next-to-Next to Leading Order accuracy."
"Large Neighborhood Search (LNS) is a combinatorial optimization heuristic that starts with an assignment of values for the variables to be optimized, and iteratively improves it by searching a large neighborhood around the current assignment. In this paper we consider a learning-based LNS approach for mixed integer programs (MIPs). We train a Neural Diving model to represent a probability distribution over assignments, which, together with an off-the-shelf MIP solver, generates an initial assignment. Formulating the subsequent search steps as a Markov Decision Process, we train a Neural Neighborhood Selection policy to select a search neighborhood at each step, which is searched using a MIP solver to find the next assignment. The policy network is trained using imitation learning. We propose a target policy for imitation that, given enough compute resources, is guaranteed to select the neighborhood containing the optimal next assignment amongst all possible choices for the neighborhood of a specified size. Our approach matches or outperforms all the baselines on five real-world MIP datasets with large-scale instances from diverse applications, including two production applications at Google. It achieves $2\times$ to $37.8\times$ better average primal gap than the best baseline on three of the datasets at large running times."
"The notion of age of information (AoI) has become an important performance metric in network and control systems. Information freshness, represented by AoI, naturally arises in the context of caching. We address optimal scheduling of cache updates for a time-slotted system where the contents vary in size. There is limited capacity for the cache and for making content updates. Each content is associated with a utility function that is monotonically decreasing in the AoI. For this combinatorial optimization problem, we present the following contributions. First, we provide theoretical results settling the boundary of problem tractability. In particular, by a reformulation using network flows, we prove the boundary is essentially determined by whether or not the contents are of equal size. Second, we derive an integer linear formulation for the problem, of which the optimal solution can be obtained for small-scale scenarios. Next, via a mathematical reformulation, we derive a scalable optimization algorithm using repeated column generation. In addition, the algorithm computes a bound of global optimum, that can be used to assess the performance of any scheduling solution. Performance evaluation of large-scale scenarios demonstrates the strengths of the algorithm in comparison to a greedy schedule. Finally, we extend the applicability of our work to cyclic scheduling."
"For a global breeding organization, identifying the next generation of superior crops is vital for its success. Recognizing new genetic varieties requires years of in-field testing to gather data about the crop's yield, pest resistance, heat resistance, etc. At the conclusion of the growing season, organizations need to determine which varieties will be advanced to the next growing season (or sold to farmers) and which ones will be discarded from the candidate pool. Specifically for soybeans, identifying their relative maturity is a vital piece of information used for advancement decisions. However, this trait needs to be physically observed, and there are resource limitations (time, money, etc.) that bottleneck the data collection process. To combat this, breeding organizations are moving toward advanced image capturing devices. In this paper, we develop a robust and automatic approach for estimating the relative maturity of soybeans using a time series of UAV images. An end-to-end hybrid model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) is proposed to extract features and capture the sequential behavior of time series data. The proposed deep learning model was tested on six different environments across the United States. Results suggest the effectiveness of our proposed CNN-LSTM model compared to the local regression method. Furthermore, we demonstrate how this newfound information can be used to aid in plant breeding advancement decisions."
"The production of high-mass, color-singlet particles in hadron colliders is universally accompanied by initial state QCD radiation that is predominantly soft with respect to the hard process scale $Q$ and/or collinear with respect to the beam axis. At TeV-scale colliders, this is in contrast to top quark and multijet processes, which, by definition, are hard and central. Consequently, vetoing events with jets possessing transverse momenta above $p_T^{\rm Veto}$ in searches for new color-singlet states can efficiently reduce non-singlet backgrounds, thereby increasing experimental sensitivity. To quantify this generic observation, we investigate the production and leptonic decay of a Sequential Standard Model $W'$ boson at the 13 TeV LHC. We systematically consider signal and background processes at next-to-leading-order (NLO) in QCD with parton shower (PS) matching; for color-singlet channels, we resum Sudakov logarithms of the form $\alpha_s^j(p_T^{\rm Veto})\log^k(Q/p_T^{\rm Veto})$ up to next-to-next-to-leading logarithmic accuracy (NNLL) with NLO matching. We obtain our results using the MadGraph5aMC@NLO and MadGraph5aMC@NLO-SCET frameworks, respectively. Associated FeynRules model files capable of handling NLO+PS- and NLO+NNLL-accurate computations are publicly available. We find that within their given uncertainties, both the NLO+PS and NLO+NNLL(veto) calculations give accurate and consistent predictions. Consequently, jet vetoes applied to color-singlet processes can be reliably modeled at the NLO+PS level. With respect to a $b$-jet veto of $p_{T}^{\rm Veto} = 30$ GeV, flavor-agnostic vetoes of $p_{T}^{\rm Veto} = 30-40$ GeV can further reduce single top and $t\overline{t}$ rates by a factor of 2-50 at a mild cost of the signal rate. Jet vetoes can increase signal-to-noise ratios by roughly 10\% for light $W'$ boson masses of 30-50 GeV and 25\%-250\% for masses of 300-800 GeV."
"Context. Solar activity cycles vary in amplitude and duration. The variations can be at least partly explained by fluctuations in dynamo parameters. Aims. We want to restrict uncertainty in fluctuating dynamo parameters and find out which properties of the fluctuations control the amplitudes of the magnetic field and energy in variable dynamo cycles. Methods. A flux-transport model for the solar dynamo with fluctuations of the Babcock-Leighton type $\alpha$-effect was applied to generate statistics of magnetic cycles for our purposes. The statistics were compared with data on solar cycle periods to restrict the correlation time of dynamo fluctuations. Results. A characteristic time of fluctuations in the $\alpha$-effect is estimated to be close to the solar rotation period. The fluctuations produce asymmetry between the times of rise and descent of dynamo cycles, the rise time being on average shorter. The affect of the fluctuations on cycle amplitudes depends on the phase of the cycle in which the fluctuations occur. Negative fluctuations (decrease in $\alpha$) in the rise phase delay decay of poloidal field and increase the cycle amplitude in toroidal field and magnetic energy. Negative fluctuation in the decline phase reduces the polar field at the end of a cycle and the amplitude of the next cycle. The low amplitude of the 24th solar cycle compared to the preceding 23rd cycle can be explained by this effect. Positive fluctuations in the descent phase enhance the magnetic energy of the next cycle by increasing the seed poloidal field for the next cycle. The statistics of the computed energies of the cycles suggest that superflares of $\ge 10^{34}$ erg are not possible on the Sun."
"We compute the two-loop heavy quark jet-function in the heavy quark limit. This is one of the key ingredients in next-to-next-to-leading order (NNLO) and next-to-next-to-leading-log order (NNLL) computations of the invariant mass distribution of top-jets at a future e+e- collider. The shape of the top invariant mass distribution is affected by large logs which we compute at NNLL order. Exploiting the non-abelian exponentiation theorem, a definition of the top jet-mass is given which is transitive and whose renormalization group evolution is determined by the cusp-anomalous dimension to all orders in perturbation theory. Relations of the jet-mass to the pole, MSbar, and 1S masses are presented at two-loop order."
"We set up a formalism, within the antenna subtraction framework, for computing the production of a massive quark-antiquark pair in electron positron collisions at next-to-next-to-leading order in the coupling $\alpha_s$ of quantum chromodynamics at the differential level. Our formalism applies to the calculation of any infrared-safe observable. We apply this set-up to the production of top-quark top antiquark pairs in the continuum. We compute the production cross section and several distributions. We determine, in particular, the top-quark forward-backward asymmetry at order $\alpha_s^2$. Our result agrees with previous computations of this observable."
"Stuttering bisimulation is a well-known behavioral equivalence that preserves CTL-X, namely CTL without the next-time operator X. Correspondingly, the stuttering simulation preorder induces a coarser behavioral equivalence that preserves the existential fragment ECTL-{X,G}, namely ECTL without the next-time X and globally G operators. While stuttering bisimulation equivalence can be computed by the well-known Groote and Vaandrager's [1990] algorithm, to the best of our knowledge, no algorithm for computing the stuttering simulation preorder and equivalence is available. This paper presents such an algorithm for finite state systems."
"We use the known soft and collinear limits of tree- and one-loop scattering amplitudes -- computed over a decade ago -- to explicitly construct a subtraction scheme for next-to-next-to-leading order (NNLO) computations. Our approach combines partitioning of the final-state phase space together with the technique of sector decomposition, following recent suggestions in Ref. [1]. We apply this scheme to a toy example: the NNLO QED corrections to the decay of the Z boson to a pair of massless leptons. We argue that the main features of this subtraction scheme remain valid for computations of processes of arbitrary complexity with NNLO accuracy."
"Anytime learning describes a relatively novel concept by which systems are able to acquire knowledge about a changing environment and adapt and behave accordingly to this. ""Anytime"" refers to the fact that the system is capable of returning imperfect results at any point in time, which allows it to remain functional even if a perfect solution could not be found within the necessary time frame. This paper focuses on illustrating the concept of anytime learning and examining how it relates to organic computing as a whole. Could anytime learning be the next step in organic computing?"
I report on a formalism introduced recently to combine parton shower Monte Carlo's and next-to-leading order QCD computations.
This is a thought piece on data-intensive science requirements for databases and science centers. It argues that peta-scale datasets will be housed by science centers that provide substantial storage and processing for scientists who access the data via smart notebooks. Next-generation science instruments and simulations will generate these peta-scale datasets. The need to publish and share data and the need for generic analysis and visualization tools will finally create a convergence on common metadata standards. Database systems will be judged by their support of these metadata standards and by their ability to manage and access peta-scale datasets. The procedural stream-of-bytes-file-centric approach to data analysis is both too cumbersome and too serial for such large datasets. Non-procedural query and analysis of schematized self-describing data is both easier to use and allows much more parallelism.
"Gait recognition is a biometric technology that identifies individuals in a video sequence by analysing their style of walking or limb movement. However, this identification is generally sensitive to appearance changes and conventional feature descriptors such as Gait Energy Image (GEI) lose some of the dynamic information in the gait sequence. Active Energy Image (AEI) focuses more on dynamic motion changes than GEI and is more suited to deal with appearance changes. We propose a new approach, which allows recognizing people by analysing the dynamic motion variations and identifying people without using a database of predicted changes. In the proposed method, the active energy image is calculated by averaging the difference frames of the silhouette sequence and divided into multiple segments. Affine moment invariants are computed as gait features for each section. Next, matching weights are calculated based on the similarity between extracted features and those in the database. Finally, the subject is identified by the weighted combination of similarities in all segments. The CASIA-B Gait Database is used as the principal dataset for the experimental analysis."
"In this paper, we try to further demonstrate that the models of random CSP instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods."
"We study the algebra $U_{\zeta}$ obtained via Lusztig's `integral' form [Lu 1, 2] of the generic quantum algebra for the Lie algebra $\frak {g=sl}_2$ modulo the two-sided ideal generated by $K^l-1$. We show that $U_{\zeta}$ is a smash product of the quantum deformation of the restricted universal enveloping algebra $\bold u_{\zeta}$ of $\frak g$ and the ordinary universal enveloping algebra $U$ of $\frak g$, and we compute the primitive (= prime) ideals of $\Uz$. Next we describe a decomposition of $\bold u_{\zeta}$ into the simple $U$- submodules, which leads to an explicit formula for the center and the indecomposable direct summands of $\Uz$. We conclude with a description of the lattice of cofinite ideals of $\Uz$ in terms of a unique set of lattice generators."
"The framework of consistent query answers and repairs has been introduced to alleviate the impact of inconsistent data on the answers to a query. A repair is a minimally different consistent instance and an answer is consistent if it is present in every repair. In this article we study the complexity of consistent query answers and repair checking in the presence of universal constraints.   We propose an extended version of the conflict hypergraph which allows to capture all repairs w.r.t. a set of universal constraints. We show that repair checking is in PTIME for the class of full tuple-generating dependencies and denial constraints, and we present a polynomial repair algorithm. This algorithm is sound, i.e. always produces a repair, but also complete, i.e. every repair can be constructed. Next, we present a polynomial-time algorithm computing consistent answers to ground quantifier-free queries in the presence of denial constraints, join dependencies, and acyclic full-tuple generating dependencies. Finally, we show that extending the class of constraints leads to intractability. For arbitrary full tuple-generating dependencies consistent query answering becomes coNP-complete. For arbitrary universal constraints consistent query answering is \Pi_2^p-complete and repair checking coNP-complete."
"The CP asymmetry in neutrino oscillations, assuming new physics at production and/or detection processes, is analyzed. We compute this CP asymmetry using the standard quantum field theory within a general new physics scenario that may generate new sources of CP and flavor violation. Well known results for the CP asymmetry are reproduced in the case of V -A operators, and additional contributions from new physics operators are derived. We apply this formalism to SUSY extensions of the Standard Model where the contributions from new operators could produce a CP asymmetry observable in the next generation of neutrino experiments."
"This thesis is concerned with the theory of invariant bilinear differential pairings on parabolic geometries. It introduces the concept formally with the help of the jet bundle formalism and provides a detailed analysis. More precisely, after introducing the most important notations and definitions, we first of all give an algebraic description for pairings on homogeneous spaces and obtain a first existence theorem. Next, a classification of first order invariant bilinear differential pairings is given under exclusion of certain degenerate cases that are related to the existence of invariant linear differential operators. Furthermore, a concrete formula for a large class of invariant bilinear differential pairings of arbitrary order is given and many examples are computed. The general theory of higher order invariant bilinear differential pairings turns out to be much more intricate and a general construction is only possible under exclusion of finitely many degenerate cases whose significance in general remains elusive (although a result for projective geometry is included). The construction relies on so-called splitting operators examples of which are described for projective geometry, conformal geometry and CR geometry in the last chapter."
"We construct the D-brane states at finite temperature in thermal equilibrium in the $\mathbb{R}^{1,p}\times{\mathbb{T}}^{d-p-1}$ spacetime in the presence of cold (unthermalized) Kalb-Ramond (KR) and U(1) gauge potential background. To this end, we first generalize the Thermo Field Dynamics (TFD) to wrapped closed strings. This generalization is consistent with the spatial translation invariance on the string world-sheet. Next, we determine the thermal string vacuum and define the entropy operator. From these data we calculate the entropy of the closed string and the free energy. Finally, we define the thermal D-brane states in $\mathbb{R}^{1,p}\times{\mathbb{T}}^{d-p-1}$ in the presence of cold constant KR field and U(1) gauge potential as the boundary states of the thermal closed string and compute their entropy."
"We investigate the presence of residual multifractal background for monofractal signals which appears due to the finite length of the signals and (or) due to the long memory the signals reveal. This phenomenon is investigated numerically within the multifractal detrended fluctuation analysis (MF-DFA) for artificially generated time series. Next, the analytical formulas enabling to describe the multifractal content in such signals are provided. Final results are shown in the frequently used generalized Hurst exponent h(q) multifractal scenario and are presented as a function of time series length L and the autocorrelation exponent value {\gamma}. The multifractal spectrum ({\alpha}, f ({\alpha})) approach is also discussed. The obtained results may be significant in any practical application of multifractality, including financial data analysis, because the ""true"" multifractal effect should be clearly separated from the so called ""multifractal noise"". Examples from finance in this context are given. The provided formulas may help to decide whether we do deal with the signal of real multifractal origin or not."
"This paper considers the enumeration of ternary trees (i.e. rooted ordered trees in which each vertex has 0 or 3 children) avoiding a contiguous ternary tree pattern. We begin by finding recurrence relations for several simple tree patterns; then, for more complex trees, we compute generating functions by extending a known algorithm for pattern-avoiding binary trees. Next, we present an alternate one-dimensional notation for trees which we use to find bijections that explain why certain pairs of tree patterns yield the same avoidance generating function. Finally, we compare our bijections to known ""replacement rules"" for binary trees and generalize these bijections to a larger class of trees."
"In the last years, there has been a large amount of research on embeddability properties of finitely generated hyperbolic groups. In this paper, we elaborate on the more general class of locally compact hyperbolic groups. We compute the equivariant $L_p$-compression in a number of locally compact examples, such as the groups $SO(n,1)$: by proving that the equivariant $L_p$-compression of a locally compact compactly generated group is minimal for $p=2$, we calculate all equivariant $L_p$-compressions of $SO(n,1)$. Next, we show that although there are locally compact, non-discrete hyperbolic groups $G$ with Kazhdan's property ($T$), it is true that any locally compact hyperbolic group admits a proper affine isometric action on an $L_p$-space for $p$ larger than the Ahlfors regular conformal dimension of $\partial G$. This answers a question asked by Yves de Cornulier. Finally, we elaborate on the locally compact version of property $(A)$ and show that, as in the discrete case, a locally compact second countable group has property (A) if its non-equivariant compression is greater than 1/2."
"In the field of empirical modeling using Genetic Programming (GP), it is important to evolve solution with good generalization ability. Generalization ability of GP solutions get affected by two important issues: bloat and over-fitting. Bloat is uncontrolled growth of code without any gain in fitness and important issue in GP. We surveyed and classified existing literature related to different techniques used by GP research community to deal with the issue of bloat. Moreover, the classifications of different bloat control approaches and measures for bloat are discussed. Next, we tested four bloat control methods: Tarpeian, double tournament, lexicographic parsimony pressure with direct bucketing and ratio bucketing on six different problems and identified where each bloat control method performs well on per problem basis. Based on the analysis of each method, we combined two methods: double tournament (selection method) and Tarpeian method (works before evaluation) to avoid bloated solutions and compared with the results obtained from individual performance of double tournament method. It was found that the results were improved with this combination of two methods."
"Components connected over a network influence each other and interact in various ways. Examples of such systems are networks of computing nodes, which the nodes interact by exchanging workload, for instance, for load balancing purposes. In this paper, we first study the Influence Model, a networked Markov chain framework, for modeling network interactions and discuss two key limitations of this model, which cause it to fall short in modeling constraint-based and dynamic interactions in networks. Next, we propose the Dynamic and Constraint-based Influence Model (DCIM) to alleviate the limitations. The DCIM extends the application of the Influence Model to more general network interaction scenarios. In this paper, the proposed DCIM is successfully applied to stochastic modeling of load balancing in networks of computing nodes allowing for prediction of the load distribution in the system, which is a novel application for the Influence Model. The DCIM is further used to identify the optimum workload distribution policy for load balancing in networked computing systems."
"One of the key research interests in the area of Constraint Satisfaction Problem (CSP) is to identify tractable classes of constraints and develop efficient solutions for them. In this paper, we introduce generalized staircase (GS) constraints which is an important generalization of one such tractable class found in the literature, namely, staircase constraints. GS constraints are of two kinds, down staircase (DS) and up staircase (US). We first examine several properties of GS constraints, and then show that arc consistency is sufficient to determine a solution to a CSP over DS constraints. Further, we propose an optimal O(cd) time and space algorithm to compute arc consistency for GS constraints where c is the number of constraints and d is the size of the largest domain. Next, observing that arc consistency is not necessary for solving a DSCSP, we propose a more efficient algorithm for solving it. With regard to US constraints, arc consistency is not known to be sufficient to determine a solution, and therefore, methods such as path consistency or variable elimination are required. Since arc consistency acts as a subroutine for these existing methods, replacing it by our optimal O(cd) arc consistency algorithm produces a more efficient method for solving a USCSP."
"The growth of wireless communication technologies has been producing the intense demand for high-speed, efficient, reliable voice & data communication. As a result, third generation partnership project (3GPP) has implemented next generation wireless communication technology long term evolution (LTE) which is designed to increase the capacity and speed of existing mobile telephone & data networks. LTE has adopted a multicarrier transmission technique known as orthogonal frequency division multiplexing (OFDM). OFDM meets the LTE requirement for spectrum flexibility and enables cost-efficient solutions for very wide carriers. One major generic problem of OFDM technique is high peak to average power ratio (PAPR) which is defined as the ratio of the peak power to the average power of the OFDM signal. A trade-off is necessary for reducing PAPR with increasing bit error rate (BER), computational complexity or data rate loss etc. In this paper, two clipping based filtering methods have been implemented & also analyzed their modulation effects on reducing PAPR."
"This work comprises a study upon the quantization and the renormalizability of the generalized electrodynamics of spinless charged particles (mesons), namely, the Generalized Scalar Electrodynamics ($GSQED_{4}$). The theory is quantized in the covariant framework of the Batalin-Fradkin-Vilkovisky method. Thereafter, the complete Green's functions are obtained through functional methods and a proper discussion on the theory's renormalizability is also given. Next, it is presented the computation and further discussion on the radiative correction at $\alpha$-order; and, as it turns out, an unexpected $m_{P}$-dependent divergence on the mesonic sector of the theory is found. Furthermore, in order to show the effectiveness of the renormalization procedure on the present theory it is also shown a diagrammatic discussion on the photon self-energy at $\alpha^{2}$-order, where is observed contributions from the meson self-energy function. Afterwards, we present the expressions of the counter-terms and effective coupling of the theory. Obtaining from the later a energy range where the theory is defined $m^{2} \leq k^{2} < m_{P}^{2}$. It is also shown in our final discussion that the new divergence is absorbed suitably by the mass counter-term $\delta_{Z_{0}}$, showing therefore that the gauge WFT identities are satisfied still."
"We investigate the dynamics of spinning binaries of compact objects at the next-to-leading order in the quadratic-in-spin effects, which corresponds to the third post-Newtonian order (3PN). Using a Dixon-type multipolar formalism for spinning point particles endowed with spin-induced quadrupoles and computing iteratively in harmonic coordinates the relevant pieces of the PN metric within the near zone, we derive the post-Newtonian equations of motion as well as the equations of spin precession. We find full equivalence with available results. We then focus on the far-zone field produced by those systems and obtain the previously unknown 3PN spin contributions to the gravitational-wave energy flux by means of the multipolar post-Minkowskian (MPM) wave generation formalism. Our results are presented in the center-of-mass frame for generic orbits, before being further specialized to the case of spin-aligned, circular orbits. We derive the orbital phase of the binary based on the energy balance equation and briefly discuss the relevance of the new terms."
"To each hyperbolic Landau level of the Poincar\'e disc is attached a generalized negative binomial distribution. In this paper, we compute the moment generating function of this distribution and supply its decomposition as a perturbation of the negative binomial distribution by a finitely-supported measure. Using the Mandel parameter, we also discuss the nonclassical nature of the associated coherent states. Next, we determine the L\'evy-Kintchine decomposition its characteristic function when the latter does not vanish and deduce that it is quasi-infinitely divisible except for the lowest hyperbolic Landau level corresponding to the negative binomial distribution. By considering the total variation of the obtained quasi-L\'evy measure, we introduce a new infinitely-divisible distribution for which we derive the characteristic function."
"This paper proposes novel approaches to design hierarchical decentralized robust controllers for homogeneous linear multi-agent systems (MASs) perturbed by disturbances/noise. Firstly, based on LQR method, we present a systematic procedure to design hierarchical decentralized optimal stabilizing controllers for MASs without disturbances/noise. Next, a method for deriving reduced-order hierarchical decentralized stabilizing controllers is presented by suitable selections of the weighting matrices in the LQR performance index. Secondly, the hierarchical decentralized robust controller designs in terms of $H_{\infty}$ and $H_{2}$ norms are introduced, which include two different scenarios namely general and LQR-based synthesis. For the general synthesis, the robust controller gains are computed as solutions of a distributed convex optimization problem with LMI constraints. On the other hand, for the LQR-based design, the robust controller gains obtained from the general synthesis are further verified as LQR stabilizing gains to be unified with the LQR-based design when there are no disturbances/noise. This results in a hierarchical decentralized inverse optimal control problem, for which we will propose a new method to resolve it. Finally, several numerical examples are presented to illustrate the effectiveness of the proposed approaches."
"Next generation deep neural networks for classification hosted on embedded platforms will rely on fast, efficient, and accurate learning algorithms. Initialization of weights in learning networks has a great impact on the classification accuracy. In this paper we focus on deriving good initial weights by modeling the error function of a deep neural network as a high-dimensional landscape. We observe that due to the inherent complexity in its algebraic structure, such an error function may conform to general results of the statistics of large systems. To this end we apply some results from Random Matrix Theory to analyse these functions. We model the error function in terms of a Hamiltonian in N-dimensions and derive some theoretical results about its general behavior. These results are further used to make better initial guesses of weights for the learning algorithm."
"This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner."
"We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods."
"If $G$ is a graph with vertex set $V$, let Conf$_n^{\text{sink}}(G,V)$ be the space of $n$-tuples of points on $G$, which are only allowed to overlap on elements of $V$. We think of Conf$_n^{\text{sink}}(G,V)$ as a configuration space of points on $G$, where points are allowed to collide on vertices. In this paper, we attempt to understand these spaces from two separate, but closely related, perspectives. Using techniques of combinatorial topology we compute the fundamental groups and homology groups of Conf$_n^{\text{sink}}(G,V)$ in the case where $G$ is a tree. Next, we use techniques of asymptotic algebra to prove statements about Conf$_n^{\text{sink}}(G,V)$, for general graphs $G$, whenever $n$ is sufficiently large. It is proven that, for general graphs, the homology groups exhibit generalized representation stability in the sense of previous work of the author."
"Characterizing thermally activated transitions in high-dimensional rugged energy surfaces is a very challenging task for classical computers. Here, we develop a quantum annealing scheme to solve this problem. First, the task of finding the most probable transition paths in configuration space is reduced to a shortest-path problem defined on a suitable weighted graph. Next, this optimization problem is mapped into finding the ground state of a generalized Ising model. A finite-size scaling analysis suggests this task may be solvable efficiently by a quantum annealing machine. Our approach leverages on the quantized nature of qubits to describe transitions between different system's configurations. Since it does not involve any lattice space discretization, it paves the way towards future biophysical applications of quantum computing based on realistic all-atom models."
"The objective is to present one important aspect of the European IST-FET project ""REV!GIS""1: the methodology which has been developed for the translation (interpretation) of the quality of the data into a ""fitness for use"" information, that we can confront to the user needs in its application. This methodology is based upon the notion of ""ontologies"" as a conceptual framework able to capture the explicit and implicit knowledge involved in the application. We do not address the general problem of formalizing such ontologies, instead, we rather try to illustrate this with three applications which are particular cases of the more general ""data fusion"" problem. In each application, we show how to deploy our methodology, by comparing several possible solutions, and we try to enlighten where are the quality issues, and what kind of solution to privilege, even at the expense of a highly complex computational approach. The expectation of the REV!GIS project is that computationally tractable solutions will be available among the next generation AI tools."
"Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a ""policy network"" and a ""value network"" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics."
"We suggest a simple adaptive step-size procedure, which does not require any line-search, for a general class of nonlinear optimization methods and prove convergence of a general method under mild assumptions. In particular, the goal function may be non-smooth and non-convex. Unlike the descent line-search methods, it does not require monotone decrease of the goal function values along the iteration points and reduces the implementation cost of each iteration essentially. The key element of this procedure consists in inserting a majorant step-size sequence such that the next element is taken only if the current iterate does not give a sufficient descent. Its applications yield in particular a new gradient projection method for smooth constrained optimization problems and a new projection type method for minimization of the gap function of a general variational inequality. Preliminary results of computational experiments confirm efficiency of the proposed modification."
"Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese->English and WMT'14 English->German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets."
"Single-Instruction, Multiple-Data (SIMD) random number generators (RNGs) take advantage of vector units to offer significant performance gain over non-vectorized libraries, but they often rely on batch production of deviates from distributions with fixed parameters. In many statistical applications such as Gibbs sampling, parameters of sampled distributions change from one iteration to the next, requiring that random deviates be generated one-at-a-time. This situation can render vectorized RNGs inefficient, and even inferior to their scalar counterparts. The C++ class BatchRNG uses buffers of base distributions such uniform, Gaussian and exponential to take advantage of vector units while allowing for sequences of deviates to be generated with varying parameters. These small buffers are consumed and replenished as needed during a program execution. Performance tests using Intel Vector Statistical Library (VSL) on various probability distributions illustrates the effectiveness of the proposed batching strategy."
"We establish several new results on Marton's coding scheme and its corresponding inner bound on the capacity region of the general broadcast channel. We show that unlike the Gaussian case, Marton's coding scheme without superposition coding is not optimal in general even for a degraded broadcast channel with no common message. We then establish properties of Marton's inner bound that help restrict the search space for computing the sum-rate. Next, we show that the inner bound is optimal along certain directions. Finally, we propose a coding scheme that may lead to a larger inner bound."
"Quantum-dot cellular automata (QCA) shows promise as a post silicon CMOS, low power computational technology. Nevertheless, to generalize QCA for next-generation digital devices, the ability to implement conventional programmable circuits based on NOR, AND, and OR gates is necessary. To this end, we devise a new QCA structure, the QCA matrix multiplier (MM), employing the standard Coulomb blocked, five quantum dot (QD) QCA cell and quasi-adiabatic switching for sequential data latching in the QCA cells. Our structure can multiply two N x M matrices, using one input and one bidirectional input/output data line. The calculation is highly parallelizable, and it is possible to achieve reduced calculation time in exchange for increasing numbers of parallel matrix multiplier units. We show convergent, ab initio simulation results using the Intercellular Hartree Approximation for one, three, and nine matrix multiplier units. The structure can generally implement any programmable logic array (PLA) or any matrix multiplication based operation."
"We propose a method for checking and enforcing multi-contact stability based on the Zero-tilting Moment Point (ZMP). The key to our development is the generalization of ZMP support areas to take into account (a) frictional constraints and (b) multiple non-coplanar contacts. We introduce and investigate two kinds of ZMP support areas. First, we characterize and provide a fast geometric construction for the support area generated by valid contact forces, with no other constraint on the robot motion. We call this set the full support area. Next, we consider the control of humanoid robots using the Linear Pendulum Mode (LPM). We observe that the constraints stemming from the LPM induce a shrinking of the support area, even for walking on horizontal floors. We propose an algorithm to compute the new area, which we call pendular support area. We show that, in the LPM, having the ZMP in the pendular support area is a necessary and sufficient condition for contact stability. Based on these developments, we implement a whole-body controller and generate feasible multi-contact motions where an HRP-4 humanoid locomotes in challenging multi-contact scenarios."
"The main goal of this work is to study systematically the quantum aspects of the interaction between scalar particles in the framework of Generalized Scalar Duffin-Kemmer-Petiau Electrodynamics (GSDKP). For this purpose the theory is quantized after a constraint analysis following Dirac's methodology by determining the Hamiltonian transition amplitude. In particular, the covariant transition amplitude is established in the generalized non-mixing Lorenz gauge. The complete Green's functions are obtained through functional methods and the theory's renormalizability is also detailed presented. Next, the radiative corrections for the Green's functions at $\alpha $-order are computed; and, as it turns out, an unexpected $m_{P}$-dependent divergence on the DKP sector of the theory is found. Furthermore, in order to show the effectiveness of the renormalization procedure on the present theory, a diagrammatic discussion on the photon self-energy and vertex part at $\alpha ^{2}$-order are presented, where it is possible to observe contributions from the DKP self-energy function, and then analyse whether or not this novel divergence propagates to higher-order contributions. Lastly, an energy range where the theory is well defined: $m^{2}\ll k^{2}<m_{p}^{2}$ was also found by evaluating the effective coupling for the GSDKP."
"Artificial intelligence (AI) is an important technology that supports daily social life and economic activities. It contributes greatly to the sustainable growth of Japan's economy and solves various social problems. In recent years, AI has attracted attention as a key for growth in developed countries such as Europe and the United States and developing countries such as China and India. The attention has been focused mainly on developing new artificial intelligence information communication technology (ICT) and robot technology (RT). Although recently developed AI technology certainly excels in extracting certain patterns, there are many limitations. Most ICT models are overly dependent on big data, lack a self-idea function, and are complicated. In this paper, rather than merely developing next-generation artificial intelligence technology, we aim to develop a new concept of general-purpose intelligence cognition technology called Beyond AI. Specifically, we plan to develop an intelligent learning model called Brain Intelligence (BI) that generates new ideas about events without having experienced them by using artificial life with an imagine function. We will also conduct demonstrations of the developed BI intelligence learning model on automatic driving, precision medical care, and industrial robots."
"Recent advances in integrated photonics enable the implementation of reconfigurable, high-bandwidth, and low energy-per-bit interconnects in next-generation data centers. We propose and evaluate an Optically Connected Memory (OCM) architecture that disaggregates the main memory from the computation nodes in data centers. OCM is based on micro-ring resonators (MRRs), and it does not require any modification to the DRAM memory modules. We calculate energy consumption from real photonic devices and integrate them into a system simulator to evaluate performance. Our results show that (1) OCM is capable of interconnecting four DDR4 memory channels to a computing node using two fibers with 1.07 pJ energy-per-bit consumption and (2) OCM performs up to 5.5x faster than a disaggregated memory with 40G PCIe NIC connectors to computing nodes."
"Acoustic quadrupole modes of sunlike stars vibrate when perturbed by a passing gravitational wave generated somewhere in the Universe. Here, we compute the imprint of the gravitational waves on the acoustic spectrum of these stars for gravitational events occurring near the supermassive black hole located at the center of the Milky Way. We found that in most cases the impact of gravitational waves in low-order quadrupole modes is not above the current observational threshold of detectability, although this should be in the reach of the next generation of near infrared observatories and asteroseismology satellite missions. Equally, we found that it is possible to follow the end phase of the coalescence of binaries with large chirp masses, as these phenomena have a unique imprint in the spectra of sunlike stars affecting sequentially several low-order quadrupole modes. Moreover, we discuss the different imprints on the acoustic spectra of the different types of binary systems constituted either by two white dwarfs, two neutron stars, two black holes or a compact star and a massive black hole."
"We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence.   Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs)."
"Public transport network constitutes for an indispensable part of a city by providing mobility services to the general masses. To improve ease of access and reduce infrastructural investments, public transport authorities often adopt proof of payment system. Such a system operates by eliminating ticket controls when boarding the vehicle and subjecting the travelers to random ticket checks by affiliated personnel (controllers). Although cost efficient, such a system promotes free-riders, who deliberately decide to evade fares for the transport service. A recent survey by the association of European transport, estimates hefty income losses due to fare evasion, highlighting that free-riding is a serious problem that needs immediate attention. To this end, we highlight the attack vectors which can be exploited by free-riders by analyzing the crowdsourced data about the control-locations. Next, we propose a framework to generate randomized control-location traces by using generative adversarial networks (GANs) in order to minimize the attack vectors. Finally, we propose metrics to evaluate such a system, quantified in terms of increased risk and higher probability of being subjected to control checks across the city."
"Next generation robots will need to understand intricate and articulated objects as they cooperate in human environments. To do so, these robots will need to move beyond their current abilities--- working with relatively simple objects in a task-indifferent manner--- toward more sophisticated abilities that dynamically estimate the properties of complex, articulated objects. To that end, we make two compelling contributions toward general articulated (physical) object understanding in this paper. First, we introduce a new dataset, SPARE: Simulated and Physical ARticulated Extendable dataset. SPARE is an extendable open-source dataset providing equivalent simulated and physical instances of articulated objects (kinematic chains), providing the greater research community with a training and evaluation tool for methods generating kinematic descriptions of articulated objects. To the best of our knowledge, this is the first joint visual and physical (3D-printable) dataset for the Vision community. Second, we present a deep neural network that can predit the number of links and the length of the links of an articulated object. These new ideas outperform classical approaches to understanding kinematic chains, such tracking-based methods, which fail in the case of occlusion and do not leverage multiple views when available."
"Long-term human motion can be represented as a series of motion modes---motion sequences that capture short-term temporal dynamics---with transitions between them. We leverage this structure and present a novel Motion Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence generation. Our model jointly learns a feature embedding for motion modes (that the motion sequence can be reconstructed from) and a feature transformation that represents the transition of one motion mode to the next motion mode. Our model is able to generate multiple diverse and plausible motion sequences in the future from the same input. We apply our approach to both facial and full body motion, and demonstrate applications like analogy-based motion transfer and video synthesis."
"In this paper a generalized fundamental solution using the boundary element method to solve the Helmholtz equation is proposed. It is observed that the commonly used fundamental solution is only valid for good conductors since the capacitive effect of the considered medium is always neglected. By the use of the well-known Lorentz gauge condition a fundamental solution which incorporates the phase as well as the attenuation transmission coefficients is derived by the authors. Next, a model is developed using this proposed fundamental solution for modelling a coating layer of a buried pipeline. Subsequently, a model of a buried coated pipeline in close proximity to a high voltage power line is developed and numerically implemented. Finally, the model is used to simulate two configurations in order the verify the proposed general fundamental solution of the boundary element method for electromagnetic field problems. Hereby, its validity is proven and it is shown that the generalized solution and related model can be used for industrial applications."
"This paper introduces a document grounded dataset for text conversations. We define ""Document Grounded Conversations"" as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses."
"We enlarge the spectral problem of a generalized D-Kaup-Newell (D-KN) spectral problem. Solving the enlarged zero-curvature equations, we produce integrable couplings. A reduction of the spectral matrix leads to a second integrable coupling system. Next, bilinear forms that are symmetric, ad-invariant, and non-degenerate on the given non-semisimple matrix Lie algebra are computed to employ the variational identity. The variational identity is then applied to the original enlarged spectral problem of a generalized D-KN hierarchy and the reduced problem. Hamiltonian structures are presented, as well as a bi-Hamiltonian formulation of the reduced problem. Both hierarchies have infinitely many commuting symmetries and conserved densities, i.e., are Liouville integrable."
"Motivated by the sequential detection of false data injection attacks (FDIAs) in a dynamic smart grid, we consider a more general problem of sequentially detecting time-varying FDIAs in dynamic linear regression models. The unknown time-varying parameter vector in the linear regression model and the FDIAs impose a significant challenge for designing a computationally efficient detector. We first propose two Cumulative-Sum-type algorithms to address this challenge. One is called generalized Cumulative-Sum (GCUSUM) algorithm, and the other one is called relaxed generalized Cumulative-Sum (RGCUSUM) algorithm, which is a modified version of the GCUSUM. It can be shown that the computational complexity of the proposed RGCUSUM algorithm scales linearly with the number of observations. Next, considering Lordon's setup, for any given constraint on the expected false alarm period, a lower bound on the threshold employed in the proposed RGCUSUM algorithm is derived, which provides a guideline for the design of the proposed RGCUSUM algorithm to achieve the prescribed performance requirement. In addition, for any given threshold employed in the proposed RGCUSUM algorithm, an upper bound on the expected detection delay is also provided. The performance of the proposed RGCUSUM algorithm is also numerically studied in the context of an IEEE standard power system under FDIAs."
"We identify the existence of nonlinear optical (NLO) activity in a number of novel $ABX_3$-type metal-free perovskites, where $A$ is a highly tuneable organic cation, $B$ is a NH$_4$ cation and $X$ a halide anion. Through systematic first-principles calculations, we identify important trends to chart the second-harmonic generation of this class of materials. We study three perovskites MDABCO-NH$_4$I$_3$, CNDABCO-NH$_4$I$_3$ and ODABCO-NH$_4$I$_3$ for use as deep-UV second-harmonic generation materials. We identify the role of the dipole moment imparted by the organic group on the $A$ cation as an important parameter to tune the NLO properties of these materials. We apply this knowledge functionalising the organic group DABCO with the highly polar cyanide CN$^-$ group, and we demonstrate a significant improvement of the NLO response in this family of materials. These findings can accelerate the application of metal free perovskites as inexpensive, non-toxic, earth-abundant materials for the next generation of optical communication applications."
"This paper applies operads and functorial semantics to address the problem of failure diagnosis in complex systems. We start with a concrete example, developing a hierarchical interaction model for the Length Scale Interferometer, a high-precision measurement system operated by the US National Institute of Standards and Technology. The model is expressed in terms of combinatorial/diagrammatic structures called port-graphs, and we explain how to extract an operad LSI from a collection of these diagrams. Next we show how functors to the operad of probabilities organize and constrain the relative probabilities of component failure in the system. Finally, we show how to extend the analysis from general component failure to specific failure modes."
"In high-energy physics, Monte Carlo event generators (MCEGs) are used to simulate the interactions of high energy particles. MCEG event records store the information on the simulated particles and their relationships, and thus reflects the simulated evolution of physics phenomena in each collision event.   We present the HepMC3 library, a next-generation framework for MCEG event record encoding and manipulation, which builds on the functionality of its widely-used predecessors to enable more sophisticated algorithms for event-record analysis. By comparison to previous versions, the event record structure has been simplified, while adding the possibility to encode arbitrary information. The I/O functionality has been extended to support common input and output formats of various HEP MCEGs, including formats used in Fortran MCEGs, the formats established by the HepMC2 library, and binary formats such as ROOT; custom input or output handlers may also be used. HepMC3 is already supported by popular modern MCEGs and can replace the older HepMC versions in many others."
"In this article, we present a new data type agnostic algorithm calculating a concept lattice from heterogeneous and complex data. Our NextPriorityConcept algorithm is first introduced and proved in the binary case as an extension of Bordat's algorithm with the notion of strategies to select only some predecessors of each concept, avoiding the generation of unreasonably large lattices. The algorithm is then extended to any type of data in a generic way. It is inspired from pattern structure theory, where data are locally described by predicates independent of their types, allowing the management of heterogeneous data."
"The Internet-of-Things (IoT) generates vast quantities of data, much of it attributable to individuals' activity and behaviour. Gathering personal data and performing machine learning tasks on this data in a central location presents a significant privacy risk to individuals as well as challenges with communicating this data to the cloud. However, analytics based on machine learning and in particular deep learning benefit greatly from large amounts of data to develop high-performance predictive models. This work reviews federated learning as an approach for performing machine learning on distributed data with the goal of protecting the privacy of user-generated data as well as reducing communication costs associated with data transfer. We survey a wide variety of papers covering communication-efficiency, client heterogeneity and privacy preserving methods that are crucial for federated learning in the context of the IoT. Throughout this review, we identify the strengths and weaknesses of different methods applied to federated learning and finally, we outline future directions for privacy preserving federated learning research, particularly focusing on IoT applications."
"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries."
"This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four integral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multilingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person's voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms."
"Monte Carlo event generators are an essential tool for data analysis in collider physics. To include subleading quantum corrections, these generators often need to produce negative weight events, which leads to statistical dilution of the datasets and downstream computational costs for detector simulation. Building on the recent proposal of a positive resampler method to rebalance weights within histogram bins, we introduce neural resampling: an unbinned approach to Monte Carlo reweighting based on neural networks that scales well to high-dimensional and variable-dimensional phase space. We pay particular attention to preserving the statistical properties of the event sample, such that neural resampling not only maintains the mean value of any observable but also its Monte Carlo uncertainty. This uncertainty preservation scheme is general and can also be applied to binned (non-neural network) resampling. To illustrate our neural resampling approach, we present a case study from the Large Hadron Collider of top quark pair production at next-to-leading order matched to a parton shower."
"Category systems are central components of knowledge bases, as they provide a hierarchical grouping of semantically related concepts and entities. They are a unique and valuable resource that is utilized in a broad range of information access tasks. To aid knowledge editors in the manual process of expanding a category system, this paper presents a method of generating categories for sets of entities. First, we employ neural abstractive summarization models to generate candidate categories. Next, the location within the hierarchy is identified for each candidate. Finally, structure-, content-, and hierarchy-based features are used to rank candidates to identify by the most promising ones (measured in terms of specificity, hierarchy, and importance). We develop a test collection based on Wikipedia categories and demonstrate the effectiveness of the proposed approach."
"We propose a hybrid seesaw model based on $A_{4}$ flavor symmetry, which generates a large hierarchical flavor structure. In our model, tree-level and one-loop seesaw mechanisms predict different flavor structures in the neutrino mass matrix, and generate a notable hierarchy among them. We find that such a hierarchical structure gives a large effective neutrino mass which can be accessible by next-generation neutrinoless double beta decay experiments. Majorana phases can also be predictable. The $A_{4}$ flavor symmetry in the model is spontaneously broken to the $Z_{2}$ symmetry, leading to a dark matter candidate which is assumed to be a neutral scalar field. The favored mass region of the dark matter is obtained by numerical computations of the relic abundance and the cross section of the nucleon. We also investigate the predictions of the several hierarchical flavor structures based on $A_{4}$ symmetry for the effective neutrino mass and the Majorana phases, and find the characteristic features depending on the hierarchical structures."
"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS and BART on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly."
"The Smart grid (SG), generally known as the next-generation power grid emerged as a replacement for ill-suited power systems in the 21st century. It is in-tegrated with advanced communication and computing capabilities, thus it is ex-pected to enhance the reliability and the efficiency of energy distribution with minimum effects. With the massive infrastructure it holds and the underlying communication network in the system, it introduced a large volume of data that demands various techniques for proper analysis and decision making. Big data analytics, machine learning (ML), and deep learning (DL) plays a key role when it comes to the analysis of this massive amount of data and generation of valuable insights. This paper explores and surveys the Smart grid architectural elements, machine learning, and deep learning-based applications and approaches in the context of the Smart grid. In addition in terms of machine learning-based data an-alytics, this paper highlights the limitations of the current research and highlights future directions as well."
"Nongaussian statistics are a powerful discriminant between inflationary models, particularly those with noncanonical kinetic terms. Focusing on theories where the Lagrangian is an arbitrary Lorentz-invariant function of a scalar field and its first derivatives, we review and extend the calculation of the observable three-point function. We compute the ""next-order"" slow-roll corrections to the bispectrum in closed form, and obtain quantitative estimates of their magnitude in DBI and power-law k-inflation. In the DBI case our results enable us to estimate corrections from the shape of the potential and the warp factor: these can be of order several tens of percent. We track the possible sources of large logarithms which can spoil ordinary perturbation theory, and use them to obtain a general formula for the scale dependence of the bispectrum. Our result satisfies the next-order version of Maldacena's consistency condition and an equivalent consistency condition for the scale dependence. We identify a new bispectrum shape available at next-order, which is similar to a shape encountered in Galileon models. If fNL is sufficiently large this shape may be independently detectable."
"Discrete data such as counts of microbiome taxa resulting from next-generation sequencing are routinely encountered in bioinformatics. Taxa count data in microbiome studies are typically high-dimensional, over-dispersed, and can only reveal relative abundance therefore being treated as compositional. Analyzing compositional data presents many challenges because they are restricted on a simplex. In a logistic normal multinomial model, the relative abundance is mapped from a simplex to a latent variable that exists on the real Euclidean space using the additive log-ratio transformation. While a logistic normal multinomial approach brings in flexibility for modeling the data, it comes with a heavy computational cost as the parameter estimation typically relies on Bayesian techniques. In this paper, we develop a novel mixture of logistic normal multinomial models for clustering microbiome data. Additionally, we utilize an efficient framework for parameter estimation using variational Gaussian approximations (VGA). Adopting a variational Gaussian approximation for the posterior of the latent variable reduces the computational overhead substantially. The proposed method is illustrated on simulated and real datasets."
"A systematic approach to finding variational approximation in an otherwise intractable non-conjugate model is to exploit the general principle of convex duality by minorizing the marginal likelihood that renders the problem tractable. While such approaches are popular in the context of variational inference in non-conjugate Bayesian models, theoretical guarantees on statistical optimality and algorithmic convergence are lacking. Focusing on logistic regression models, we provide mild conditions on the data generating process to derive non-asymptotic upper bounds to the risk incurred by the variational optima. We demonstrate that these assumptions can be completely relaxed if one considers a slight variation of the algorithm by raising the likelihood to a fractional power. Next, we utilize the theory of dynamical systems to provide convergence guarantees for such algorithms in logistic and multinomial logit regression. In particular, we establish local asymptotic stability of the algorithm without any assumptions on the data-generating process. We explore a special case involving a semi-orthogonal design under which a global convergence is obtained. The theory is further illustrated using several numerical studies."
"The magnetic and convective nature of the Sun's photosphere provides a unique platform from which generated waves can be modelled, observed, and interpreted across a wide breadth of spatial and temporal scales. As oscillations are generated in-situ or emerge through the photospheric layers, the interplay between the rapidly evolving densities, temperatures, and magnetic field strengths provides dynamic evolution of the embedded wave modes as they propagate into the tenuous solar chromosphere. A focused science team was assembled to discuss the current challenges faced in wave studies in the lower solar atmosphere, including those related to spectropolarimetry and radiative transfer in the optically thick regions. Following the Theo Murphy international scientific meeting held at Chicheley Hall during February 2020, the scientific team worked collaboratively to produce 15 independent publications for the current Special Issue, which are introduced here. Implications from the current research efforts are discussed in terms of upcoming next-generation observing and high performance computing facilities."
"Super-high-\k{appa} materials that exhibit exceptionally high dielectric permittivity are recognized as potential candidates for a wide range of next-generation photonic and electronic devices. Generally, the high dielectricity for achieving a high-\k{appa} state requires a low symmetry of materials so that most of the discovered high-\k{appa} materials are symmetry-broken crystals. There are scarce reports on fluidic high-\k{appa} dielectrics. Here we demonstrate a rational molecular design, supported by machine-learning analyses, that introduces high polarity to asymmetric molecules, successfully realizing super-high-\k{appa} fluid materials (dielectric permittivity, {\epsilon} > 104) and strong second harmonic generation with macroscopic spontaneous polar ordering. The polar structures are confirmed to be identical for all the synthesized materials. Our experiments and computational calculation reveal the unique orientational structures coupled with the emerging polarity. Furthermore, adopting this strategy to high-molecular-weight systems additionally extends the novel material category from monomer to polar polymer materials, creating polar soft matters with spontaneous symmetry breaking."
"The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models."
"Because of the invisible human keypoints in images caused by illumination, occlusion and overlap, it is likely to produce unreasonable human pose prediction for most of the current human pose estimation methods. In this paper, we design a novel generative adversarial network (GAN) to improve the localization accuracy of visible joints when some joints are invisible. The network consists of two simple but efficient modules, Cascade Feature Network (CFN) and Graph Structure Network (GSN). First, the CFN utilizes the prediction maps from the previous stages to guide the prediction maps in the next stage to produce accurate human pose. Second, the GSN is designed to contribute to the localization of invisible joints by passing message among different joints. According to GAN, if the prediction pose produced by the generator G cannot be distinguished by the discriminator D, the generator network G has successfully obtained the underlying dependence of human joints. We conduct experiments on three widely used human pose estimation benchmark datasets, LSP, MPII and COCO, whose results show the effectiveness of our proposed framework."
"Commonly, machine learning models minimize an empirical expectation. As a result, the trained models typically perform well for the majority of the data but the performance may deteriorate in less dense regions of the dataset. This issue also arises in generative modeling. A generative model may overlook underrepresented modes that are less frequent in the empirical data distribution. This problem is known as complete mode coverage. We propose a sampling procedure based on ridge leverage scores which significantly improves mode coverage when compared to standard methods and can easily be combined with any GAN. Ridge leverage scores are computed by using an explicit feature map, associated with the next-to-last layer of a GAN discriminator or of a pre-trained network, or by using an implicit feature map corresponding to a Gaussian kernel. Multiple evaluations against recent approaches of complete mode coverage show a clear improvement when using the proposed sampling strategy."
"Optical wireless communications (OWCs) have been recognized as a candidate enabler of next generation in-body nano-scale networks and implants. The development of an accurate channel model capable of accommodating the particularities of different type of tissues is expected to boost the design of optimized communication protocols for such applications. Motivated by this, this paper focuses on presenting a general pathloss model for in-body OWCs. In particular, we use experimental measurements in order to extract analytical expressions for the absorption coefficients of the five main tissues' constitutions, namely oxygenated and de-oxygenated blood, water, fat, and melanin. Building upon these expressions, we derive a general formula for the absorption coefficient evaluation of any biological tissue. To verify the validity of this formula, we compute the absorption coefficient of complex tissues and compare them against respective experimental results reported by independent research works. Interestingly, we observe that the analytical formula has high accuracy and is capable of modeling the pathloss and, therefore, the penetration depth in complex tissues."
"A flip-swap language is a set S of binary strings of length n such that $S \cup 0^n$ is closed under two operations (when applicable): (1) Flip the leftmost 1; and (2) Swap the leftmost 1 with the bit to its right. Flip-swap languages model many combinatorial objects including necklaces, Lyndon words, prefix normal words, left factors of k-ary Dyck words, and feasible solutions to 0-1 knapsack problems. We prove that any flip-swap language forms a cyclic 2-Gray code when listed in binary reflected Gray code (BRGC) order. Furthermore, a generic successor rule computes the next string when provided with a membership tester. The rule generates each string in the aforementioned flip-swap languages in O(n)-amortized per string, except for prefix normal words of length n which require O($n^{1.864}$)-amortized per string. Our work generalizes results on necklaces and Lyndon words by Vajnovski [Inf. Process. Lett. 106(3):96$-$99, 2008]."
"Continuous-variable quantum information processing through quantum optics offers a promising platform for building the next generation of scalable fault-tolerant information processors. To achieve quantum computational advantages and fault tolerance, non-Gaussian resources are essential. In this work, we propose and analyze a method to generate a variety of non-Gaussian states using coherent photon subtraction from a two-mode squeezed state followed by photon-number-resolving measurements. The proposed method offers a promising way to generate rotation-symmetric states conventionally used for quantum error correction with binomial codes and truncated Schr\""{o}dinger cat codes. We consider the deleterious effects of experimental imperfections such as detection inefficiencies and losses in the state engineering protocol. Our method can be readily implemented with current quantum photonic technologies."
"In this work we introduce NWT, an expressive speech-to-video model. Unlike approaches that use domain-specific intermediate representations such as pose keypoints, NWT learns its own latent representations, with minimal assumptions about the audio and video content. To this end, we propose a novel discrete variational autoencoder with adversarial loss, dVAE-Adv, which learns a new discrete latent representation we call Memcodes. Memcodes are straightforward to implement, require no additional loss terms, are stable to train compared with other approaches, and show evidence of interpretability. To predict on the Memcode space, we use an autoregressive encoder-decoder model conditioned on audio. Additionally, our model can control latent attributes in the generated video that are not annotated in the data. We train NWT on clips from HBO's Last Week Tonight with John Oliver. NWT consistently scores above other approaches in Mean Opinion Score (MOS) on tests of overall video naturalness, facial naturalness and expressiveness, and lipsync quality. This work sets a strong baseline for generalized audio-to-video synthesis. Samples are available at https://next-week-tonight.github.io/NWT/."
"Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55% and a recall of 97.92%. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size."
"In this work, we present a simple yet effective framework to address the domain translation problem between different sensor modalities with unique data formats. By relying only on the semantics of the scene, our modular generative framework can, for the first time, synthesize a panoramic color image from a given full 3D LiDAR point cloud. The framework starts with semantic segmentation of the point cloud, which is initially projected onto a spherical surface. The same semantic segmentation is applied to the corresponding camera image. Next, our new conditional generative model adversarially learns to translate the predicted LiDAR segment maps to the camera image counterparts. Finally, generated image segments are processed to render the panoramic scene images. We provide a thorough quantitative evaluation on the SemanticKitti dataset and show that our proposed framework outperforms other strong baseline models.   Our source code is available at https://github.com/halmstad-University/TITAN-NET"
"Speckle patterns generated in a disordered medium carry a lot of information despite the complete randomness in the intensity pattern. When the medium possesses $\chi^{(2)}$ nonlinearity, the speckle is sensitive to the phase of the incident fundamental light, as well as the light generated within. Here, we examine the speckle decorrelation in the fundamental and second-harmonic transmitted light as a function of varying power in the fundamental beam. At low powers, the speckle exhibits strong spatial correlations, which decrease with increasing incident power. We measure the statistical distributions of the correlation coefficients, which transform from sharp-peaked distributions at low power, to wide flat distributions at higher power. The average correlation in the second-harmonic speckle decays faster than in the fundamental speckle. Next, we construct a theoretical model, backed up by numerical computations, to obtain deeper physical insights on the faster decorrelations in the second-harmonic light. Whilst providing excellent qualitative agreement with the experiments, the model sheds important light on the contribution of two effects in the correlations, namely, the generation of second-harmonic light, and the propagation thereof."
"To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization."
"Understanding how laser light scatters from realistic mirror surfaces is crucial for the design, com- missioning and operation of precision interferometers, such as the current and next generation of gravitational-wave detectors. Numerical simulations are indispensable tools for this task but their utility can in practice be limited by the computational cost of describing the scattering process. In this paper we present an efficient method to significantly reduce the computational cost of optical simulations that incorporate scattering. This is accomplished by constructing a near optimal representation of the complex, multi-parameter 2D overlap integrals that describe the scattering process (referred to as a reduced order quadrature). We demonstrate our technique by simulating a near-unstable Fabry-Perot cavity and its control signals using similar optics to those installed in one of the LIGO gravitational-wave detectors. We show that using reduced order quadrature reduces the computational time of the numerical simulation from days to minutes (a speed-up of $\approx 2750 \times$) whilst incurring negligible errors. This significantly increases the feasibility of modelling interferometers with realistic imperfections to overcome current limits in state-of-the-art optical systems. Whilst we focus on the Hermite-Gaussian basis for describing the scattering of the optical fields, our method is generic and could be applied with any suitable basis. An implementation of this reduced order quadrature method is provided in the open source interferometer simulation software Finesse."
"A new class of Second generation high-performance computing applications with heterogeneous, dynamic and data-intensive properties have an extended set of requirements, which cover application deployment, resource allocation, -control, and I/O scheduling. These requirements are not met by the current production HPC platform models and policies. This results in a loss of opportunity, productivity and innovation for new computational methods and tools. It also decreases effective system utilization for platform providers due to unsupervised workarounds and rogue resource management strategies implemented in application space. In this paper we critically discuss the dominant HPC platform model and describe the challenges it creates for second generation applications because of its asymmetric resource view, interfaces and software deployment policies. We present an extended, more symmetric and application-centric platform model that adds decentralized deployment, introspection, bidirectional control and information flow and more comprehensive resource scheduling. We describe cHPC: an early prototype of a non-disruptive implementation based on Linux Containers (LXC). It can operate alongside existing batch queuing systems and exposes a symmetric platform API without interfering with existing applications and usage modes. We see our approach as a viable, incremental next step in HPC platform evolution that benefits applications and platform providers alike. To demonstrate this further, we layout out a roadmap for future research and experimental evaluation."
"A new generation of educational mathematics software is being shaped in ThEdu and other academic communities on the side of computer mathematics. Respective concepts and technologies have been clarified to an extent, which calls for cooperation with educational sciences in order to optimise the new generation's impact on educational practice. The paper addresses educational scientists who want to examine specific software features and estimate respective effects in STEM education at universities and subsequently at high-school. The key features are characterised as a ""complete, transparent and interactive model of mathematics"", which offers interactive experience in all relevant aspects in doing mathematics. Interaction uses several layers of formal languages: the language of terms, of specifications, of proofs and of program language, which are connected by Lucas-Interpretation providing ""next-step-guidance"" as well as providing prover power to check user input. So this paper is structured from the point of view of computer mathematics and thus cannot give a serious description of effects on educational practice -- this is up to collaboration with educational science; such collaboration is prepared by a series of questions, some of which are biased towards software usability (and mainly to be solved by computer mathematicians) and some of which are biased towards genuine research in educational sciences."
"Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular function in the streaming setting with a cardinality constraint $k$. We first propose Sieve-Streaming++, which requires just one pass over the data, keeps only $O(k)$ elements and achieves the tight $(1/2)$-approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal $(1/4)$-approximation with $\Theta(k)$ memory or the optimal $(1/2)$-approximation with $O(k\log k)$ memory. Next, we show that by buffering a small fraction of the stream and applying a careful filtering procedure, one can heavily reduce the number of adaptive computational rounds, thus substantially lowering the computational complexity of Sieve-Streaming++. We then generalize our results to the more challenging multi-source streaming setting. We show how one can achieve the tight $(1/2)$-approximation guarantee with $O(k)$ shared memory while minimizing not only the required rounds of computations but also the total number of communicated bits. Finally, we demonstrate the efficiency of our algorithms on real-world data summarization tasks for multi-source streams of tweets and of YouTube videos."
"We present a computational and theoretical model of the neural mechanisms underlying human decision-making. We propose a detailed model of the interaction between brain regions, under a proposer-predictor-actor-critic framework. Task-relevant areas of cortex propose a candidate plan using fast, model-free, parallel constraint-satisfaction computations. Other areas of cortex and medial temporal lobe can then predict likely outcomes of that plan in this situation. This step is optional. This prediction-(or model-) based computation produces better accuracy and generalization, at the expense of speed. Next, linked regions of basal ganglia act to accept or reject the proposed plan based on its reward history in similar contexts. Finally the reward-prediction system acts as a critic to determine the value of the outcome relative to expectations, and produce dopamine as a training signal for cortex and basal ganglia. This model gains many constraints from the hypothesis that the mechanisms of complex human decision-making are closely analogous to those that have been empirically studied in detail for animal action-selection. We argue that by operating sequentially and hierarchically, these same mechanisms are responsible for the most complex human plans and decisions. Finally, we use the computational model to generate novel hypotheses on causes of human risky decision-making, and compare this to other theories of human decision-making."
"In the age of post-Moore era, the next-generation computing model would be a hybrid architecture consisting of different physical components such as photonic chips. In 2008, it has been proposed that the solving of NAND-tree problem can be sped up by quantum walk. Such scheme is groundbreaking due to the universality of NAND gate. However, experimental demonstration has never been achieved so far, mostly due to the challenge in preparing the propagating initial state. Here we propose an alternative solution by including a structure called ""quantum slide"", where a propagating Gaussian wave-packet can be generated deterministically along a properly-engineered chain. In this way, the optical computation can be achieved with ordinary laser light instead of single photon, and the output can be obtained by single-shot measurements instead of repeated quantum measurements. In our experimental demonstration, the optical NAND-tree is capable of solving computational problems with a total of four input bits, based on the femtosecond laser 3D direct-writing technique on a photonic chip. These results remove one main roadblock to photonic NAND-tree computation, and the construction of quantum slide may find other interesting applications in quantum information and quantum optics."
"We introduce a method to automatically compute LEGO Technic models from user input sketches, optionally with motion annotations. The generated models resemble the input sketches with coherently-connected bricks and simple layouts, while respecting the intended symmetry and mechanical properties expressed in the inputs. This complex computational assembly problem involves an immense search space, and a much richer brick set and connection mechanisms than regular LEGO. To address it, we first comprehensively model the brick properties and connection mechanisms, then formulate the construction requirements into an objective function, accounting for faithfulness to input sketch, model simplicity, and structural integrity. Next, we model the problem as a sketch cover, where we iteratively refine a random initial layout to cover the input sketch, while guided by the objective. At last, we provide a working system to analyze the balance, stress, and assemblability of the generated model. To evaluate our method, we compared it with four baselines and professional designs by a LEGO expert, demonstrating the superiority of our automatic designs. Also, we recruited several users to try our system, employed it to create models of varying forms and complexities, and physically built most of them."
"The rise of digital payments has caused consequential changes in the financial crime landscape. As a result, traditional fraud detection approaches such as rule-based systems have largely become ineffective. AI and machine learning solutions using graph computing principles have gained significant interest in recent years. Graph-based techniques provide unique solution opportunities for financial crime detection. However, implementing such solutions at industrial-scale in real-time financial transaction processing systems has brought numerous application challenges to light. In this paper, we discuss the implementation difficulties current and next-generation graph solutions face. Furthermore, financial crime and digital payments trends indicate emerging challenges in the continued effectiveness of the detection techniques. We analyze the threat landscape and argue that it provides key insights for developing graph-based solutions."
"We present an adaptive methodology for the solution of (linear and) non-linear time dependent problems that is especially tailored for massively parallel computations. The basic concept is to solve for large blocks of space-time unknowns instead of marching sequentially in time. The methodology is a combination of a computationally efficient implementation of a parallel-in-space-time finite element solver coupled with a posteriori space-time error estimates and a parallel mesh generator. This methodology enables, in principle, simultaneous adaptivity in both space and time (within the block) domains. We explore this basic concept in the context of a variety of time-steppers including $\Theta$-schemes and Backward Differentiate Formulas. We specifically illustrate this framework with applications involving time dependent linear, quasi-linear and semi-linear diffusion equations. We focus on investigating how the coupled space-time refinement indicators for this class of problems affect spatial adaptivity. Finally, we show good scaling behavior up to 150,000 processors on the Blue Waters machine. This is achieved by careful usage of memory via block storage and non-zero formats along with lumped communication for matrix assembly. This methodology enables scaling on next generation multi-core machines by simultaneously solving for large number of time-steps, and reduces computational overhead by refining spatial blocks that can track localized features. This methodology also opens up the possibility of efficiently incorporating adjoint equations for error estimators and inverse design problems, since blocks of space-time are simultaneously solved and stored in memory."
"Indexing very large collections of strings, such as those produced by the widespread next generation sequencing technologies, heavily relies on multistring generalization of the Burrows-Wheeler Transform (BWT): large requirements of in-memory approaches have stimulated recent developments on external memory algorithms. The related problem of computing the Longest Common Prefix (LCP) array of a set of strings is instrumental to compute the suffix-prefix overlaps among strings, which is an essential step for many genome assembly algorithms. In a previous paper, we presented an in-memory divide-and-conquer method for building the BWT and LCP where we merge partial BWTs with a forward approach to sort suffixes. In this paper, we propose an alternative backward strategy to develop an external memory method to simultaneously build the BWT and the LCP array on a collection of m strings of different lengths. The algorithm over a set of strings having constant length k has O(mkl) time and I/O volume, using O(k + m) main memory, where l is the maximum value in the LCP array."
"Banach's fixed point theorem for contraction maps has been widely used to analyze the convergence of iterative methods in non-convex problems. It is a common experience, however, that iterative maps fail to be globally contracting under the natural metric in their domain, making the applicability of Banach's theorem limited. We explore how generally we can apply Banach's fixed point theorem to establish the convergence of iterative methods when pairing it with carefully designed metrics.   Our first result is a strong converse of Banach's theorem, showing that it is a universal analysis tool for establishing global convergence of iterative methods to unique fixed points, and for bounding their convergence rate. In other words, we show that, whenever an iterative map globally converges to a unique fixed point, there exists a metric under which the iterative map is contracting and which can be used to bound the number of iterations until convergence. We illustrate our approach in the widely used power method, providing a new way of bounding its convergence rate through contraction arguments.   We next consider the computational complexity of Banach's fixed point theorem. Making the proof of our converse theorem constructive, we show that computing a fixed point whose existence is guaranteed by Banach's fixed point theorem is CLS-complete. We thus provide the first natural complete problem for the class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capture the complexity of problems such as P-matrix LCP, computing KKT-points, and finding mixed Nash equilibria in congestion and network coordination games."
"The rapid advancement in high-throughput techniques has fueled the generation of large volume of biological data rapidly with low cost. Some of these techniques are microarray and next generation sequencing which provides genome level insight of living cells. As a result, the size of most of the biological databases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These biological data are analyzed using computational techniques for knowledge discovery - which is one of the objectives of bioinformatics research. Gene regulatory network (GRN) is a gene-gene interaction network which plays pivotal role in understanding gene regulation process and disease studies. From the last couple of decades, the researchers are interested in developing computational algorithms for GRN inference (GRNI) using high-throughput experimental data. Several computational approaches have been applied for inferring GRN from gene expression data including statistical techniques (correlation coefficient), information theory (mutual information), regression based approaches, probabilistic approaches (Bayesian networks, naive byes), artificial neural networks, and fuzzy logic. The fuzzy logic, along with its hybridization with other intelligent approach, is well studied in GRNI due to its several advantages. In this paper, we present a consolidated review on fuzzy logic and its hybrid approaches for GRNI developed during last two decades."
"A supergrid graph is a finite vertex-induced subgraph of the infinite graph whose vertex set consists of all points of the plane with integer coordinates and in which two vertices are adjacent if the difference of their x or y coordinates is not larger than 1. The Hamiltonian path (cycle) problem is to determine whether a graph contains a simple path (cycle) in which each vertex of the graph appears exactly once. This problem is NP-complete for general graphs and it is also NP-complete for general supergrid graphs. Despite the many applications of the problem, it is still open for many classes, including solid supergrid graphs and supergrid graphs with some holes. A graph is called Hamiltonian connected if it contains a Hamiltonian path between any two distinct vertices. In this paper, first we will study the Hamiltonian cycle property of C-shaped supergrid graphs, which are a special case of rectangular supergrid graphs with a rectangular hole. Next, we will show that C-shaped supergrid graphs are Hamiltonian connected except few conditions. Finally, we will compute a longest path between two distinct vertices in these graphs. The Hamiltonian connectivity of C-shaped supergrid graphs can be applied to compute the optimal stitching trace of computer embroidery machines, and construct the minimum printing trace of 3D printers with a C-like component being printed."
"In recent years, the prospects of performing fundamental and applied studies at the next-generation high-intensity laser facilities have greatly stimulated the interest in performing large-scale simulations of laser interaction with matter with the account for quantum electrodynamics (QED) processes such as emission of high energy photons and decay of such photons into electron-positron pairs. These processes can be modeled via probabilistic routines that include frequent computation of synchrotron functions and can constitute significant computational demands within accordingly extended Particle-in-Cell (QED-PIC) algorithms. In this regard, the optimization of these routines is of great interest. In this paper, we propose and describe two modifications. First, we derive a more accurate upper-bound estimate for the rate of QED events and use it to arrange local sub-stepping of the global time step in a significantly more efficient way than done previously. Second, we present a new high-performance implementation of synchrotron functions. Our optimizations made it possible to speed up the computations by a factor of up to 13.7 depending on the problem. Our implementation is integrated into the PICADOR and Hi-Chi codes, the latter of which is distributed publicly (https://github.com/hi-chi/pyHiChi)."
"A large number of problems in computer vision can be modelled as energy minimization problems in a Markov Random Field (MRF) or Conditional Random Field (CRF) framework. Graph-cuts based $\alpha$-expansion is a standard move-making method to minimize the energy functions with sub-modular pairwise terms. However, certain problems require more complex pairwise terms where the $\alpha$-expansion method is generally not applicable.   In this paper, we propose an iterative {\em tiered move making algorithm} which is able to handle general pairwise terms. Each move to the next configuration is based on the current labeling and an optimal tiered move, where each tiered move requires one application of the dynamic programming based tiered labeling method introduced in Felzenszwalb et. al. \cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum for any general pairwise potential, and we give a theoretical analysis of the properties of the algorithm, characterizing the situations in which we can expect good performance. We first evaluate our method on an object-class segmentation problem using the Pascal VOC-11 segmentation dataset where we learn general pairwise terms. Further we evaluate the algorithm on many other benchmark labeling problems such as stereo, image segmentation, image stitching and image denoising. Our method consistently gets better accuracy and energy values than alpha-expansion, loopy belief propagation (LBP), quadratic pseudo-boolean optimization (QPBO), and is competitive with TRWS."
"This paper proposes an original adaptive refinement framework using Radial Basis Functions-generated Finite Differences method. Node distributions are generated with a Poisson Disk Sampling-based algorithm from a given continuous density function, which is altered during the refinement process based on the error indicator. All elements of the proposed adaptive strategy rely only on meshless concepts, which leads to great flexibility and generality of the solution procedure. The proposed framework is tested on four gradually more complex contact problems, governed by the Cauchy-Navier equations. First, a disk under pressure is considered and the computed stress field is compared to the closed form solution of the problem to assess the basic behaviour of the algorithm and the influence of free parameters. Second, a Hertzian contact problem, also with known closed form solution, is studied to analyse the proposed algorithm with an ad-hoc error indicator and to test both refinement and derefinement. A contact problem, typical for fretting fatigue, with no known closed form solution is considered and solved next. It is demonstrated that the proposed methodology can be used in practical application and produces results comparable with FEM without the need for manual refinement or any human intervention. In the last case, generality of the proposed approach is demonstrated by solving a 3-D Boussinesq's problem of the concentrated normal traction acting on an isotropic half-space."
"Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression."
"This is the first paper of a two-long series in which we study linear generalized inverses that minimize matrix norms. Such generalized inverses are famously represented by the Moore-Penrose pseudoinverse (MPP) which happens to minimize the Frobenius norm. Freeing up the degrees of freedom associated with Frobenius optimality enables us to promote other interesting properties. In this Part I, we look at the basic properties of norm-minimizing generalized inverses, especially in terms of uniqueness and relation to the MPP. We first show that the MPP minimizes many norms beyond those unitarily invariant, thus further bolstering its role as a robust choice in many situations. We then concentrate on some norms which are generally not minimized by the MPP, but whose minimization is relevant for linear inverse problems and sparse representations. In particular, we look at mixed norms and the induced $\ell^p \rightarrow \ell^q$ norms. An interesting representative is the sparse pseudoinverse which we study in much more detail in Part II. Next, we shift attention from norms to matrices with interesting behaviors. We exhibit a class whose generalized inverse is always the MPP-even for norms that normally result in different inverses-and a class for which many generalized inverses coincide, but not with the MPP. Finally, we discuss efficient computation of norm-minimizing generalized inverses."
"We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that autonomously creates artifacts in constrained spaces according to its own evolving interestingness criterion. DeLeNoX proceeds in alternating phases of exploration and transformation. In the exploration phases, a version of novelty search augmented with constraint handling searches for maximally diverse artifacts using a given distance function. In the transformation phases, a deep learning autoencoder learns to compress the variation between the found artifacts into a lower-dimensional space. The newly trained encoder is then used as the basis for a new distance function, transforming the criteria for the next exploration phase. In the current paper, we apply DeLeNoX to the creation of spaceships suitable for use in two-dimensional arcade-style computer games, a representative problem in procedural content generation in games. We also situate DeLeNoX in relation to the distinction between exploratory and transformational creativity, and in relation to Schmidhuber's theory of creativity through the drive for compression progress."
"Cross-view video synthesis task seeks to generate video sequences of one view from another dramatically different view. In this paper, we investigate the exocentric (third-person) view to egocentric (first-person) view video generation task. This is challenging because egocentric view sometimes is remarkably different from the exocentric view. Thus, transforming the appearances across the two different views is a non-trivial task. Particularly, we propose a novel Bi-directional Spatial Temporal Attention Fusion Generative Adversarial Network (STA-GAN) to learn both spatial and temporal information to generate egocentric video sequences from the exocentric view. The proposed STA-GAN consists of three parts: temporal branch, spatial branch, and attention fusion. First, the temporal and spatial branches generate a sequence of fake frames and their corresponding features. The fake frames are generated in both downstream and upstream directions for both temporal and spatial branches. Next, the generated four different fake frames and their corresponding features (spatial and temporal branches in two directions) are fed into a novel multi-generation attention fusion module to produce the final video sequence. Meanwhile, we also propose a novel temporal and spatial dual-discriminator for more robust network optimization. Extensive experiments on the Side2Ego and Top2Ego datasets show that the proposed STA-GAN significantly outperforms the existing methods."
"Future surveys of large-scale structure will be able to measure perturbations on the scale of the cosmological horizon, and so could potentially probe a number of novel relativistic effects that are negligibly small on sub-horizon scales. These effects leave distinctive signatures in the power spectra of clustering observables and, if measurable, would open a new window on relativistic cosmology. We quantify the size and detectability of the effects for the most relevant future large-scale structure experiments: spectroscopic and photometric galaxy redshift surveys, intensity mapping surveys of neutral hydrogen, and radio continuum surveys. Our forecasts show that next-generation experiments, reaching out to redshifts $z\simeq 4$, will not be able to detect previously-undetected general-relativistic effects by using individual tracers of the density field, although the contribution of weak lensing magnification on large scales should be clearly detectable. We also perform a rigorous joint forecast for the detection of primordial non-Gaussianity through the excess power it produces in the clustering of biased tracers on large scales, finding that uncertainties of $\sigma(f_{\rm NL})\sim 1-2$ should be achievable. We study the level of degeneracy of these large-scale effects with several tracer-dependent nuisance parameters, quantifying the minimal priors on the latter that are needed for an optimal measurement of the former. Finally, we discuss the systematic effects that must be mitigated to achieve this level of sensitivity, and some alternative approaches that should help to improve the constraints. The computational tools developed to carry out this study, which requires the full-sky computation of the theoretical angular power spectra for $\mathcal{O}(100)$ redshift bins, as well as realistic models of the luminosity function, are publicly available."
"Recently a Mellin-space formula was conjectured for the form of correlation functions of $1/2$ BPS operators in planar $\mathcal{N}=4$ SYM in the strong 't Hooft coupling limit. In this work we report on the computation of two previously unknown four-point functions of operators with weights $\langle 2345 \rangle$ and $\langle 3456\rangle$, from the effective type-IIB supergravity action using AdS/CFT. These correlators are novel: they are the first correlators with all-different weights and in particular $\langle 3456\rangle$ is the first next-next-next-to-extremal correlator to ever have been computed. We also present simplifications of the known algorithm, without which these computations could not have been executed without considerable computer power. The main simplifications we found are present in the computation of the exchange Lagrangian and in the computation of $a$ tensors. After bringing our results in the appropriate form we successfully corroborate the recently conjectured formula."
"Recent analyses have calculated the minimal thermodynamic work required to perform a computation pi when two conditions hold: the output of pi is independent of its input (e.g., as in bit erasure); we use a physical computer C to implement pi that is specially tailored to the environment of C, i.e., to the precise distribution over C's inputs, P_0. First I extend these analyses to calculate the work required even if the output of pi depends on its input, and even if C is not used with the distribution P_0 it was tailored for. Next I show that if C will be re-used, then the minimal work to run it depends only on the logical computation pi, independent of the physical details of C. This establishes a formal identity between the thermodynamics of (re-usable) computers and theoretical computer science. I use this identity to prove that the minimal work required to compute a bit string sigma on a ""general purpose computer"" rather than a special purpose one, i.e., on a universal Turing machine U, is k_BT ln(2) times the sum of three terms: The Kolmogorov complexity of sigma, log of the Bernoulli measure of the set of strings that compute sigma, and log of the halting probability of U. I also prove that using C with a distribution over environments results in an unavoidable increase in the work required to run the computer, even if it is tailored to the distribution over environments. I end by using these results to relate the free energy flux incident on an organism / robot / biosphere to the maximal amount of computation that the organism / robot / biosphere can do per unit time."
"The standard wide-field imaging technique, the $w$-projection, allows correction for wide-fields of view for non-coplanar radio interferometric arrays. However, calculating exact corrections for each measurement has not been possible due to the amount of computation required at high resolution and with the large number of visibilities from current interferometers. The required accuracy and computational cost of these corrections is one of the largest unsolved challenges facing next generation radio interferometers such as the Square Kilometre Array. We show that the same calculation can be performed with a radially symmetric $w$-projection kernel, where we use one dimensional adaptive quadrature to calculate the resulting Hankel transform, decreasing the computation required for kernel generation by several orders of magnitude, whilst preserving the accuracy. We confirm that the radial $w$-projection kernel is accurate to approximately 1% by imaging the zero-spacing with an added $w$-term. We demonstrate the potential of our radially symmetric $w$-projection kernel via sparse image reconstruction, using the software package PURIFY. We develop a distributed $w$-stacking and $w$-projection hybrid algorithm. We apply this algorithm to individually correct for non-coplanar effects in 17.5 million visibilities over a $25$ by $25$ degree field of view MWA observation for image reconstruction. Such a level of accuracy and scalability is not possible with standard $w$-projection kernel generation methods. This demonstrates that we can scale to a large number of measurements with large image sizes whilst still maintaining both speed and accuracy."
"Cascaded or central-moment-based lattice Boltzmann method (CLBM) proposed in [Geier \textit{et al.}, Phys. Rev. E \textbf{63}, 066705 (2006)] possesses very good numerical stability. However, two constraints exist in three-dimensional (3D) CLBM simulations. Firstly, the conventional implementation for 3D CLBM involves cumbersome operations and requires much higher computational cost compared to the single-relaxation-time (SRT) LBM. Secondly, it is a challenge to accurately incorporate a general force field into the 3D CLBM. In this paper, we present an improved method to implement CLBM in 3D. The main strategy is to adopt a simplified central moment set, and carry out the central-moment-based collision operator based on a general multi-relaxation-time (GMRT) framework. Next, the recently proposed consistent forcing scheme in CLBM [L. Fei and K. H. Luo, Phys. Rev. E \textbf{96}, 053307 (2017)] is extended to incorporate a general force field into 3D CLBM. Compared with the recently developed non-orthogonal CLBM [A. D. Rosis, Phys. Rev. E \textbf{95}, 013310 (2017)], our implementation is proved to reduce the computational cost significantly. The inconsistency of adopting the discrete equilibrium distribution functions (EDFs) in the non-orthogonal CLBM is revealed and discussed. The 3D CLBM developed here in conjunction with the consistent forcing scheme is verified through numerical simulations of several canonical force-driven flows, highlighting very good properties in terms of accuracy, convergence and consistency with the nonslip rule. Finally, the techniques developed here for 3D CLBM can be applied to make the implementation and execution of 3D MRT-LBM much more efficient."
"We take a fresh look at analogue-digital systems focussing on their physical behaviour. We model a general analogue-digital system as a physical process controlled by an algorithm by viewing the physical process as physical oracle to the algorithm, generalising the notion of Turing. We develop a theoretical framework for the specification and analysis of such systems that combines five semantical notions: actual physical behaviour, measured behaviour, predicted behaviour, computed behaviour and exceptional behaviour. Next, we consider the more general and applicable situation of complex processes that exhibit several distinct modes of physical behaviour. Thus, for their design, a set of mathematical models may be needed, each model having its own domain of application and representing a particular mode of behaviour or operation of physical reality with its own physical oracle. The models may be of disparate kinds and, furthermore, not all physical modes may even have a reliable model. We address the questions: How do we specify algorithms and software that monitor or govern a complex physical situation with many physical modes? How do we specify a portfolio of modes, and the computational problem of transitioning from using one mode to another mode as physical behaviour changes? We propose a general definition of an analogue-digital system with modes, and show how any diverse set of modes, with or without models, can be bound together, and how the transitions between modes can be determined, by constructing a data type and mode selection functions. We illustrate the ideas of physical modes and our theory by reflecting on simple examples, including driverless racing cars."
"This paper establishes single-letter formulas for the exact entanglement cost of generating bipartite quantum states and simulating quantum channels under free quantum operations that completely preserve positivity of the partial transpose (PPT). First, we establish that the exact entanglement cost of any bipartite quantum state under PPT-preserving operations is given by a single-letter formula, here called the $\kappa$-entanglement of a quantum state. This formula is calculable by a semidefinite program, thus allowing for an efficiently computable solution for general quantum states. Notably, this is the first time that an entanglement measure for general bipartite states has been proven not only to possess a direct operational meaning but also to be efficiently computable, thus solving a question that has remained open since the inception of entanglement theory over two decades ago. Next, we introduce and solve the exact entanglement cost for simulating quantum channels in both the parallel and sequential settings, along with the assistance of free PPT-preserving operations. The entanglement cost in both cases is given by the same single-letter formula and is equal to the largest $\kappa$-entanglement that can be shared by the sender and receiver of the channel. It is also efficiently computable by a semidefinite program."
"The task of heart rate estimation using photoplethysmographic (PPG) signal is challenging due to the presence of various motion artifacts in the recorded signals. In this paper, a fast algorithm for heart rate estimation based on modified SPEctral subtraction scheme utilizing Composite Motion Artifacts Reference generation (SPECMAR) is proposed using two-channel PPG and three-axis accelerometer signals. First, the preliminary noise reduction is obtained by filtering unwanted frequency components from the recorded signals. Next, a composite motion artifacts reference generation method is developed to be employed in the proposed SPECMAR algorithm for motion artifacts reduction. The heart rate is then computed from the noise and motion artifacts reduced PPG signal. Finally, a heart rate tracking algorithm is proposed considering neighboring estimates. The performance of the SPECMAR algorithm has been tested on publicly available PPG database. The average heart rate estimation error is found to be 2.09 BPM on 23 recordings. The Pearson correlation is 0.9907. Due to low computational complexity, the method is faster than the comparing methods. The low estimation error, smooth and fast heart rate tracking makes SPECMAR an ideal choice to be implemented in wearable devices."
"In the field of multi-objective optimization algorithms, multi-objective Bayesian Global Optimization (MOBGO) is an important branch, in addition to evolutionary multi-objective optimization algorithms (EMOAs). MOBGO utilizes Gaussian Process models learned from previous objective function evaluations to decide the next evaluation site by maximizing or minimizing an infill criterion. A common criterion in MOBGO is the Expected Hypervolume Improvement (EHVI), which shows a good performance on a wide range of problems, with respect to exploration and exploitation. However, so far it has been a challenge to calculate exact EHVI values efficiently. In this paper, an efficient algorithm for the computation of the exact EHVI for a generic case is proposed. This efficient algorithm is based on partitioning the integration volume into a set of axis-parallel slices. Theoretically, the upper bound time complexities are improved from previously $O (n^2)$ and $O(n^3)$, for two- and three-objective problems respectively, to $\Theta(n\log n)$, which is asymptotically optimal. This article generalizes the scheme in higher dimensional case by utilizing a new hyperbox decomposition technique, which was proposed by D{\""a}chert et al, EJOR, 2017. It also utilizes a generalization of the multilayered integration scheme that scales linearly in the number of hyperboxes of the decomposition. The speed comparison shows that the proposed algorithm in this paper significantly reduces computation time. Finally, this decomposition technique is applied in the calculation of the Probability of Improvement (PoI)."
"Economic model predictive control (EMPC) has attracted significant attention in recent years and is recognized as a promising advanced process control method for the next generation smart manufacturing. It can lead to improving economic performance but at the same time increases the computational complexity significantly. Model approximation has been a standard approach for reducing computational complexity in process control. In this work, we perform a study on three types of representative model approximation methods applied to EMPC, including model reduction based on available first-principle models (e.g., proper orthogonal decomposition), system identification based on input-output data (e.g., subspace identification) that results in an explicitly expressed mathematical model, and neural networks based on input-output data. A representative algorithm from each model approximation method is considered. Two processes that are very different in dynamic nature and complexity were selected as benchmark processes for computational complexity and economic performance comparison, namely an alkylation process and a wastewater treatment plant (WWTP). The strengths and drawbacks of each method are summarized according to the simulation results, with future research direction regarding control oriented model approximation proposed at the end."
"We study a matrix model that has $\phi_a^i\ (a=1,2,\ldots,N,\ i=1,2,\ldots,R)$ as its dynamical variable, whose lower indices are pairwise contracted, but upper ones are not always done so. This matrix model has a motivation from a tensor model for quantum gravity, and is also related to the physics of glasses, because it has the same form as what appears in the replica trick of the spherical $p$-spin model for spin glasses, though the parameter range of our interest is different. To study the dynamics, which in general depends on $N$ and $R$, we perform Monte Carlo simulations and compare with some analytical computations in the leading and the next-leading orders. A transition region has been found around $R\sim N^2/2$, which matches a relation required by the consistency of the tensor model. The simulation and the analytical computations agree well outside the transition region, but not in this region, implying that some relevant configurations are not properly included by the analytical computations. With a motivation coming from the tensor model, we also study the persistent homology of the configurations generated in the simulations, and have observed its gradual change from $S^1$ to higher dimensional cycles with the increase of $R$ around the transition region."
"A large number of engineering, science and computational problems have yet to be solved in a computationally efficient way. One of the emerging challenges is how evolving technologies grow towards autonomy and intelligent decision making. This leads to collection of large amounts of data from various sensing and measurement technologies, e.g., cameras, smart phones, health sensors, smart electricity meters, and environment sensors. Hence, it is imperative to develop efficient algorithms for generation, analysis, classification, and illustration of data. Meanwhile, data is structured purposefully through different representations, such as large-scale networks and graphs. We focus on data science as a crucial area, specifically focusing on a curse of dimensionality (CoD) which is due to the large amount of generated/sensed/collected data. This motivates researchers to think about optimization and to apply nature-inspired algorithms, such as evolutionary algorithms (EAs) to solve optimization problems. Although these algorithms look un-deterministic, they are robust enough to reach an optimal solution. Researchers do not adopt evolutionary algorithms unless they face a problem which is suffering from placement in local optimal solution, rather than global optimal solution. In this chapter, we first develop a clear and formal definition of the CoD problem, next we focus on feature extraction techniques and categories, then we provide a general overview of meta-heuristic algorithms, its terminology, and desirable properties of evolutionary algorithms."
"In order to learn complex grammars, recurrent neural networks (RNNs) require sufficient computational resources to ensure correct grammar recognition. A widely-used approach to expand model capacity would be to couple an RNN to an external memory stack. Here, we introduce a ""neural state"" pushdown automaton (NSPDA), which consists of a digital stack, instead of an analog one, that is coupled to a neural network state machine. We empirically show its effectiveness in recognizing various context-free grammars (CFGs). First, we develop the underlying mechanics of the proposed higher order recurrent network and its manipulation of a stack as well as how to stably program its underlying pushdown automaton (PDA) to achieve desired finite-state network dynamics. Next, we introduce a noise regularization scheme for higher-order (tensor) networks, to our knowledge the first of its kind, and design an algorithm for improved incremental learning. Finally, we design a method for inserting grammar rules into a NSPDA and empirically show that this prior knowledge improves its training convergence time by an order of magnitude and, in some cases, leads to better generalization. The NSPDA is also compared to a classical analog stack neural network pushdown automaton (NNPDA) as well as a wide array of first and second-order RNNs with and without external memory, trained using different learning algorithms. Our results show that, for Dyck(2) languages, prior rule-based knowledge is critical for optimization convergence and for ensuring generalization to longer sequences at test time. We observe that many RNNs with and without memory, but no prior knowledge, fail to converge and generalize poorly on CFGs."
"Deciding what's next? is a fundamental problem in robotics and Artificial Intelligence. Under belief space planning (BSP), in a partially observable setting, it involves calculating the expected accumulated belief-dependent reward, where the expectation is with respect to all future measurements. Since solving this general un-approximated problem quickly becomes intractable, state of the art approaches turn to approximations while still calculating planning sessions from scratch. In this work we propose a novel paradigm, Incremental BSP (iX-BSP), based on the key insight that calculations across planning sessions are similar in nature and can be appropriately re-used. We calculate the expectation incrementally by utilizing Multiple Importance Sampling techniques for selective re-sampling and re-use of measurement from previous planning sessions. The formulation of our approach considers general distributions and accounts for data association aspects. We demonstrate how iX-BSP could benefit existing approximations of the general problem, introducing iML-BSP, which re-uses calculations across planning sessions under the common Maximum Likelihood assumption. We evaluate both methods and demonstrate a substantial reduction in computation time while statistically preserving accuracy. The evaluation includes both simulation and real-world experiments considering autonomous vision-based navigation and SLAM. As a further contribution, we introduce to iX-BSP the non-integral wildfire approximation, allowing one to trade accuracy for computational performance by averting from updating re-used beliefs when they are ""close enough"". We evaluate iX-BSP under wildfire demonstrating a substantial reduction in computation time while controlling the accuracy sacrifice. We also provide analytical and empirical bounds of the effect wildfire holds over the objective value."
"The cross-section for Drell-Yan production of a vector boson has been previously calculated at next-to-next-to-leading order, supplemented by enhanced logarithmic terms associated with the threshold region. In this paper, we calculate a large set of enhanced terms associated with the colour structure $C_F^3$ at N$^3$LO, for the double real emission contribution in the quark-antiquark channel, as an expansion around the threshold region up to and including the first subleading power. We perform our calculation using the method of regions, which systematically characterises all contributions according to whether the virtual gluon is (next-to) soft, collinear or hard in nature. Our results will prove useful for developing general formalisms for classifying next-to-leading power (NLP) threshold effects. They are also interesting in their own right, given that they constitute a previously unknown contribution to the Drell-Yan cross-section at ${\cal O}(\alpha_s^3)$."
"We calculate cross section and azimuthal decorrellation of Mueller Navelet jets at the LHC in the complete next-lo-leading order BFKL framework, i.e. including next-to-leading corrections to the Green's function as well as next-to-leading corrections to the Mueller Navelet vertices. The obtained results for standard observables proposed for studies of Mueller Navelet jets show that both sources of corrections are of equal, big importance for final magnitude and final behavior of observables. The astonishing conclusion of our analysis is that the observables obtained within the complete next-lo-leading order BFKL framework of the present paper are quite similar to the same observables obtained within next-to-leading logarithm DGLAP type treatment. This fact sheds doubts on general belief that the studies of Mueller Navelet jets at the LHC will lead to clear discrimination between the BFKL and the DGLAP dynamics."
"For the first time, a next-to-leading BFKL study of the cross section and azimuthal decorrellation of Mueller Navelet jets is performed, i.e. including next-to-leading corrections to the Green's function as well as next-to-leading corrections to the Mueller Navelet vertices. The obtained results for standard observables proposed for studies of Mueller Navelet jets show that both sources of corrections are of equal and big importance for final magnitude and final behavior of observables, in particular for the LHC kinematics investigated here in detail. The astonishing conclusion of our analysis is that the observables obtained within the complete next-lo-leading order BFKL framework of the present paper are quite similar to the same observables obtained within next-to-leading logarithm DGLAP type treatment. This fact sheds doubts on general belief that the studies of Mueller Navelet jets at the LHC will lead to clear discrimination between the BFKL and the DGLAP dynamics."
"We present nonperturbative fragmentation functions (FFs) for bottom-flavored ($B$) hadrons both at next-to-leading (NLO) and, for the first time, at next-to-next-to-leading order (NNLO) in the $\overline{\mathrm{MS}}$ factorization scheme with five massless quark flavors. They are determined by fitting all available experimental data of inclusive single $B$-hadron production in $e^+e^-$ annihilation, from the ALEPH, DELPHI, and OPAL Collaborations at CERN LEP1 and the SLD Collaboration at SLAC SLC.   The uncertainties in these FFs as well as in the corresponding observables are estimated using the Hessian approach.   We perform comparisons with available NLO sets of $B$-hadron FFs.   We apply our new FFs to generate theoretical predictions for the energy distribution of $B$ hadrons produced through the decay of unpolarized or polarized top quarks, to be measured at the CERN LHC."
"In a superconducting quantum processor with nearest neighbor coupling, the dispersive interaction between adjacent qubits can result in an effective next-nearest-neighbor coupling whose strength depends on the state of the intermediary qubit. Here, we theoretically explore the possibility of engineering this next-nearest-neighbor coupling to implement controlled two-qubit operations where the intermediary qubit controls the operation on the next-nearest neighbor pair of qubits. In particular, in a system comprising two types of superconducting qubits with anharmonicities of opposite-sign arranged in an -A-B-A- pattern, where the unwanted static ZZ coupling between adjacent qubits could be heavily suppressed, a switchable coupling between the next-nearest-neighbor qubits can be achieved via the intermediary qubit, the qubit state of which functions as an on/off switch for this coupling. Therefore, depending on the adopted activating scheme, various controlled two-qubit operations such as controlled-iSWAP gate can be realized, potentially enabling circuit depth reductions as to a standard decomposition approach for implementing generic quantum algorithms."
"Computational Grids are emerging as a platform for next-generation parallel and distributed computing. Large-scale parametric studies and parameter sweep applications find a natural place in the Grid?s distribution model. There is little or no communication between jobs. The task of parallelizing and distributing existing applications is conceptually trivial. These properties of parametric studies make it an ideal place to start developing integrated development environments (IDEs) for rapidly Grid-enabling applications. However, the availability of IDEs for scientists to Grid-enable their applications, without the need of developing them as parallel applications explicitly, is still lacking. This paper presents a Java based IDE called Visual Parametric Tool (VPT), developed as part of the Gridbus project, for rapid creation of parameter sweep (data parallel/SPMD) applications. It supports automatic creation of parameter script and parameterisation of the input data files, which is compatible with the Nimrod-G parameter specification language. The usefulness of VPT is demonstrated by a case study on composition of molecular docking application as a parameter sweep application. Such applications can be deployed on clusters using the Nimrod/enFuzion system and on global Grids using the Nimrod-G grid resource broker."
"We discuss a method to compute the microcanonical entropy at fixed magnetization without direct counting. Our approach is based on the evaluation of a saddle-point leading to an optimization problem. The method is applied to a benchmark Ising model with simultaneous presence of mean-field and nearest-neighbour interactions for which direct counting is indeed possible, thus allowing a comparison. Moreover, we apply the method to an Ising model with mean-field, nearest-neighbour and next-nearest-neighbour interactions, for which direct counting is not straightforward. For this model, we compare the solution obtained by our method with the one obtained from the formula for the entropy in terms of all correlation functions. This example shows that for general couplings our method is much more convenient than direct counting methods to compute the microcanonical entropy at fixed magnetization."
"With the advent of the gyrokinetic formalism, recent developments in low-noise nonlinear $\delta f$ methods, and enormous gains in computing power, large-scale gyrokinetic simulations have become an important tool for improved understanding of anomalous transport in tokamaks. Simulating the non-linear behaviour requires solving for the perturbations of distrbution function in five dimensions. We use a particle-in-cell approach to solve the equations via the non-linear characteristic method. The code has been parallelized for a variety of architecures (C90, CM-5, T3D) using a 1-D domain decomposition along the torroidal axis, for which the number of particles in each cell remains approximately constant. The quasi uniform distribution of particles, which minimizes load imbalance, coupled with the relatively small movement of particles across cells, which minimizes communications, makes this problem ideally suited to massively parallel architectures. We present the performance of the program as a function of number of processors and problem size, which demonstrates the near perfect scalability of the code. In addition, we discuss the scientific results obtained from the code and the types of problems that will be addressable as the next generation of super-computers become available."
"Large medical image data sets with high dimensionality require substantial amount of computation time for data creation and data processing. This paper presents a novel generalized method that finds optimal image-based feature sets that reduce computational time complexity while maximizing overall classification accuracy for detection of diabetic retinopathy (DR). First, region-based and pixel-based features are extracted from fundus images for classification of DR lesions and vessel-like structures. Next, feature ranking strategies are used to distinguish the optimal classification feature sets. DR lesion and vessel classification accuracies are computed using the boosted decision tree and decision forest classifiers in the Microsoft Azure Machine Learning Studio platform, respectively. For images from the DIARETDB1 data set, 40 of its highest-ranked features are used to classify four DR lesion types with an average classification accuracy of 90.1% in 792 seconds. Also, for classification of red lesion regions and hemorrhages from microaneurysms, accuracies of 85% and 72% are observed, respectively. For images from STARE data set, 40 high-ranked features can classify minor blood vessels with an accuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis systems can significantly enhance the borderline classification performances in automated screening systems."
"The past century of telecommunications has shown that failures in networks are prevalent. Although much has been done to prevent failures, network nodes and links are bound to fail eventually. Failure recovery processes are therefore needed. Failure recovery is mainly influenced by (1) detection of the failure, and (2) circumvention of the detected failure. However, especially in SDNs where controllers recompute network state reactively, this leads to high delays. Hence, next to primary rules, backup rules should be installed in the switches to quickly detour traffic once a failure occurs. In this work, we propose algorithms for computing an all-to-all primary and backup network forwarding configuration that is capable of circumventing link and node failures. Omitting the high delay invoked by controller recomputation through preconfiguration, our proposal's recovery delay is close to the detection time which is significantly below the 50 ms rule of thumb. After initial recovery, we recompute network configuration to guarantee protection from future failures. Our algorithms use packet-labeling to guarantee correct and shortest detour forwarding. The algorithms and labeling technique allow packets to return to the primary path and are able to discriminate between link and node failures. The computational complexity of our solution is comparable to that of all-to-all-shortest paths computations. Our experimental evaluation on both real and generated networks shows that network configuration complexity highly decreases compared to classic disjoint paths computations. Finally, we provide a proof-of-concept OpenFlow controller in which our proposed configuration is implemented, demonstrating that it readily can be applied in production networks."
"Cloud Computing has been envisioned as the next generation architecture of IT Enterprise. The Cloud computing concept offers dynamically scalable resources provisioned as a service over the Internet. Economic benefits are the main driver for the Cloud, since it promises the reduction of capital expenditure and operational expenditure. In order for this to become reality, however, there are still some challenges to be solved. Most important among these are security and trust issues,since the users data has to be released to the Cloud and thus leaves the protection sphere of the data owner.In contrast to traditional solutions, where the IT services are under proper physical,logical and personnel controls, Cloud Computing moves the application software and databases to the large data centers, where the management of the data and services may not be fully trustworthy. This unique attribute, however, poses many new security challenges which have not been well understood. Security is to save data from danger and vulnerability. There are so many dangers and vulnerabilities to be handled. Various security issues and some of their solution are explained and are concentrating mainly on public cloud security issues and their solutions. Data should always be encrypted when stored(using separate symmetric encryption keys)and transmitted. If this is implemented appropriately, even if another tenant can access the data, all that will appear is gibberish. So a method is proposed such that we are encrypting the whole data along with the cryptographic key."
"Cloud radio access network (C-RAN) has emerged as a potential candidate of the next generation access network technology to address the increasing mobile traffic, while mobile cloud computing (MCC) offers a prospective solution to the resource-limited mobile user in executing computation intensive tasks. Taking full advantages of above two cloud-based techniques, C-RAN with MCC are presented in this paper to enhance both performance and energy efficiencies. In particular, this paper studies the joint energy minimization and resource allocation in C-RAN with MCC under the time constraints of the given tasks. We first review the energy and time model of the computation and communication. Then, we formulate the joint energy minimization into a non-convex optimization with the constraints of task executing time, transmitting power, computation capacity and fronthaul data rates. This non-convex optimization is then reformulated into an equivalent convex problem based on weighted minimum mean square error (WMMSE). The iterative algorithm is finally given to deal with the joint resource allocation in C-RAN with mobile cloud. Simulation results confirm that the proposed energy minimization and resource allocation solution can improve the system performance and save energy."
"Medical Informatics and the application of modern signal processing in the assistance of the diagnostic process in medical imaging is one of the more recent and active research areas today. This thesis addresses a variety of issues related to the general problem of medical image analysis, specifically in mammography, and presents a series of algorithms and design approaches for all the intermediate levels of a modern system for computer-aided diagnosis (CAD). The diagnostic problem is analyzed with a systematic approach, first defining the imaging characteristics and features that are relevant to probable pathology in mammo-grams. Next, these features are quantified and fused into new, integrated radio-logical systems that exhibit embedded digital signal processing, in order to improve the final result and minimize the radiological dose for the patient. In a higher level, special algorithms are designed for detecting and encoding these clinically interest-ing imaging features, in order to be used as input to advanced pattern classifiers and machine learning models. Finally, these approaches are extended in multi-classifier models under the scope of Game Theory and optimum collective deci-sion, in order to produce efficient solutions for combining classifiers with minimum computational costs for advanced diagnostic systems. The material covered in this thesis is related to a total of 18 published papers, 6 in scientific journals and 12 in international conferences."
"Cloud Radio Access Network (C-RAN) is emerging as a transformative architecture for the next generation of mobile cellular networks. In C-RAN, the Baseband Unit (BBU) is decoupled from the Base Station (BS) and consolidated in a centralized processing center. While the potential benefits of C-RAN have been studied extensively from the theoretical perspective, there are only a few works that address the system implementation issues and characterize the computational requirements of the virtualized BBU. In this paper, a programmable C-RAN testbed is presented where the BBU is virtualized using the OpenAirInterface (OAI) software platform, and the eNodeB and User Equipment (UEs) are implemented using USRP boards. Extensive experiments have been performed in a FDD downlink LTE emulation system to characterize the performance and computing resource consumption of the BBU under various conditions. It is shown that the processing time and CPU utilization of the BBU increase with the channel resources and with the Modulation and Coding Scheme (MCS) index, and that the CPU utilization percentage can be well approximated as a linear increasing function of the maximum downlink data rate. These results provide real-world insights into the characteristics of the BBU in terms of computing resource and power consumption, which may serve as inputs for the design of efficient resource-provisioning and allocation strategies in C-RAN systems."
"With the emergence of industrial IoT and cloud computing, and the advent of 5G and edge clouds, there are ambitious expectations on elasticity, economies of scale, and fast time to market for demanding use cases in the next generation of ICT networks. Responsiveness and reliability of wireless communication links and services in the cloud are set to improve significantly as the concept of edge clouds is becoming more prevalent. To enable industrial uptake we must provide cloud capacity in the networks but also a sufficient level of simplicity and self-sustainability in the software platforms. In this paper, we present a research test-bed built to study mission-critical control over the distributed edge cloud. We evaluate system properties using a conventional control application in the form of a Model Predictive Controller. Our cloud platform provides the means to continuously operate our mission-critical application while seamlessly relocating computations across geographically dispersed compute nodes. Through our use of 5G wireless radio, we allow for mobility and reliably provide compute resources with low latency, at the edge. The primary contribution of this paper is a state-of-the art, fully operational test-bed showing the potential for merged IoT, 5G, and cloud. We also provide an evaluation of the system while operating a mission-critical application and provide an outlook on a novel research direction."
"Mollusk shells are an ideal model system for understanding the morpho-elastic basis of morphological evolution of invertebrates' exoskeletons. During the formation of the shell, the mantle tissue secretes proteins and minerals that calcify to form a new incremental layer of the exoskeleton. Most of the existing literature on the morphology of mollusks is descriptive. The mathematical understanding of the underlying coupling between pre-existing shell morphology, de novo surface deposition and morpho-elastic volume growth is at a nascent stage, primarily limited to reduced geometric representations. Here, we propose a general, three-dimensional computational framework coupling pre-existing morphology, incremental surface growth by accretion, and morpho-elastic volume growth. We exercise this framework by applying it to explain the stepwise morphogenesis of seashells during growth: new material surfaces are laid down by accretive growth on the mantle whose form is determined by its morpho-elastic growth. Calcification of the newest surfaces extends the shell as well as creates a new scaffold that constrains the next growth step. We study the effects of surface and volumetric growth rates, and of previously deposited shell geometries on the resulting modes of mantle deformation, and therefore of the developing shell's morphology. Connections are made to a range of complex shells ornamentations."
"HEPCloud is rapidly becoming the primary system for provisioning compute resources for all Fermilab-affiliated experiments. In order to reliably meet the peak demands of the next generation of High Energy Physics experiments, Fermilab must plan to elastically expand its computational capabilities to cover the forecasted need. Commercial cloud and allocation-based High Performance Computing (HPC) resources both have explicit and implicit costs that must be considered when deciding when to provision these resources, and at which scale. In order to support such provisioning in a manner consistent with organizational business rules and budget constraints, we have developed a modular intelligent decision support system (IDSS) to aid in the automatic provisioning of resources spanning multiple cloud providers, multiple HPC centers, and grid computing federations. In this paper, we discuss the goals and architecture of the HEPCloud Facility, the architecture of the IDSS, and our early experience in using the IDSS for automated facility expansion both at Fermi and Brookhaven National Laboratory."
"Sequence generators obtained by linear recursions over the two-element field $\mathbb{F}_2$, i.e., $\mathbb{F}_2$-linear generators, are widely used as pseudorandom number generators. For example, the Mersenne Twister MT19937 is one of the most successful applications. An advantage of such generators is that we can assess them quickly by using theoretical criteria, such as the dimension of equidistribution with $v$-bit accuracy. To compute these dimensions, several polynomial-time lattice reduction algorithms have been proposed in the case of $\mathbb{F}_2$-linear generators.   In this paper, in order to assess non-random bit patterns in dimensions that are higher than the dimension of equidistribution with $v$-bit accuracy,we focus on the relationship between points in the Couture--L'Ecuyer dual lattices and $\mathbb{F}_2$-linear relations on the most significant $v$ bits of output sequences, and consider a new figure of merit $N_v$ based on the minimum weight of $\mathbb{F}_2$-linear relations whose degrees are minimal for $v$. Next, we numerically show that MT19937 has low-weight $\mathbb{F}_2$-linear relations in dimensions higher than 623, and show that some output vectors with specific lags are rejected or have small $p$-values in the birthday spacings tests. We also report that some variants of Mersenne Twister, such as WELL generators, are significantly improved from the perspective of $N_v$."
"System-level audit logs often play a critical role in computer forensics. They capture low-level interactions between programs and users in much detail, making them a rich source of insight and provenance on malicious user activity. However, using these logs to discover and understand malicious activities when a typical computer generates more than 2.5 million system events hourly is both compute and time-intensive. We introduce a graphical system called GrAALF for efficiently loading, storing, processing, querying, and displaying system events to support computer forensics. In comparison to other related systems such as AIQL [13] and SAQL [12], GrAALF offers the flexibility of multiple backend storage solutions, easy-to-use and intuitive querying of logs, and the ability to trace back longer sequences of system events in (near) real-time to help identify and isolate attacks. Equally important, both AIQL and SAQL are not available for public use, whereas GrAALF is open-source. GrAALF offers the choice of compactly storing the logs in main memory, in a relational database system, in a hybrid main memory-database system, and a graph-based database. We compare the responsiveness of each of these options, using multiple huge system-call log files. Next, in multiple real-world attack scenarios, we demonstrate the efficacy and usefulness of GrAALF in identifying the attack and discovering its provenance. Consequently, GrAALF offers a robust solution for analysis of audit logs to support computer forensics."
"The laptops, cell phones, and internet applications commonplace in our daily lives are all rooted in the idea of zeros and ones - in bits. This foundational element originated from the combination of mathematics and Claude Shannon's Theory of Information. Coupled with the 50-year legacy of Moore's Law, the bit has propelled the digitization of our world. In recent years, artificial intelligence systems, merging neuron-inspired biology with information, have achieved superhuman accuracy in a range of narrow classification tasks by learning from labelled data. Advancing from Narrow AI to Broad AI will encompass the unification of learning and reasoning through neuro-symbolic systems, resulting in a form of AI which will perform multiple tasks, operate across multiple domains, and learn from small quantities of multi-modal input data. Finally, the union of physics and information led to the emergence of Quantum Information Theory and the development of the quantum bit - the qubit - forming the basis of quantum computers. We have built the first programmable quantum computers, and although the technology is still in its early days, these systems offer the potential to solve problems which even the most powerful classical computers cannot. The future of computing will look fundamentally different than it has in the past. It will not be based on more and cheaper bits alone, but rather, it will be built upon bits + neurons + qubits. This future will enable the next generation of intelligent mission-critical systems and accelerate the rate of science-driven discovery."
"Improving efficiency of healthcare systems is a top national interest worldwide. However, the need of delivering scalable healthcare services to the patients while reducing costs is a challenging issue. Among the most promising approaches for enabling smart healthcare (s-health) are edge-computing capabilities and next-generation wireless networking technologies that can provide real-time and cost-effective patient remote monitoring. In this paper, we present our vision of exploiting multi-access edge computing (MEC) for s-health applications. We envision a MEC-based architecture and discuss the benefits that it can bring to realize in-network and context-aware processing so that the s-health requirements are met. We then present two main functionalities that can be implemented leveraging such an architecture to provide efficient data delivery, namely, multimodal data compression and edge-based feature extraction for event detection. The former allows efficient and low distortion compression, while the latter ensures high-reliability and fast response in case of emergency applications. Finally, we discuss the main challenges and opportunities that edge computing could provide and possible directions for future research."
"The capacity of offloading data and control tasks to the network is becoming increasingly important, especially if we consider the faster growth of network speed when compared to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, enabling additional computation/communication overlap and potentially improving overall application performance. However, sustaining bandwidths provided by next-generation networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model for in-NIC compute, where users specify handler functions that are executed on the NIC, for each incoming packet belonging to a given message or flow. It enables a CUDA-like acceleration, where the NIC is equipped with lightweight processing elements that process network packets in parallel. We investigate the architectural specialties that a sPIN NIC should provide to enable high-performance, low-power, and flexible packet processing. We introduce PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process packets at 400 Gbit/s for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of 18.5 mm 2 (22 nm FDSOI)."
"The increased energy and power density required in modern electronics poses a challenge for designing new dielectric polymer materials with high energy density while maintaining low loss at high applied electric fields. Recently, an advanced computational screening method coupled with hierarchical modelling has accelerated the identification of promising high energy density materials. It is well known that the dielectric response of polymeric materials is largely influenced by their phases and local heterogeneous structures as well as operational temperature. Such inputs are crucial to accelerate the design and discovery of potential polymer candidates. However, an efficient computational framework to probe temperature dependence of the dielectric properties of polymers, while incorporating effects controlled by their morphology is still lacking. In this paper, we propose a scalable computational framework based on reactive molecular dynamics with a valence-state aware polarizable charge model, which is capable of handling practically relevant polymer morphologies and simultaneously provide near-quantum accuracy in estimating dielectric properties of various polymer systems. We demonstrate the predictive power of our framework on high energy density polymer systems recently identified through rational experimental-theoretical co-design. Our scalable and automated framework may be used for high-throughput theoretical screenings of combinatorial large design space to identify next-generation high energy density polymer materials."
"We study implications of thermal leptogenesis for the superparticle mass spectrum. A consistent picture is obtained if the lightest superparticle is the gravitino, which can be the dominant component of cold dark matter. In the case of a long-lived charged scalar lepton as next-to-lightest superparticle, supergravity can be tested at the next generation of colliders, LHC and ILC."
"Compact binary systems with spinning components are considered. Finite size effects due to rotational deformation are taken into account. The dynamical evolution and next to leading order gravitational wave forms are calculated, taking into account the orbital motion up to the first post-Newtonian approximation."
"DARWIN is a design-study for a next-to-next generation experiment to directly detect WIMP dark matter in a detector based on a liquid xenon/liquid argon two-phase time projection chamber. This article describes the project, its goals and challenges, and presents some of the recent R&D highlights"
An extension with next-to-nearest neighbour interactions of the simplest XX spin chain with perfect state transfer (PST) is presented. The conditions for PST and entanglement generation (balanced fractional revival) can be obtained exactly and are discussed.
"We make use of the Next Generation model atmospheres by Allard et al. (1997) and Hauschildt, Allard & Baron (1999) to compute theoretical models for low and very low-mass stars for selected metallicities in the range Z= 0.0002 to 0.002. On this basis, we present theoretical predictions covering the sequence of H-burning stars as observed in galactic globulars from the faint end of the Main Sequence up to, and beyond, the cluster Turn Off. The role played by the new model atmospheres is discussed, showing that present models appear in excellent agreement with models by Baraffe et al. (1997) as computed on quite similar physical basis. One finds that the theoretical mass-luminosity relations based on this updated set of models, are in good agreement with the empirical data provided by Henry & McCarthy (1993). Comparison with HST observation discloses that the location in the Color-Magnitude diagram of the lower Main Sequence in galactic globular clusters appears again in good agreement with the predicted sensitive dependence of these sequences on the cluster metallicity."
"Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV."
"We present a survey of the cosmological applications of the next generation of weak lensing surveys, paying special attention to the computational challenges presented by the number of galaxies, $N_{gal} ~$ 10$^{5}$. We focus on optimal methods with no pixelization and derive a multigrid $P^3M$ algorithm that performs the relevant computations in $O(N_{gal} \log N_{gal})$ time. We test the algorithm by studying three applications of weak lensing surveys - convergence map reconstruction, cluster detection and $E$ and $B$ power spectrum estimation using realistic 1 deg^{2} simulations derived from N-body simulations. The map reconstruction is able to reconstruct large scale features without artifacts. Detecting clusters using only weak lensing is difficult because of line of sight contamination and noise, with low completeness if one desires low contamination of the sample. A power spectrum analysis of the convergence field is more promising and we are able to reconstruct the convergence spectrum with no loss of information down to the smallest scales. The numerical methods used here can be applied to other data sets with same $O(N\log N)$ scaling and can be generalised to a sphere."
"In this paper I report on an investigation into the problem of assigning tones to pitch contours. The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages. Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and others, I provide a parametrised F_0 prediction function P which generates F_0 values from a tone sequence, and I explore the asymptotic behaviour of downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0) values amounts to finding a tone sequence T such that P(T) {}~= X. This is a combinatorial optimisation problem, for which two non-deterministic search techniques are provided: a genetic algorithm and a simulated annealing algorithm. Finally, two implementations---one for each technique---are described and then compared using both artificial and real data for sequences of up to 20 tones. These programs can be adapted to other tone languages by adjusting the F_0 prediction function."
"We discuss an extension of the standard logical rules (functional application and abstraction) in Categorial Grammar (CG), in order to deal with some specific cases of polysemy. We borrow from Generative Lexicon theory which proposes the mechanism of {\em coercion}, next to a rich nominal lexical semantic structure called {\em qualia structure}.   In a previous paper we introduced coercion into the framework of {\em sign-based} Categorial Grammar and investigated its impact on traditional Fregean compositionality. In this paper we will elaborate on this idea, mostly working towards the introduction of a new semantic dimension. Where in current versions of sign-based Categorial Grammar only two representations are derived: a prosodic one (form) and a logical one (modelling), here we introduce also a more detaled representation of the lexical semantics. This extra knowledge will serve to account for linguistic phenomena like {\em metonymy\/}."
"In this paper I describe how miscommunication problems are dealt with in the spoken language system DIALOGOS. The dialogue module of the system exploits dialogic expectations in a twofold way: to model what future user utterance might be about (predictions), and to account how the user's next utterance may be related to previous ones in the ongoing interaction (pragmatic-based expectations). The analysis starts from the hypothesis that the occurrence of miscommunication is concomitant with two pragmatic phenomena: the deviation of the user from the expected behaviour and the generation of a conversational implicature. A preliminary evaluation of a large amount of interactions between subjects and DIALOGOS shows that the system performance is enhanced by the uses of both predictions and pragmatic-based expectations."
"An overview of computer simulations of tribology is presented. The chapter begins with a brief overview of simulation techniques and the special requirements for simulations of tribological processes. Then simple one-dimensional models of friction between crystalline surfaces are introduced to illustrate general features of friction, such as the importance of metastability and the effect of commensurability. Next two- and three-dimensional studies of dry sliding between crystalline surfaces are described, and compared to scanning probe experiments and measurements of the friction on adsorbed monolayers. Lubrication is then discussed, starting from thick films and describing the breakdown in bulk hydrodynamics as the thickness of the lubricant decreases to molecular scales. Deviations from the usual no-slip boundary condition are quantified and the onset of solid behavior in molecularly thick films is described. The possibility that solidification of thin layers of adventitious carbon is responsible for the prevalence of static friction is explored. The final sections describe stick-slip motion, tribochemical reactions, machining, and the evolution of microstructure in sliding contacts."
"Charles Babbage's vision of computing has largely been realized. We are on the verge of realizing Vannevar Bush's Memex. But, we are some distance from passing the Turing Test. These three visions and their associated problems have provided long-range research goals for many of us. For example, the scalability problem has motivated me for several decades. This talk defines a set of fundamental research problems that broaden the Babbage, Bush, and Turing visions. They extend Babbage's computational goal to include highly-secure, highly-available, self-programming, self-managing, and self-replicating systems. They extend Bush's Memex vision to include a system that automatically organizes, indexes, digests, evaluates, and summarizes information (as well as a human might). Another group of problems extends Turing's vision of intelligent machines to include prosthetic vision, speech, hearing, and other senses. Each problem is simply stated and each is orthogonal from the others, though they share some common core technologies"
"We study self-referential sentences of the type related to the Liar paradox. In particular, we consider the problem of assigning consistent fuzzy truth values to collections of self-referential sentences. We show that the problem can be reduced to the solution of a system of nonlinear equations. Furthermore, we prove that, under mild conditions, such a system always has a solution (i.e. a consistent truth value assignment) and that, for a particular implementation of logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution is always consistent. Next we turn to computational issues and present several truth-value assignment algorithms; we argue that these algorithms can be understood as generalized sequential reasoning. In an Appendix we present a large number of examples of self-referential collections (including the Liar and the Strengthened Liar), we formulate the corresponding truth value equations and solve them analytically and/ or numerically."
"We consider a the general online convex optimization framework introduced by Zinkevich. In this setting, there is a sequence of convex functions. Each period, we must choose a signle point (from some feasible set) and pay a cost equal to the value of the next function on our chosen point. Zinkevich shows that, if the each function is revealed after the choice is made, then one can achieve vanishingly small regret relative the best single decision chosen in hindsight.   We extend this to the bandit setting where we do not find out the entire functions but rather just their value at our chosen point. We show how to get vanishingly small regret in this setting.   Our approach uses a simple approximation of the gradient that is computed from evaluating a function at a single (random) point. We show that this estimate is sufficient to mimic Zinkevich's gradient descent online analysis, with access to the gradient (only being able to evaluate the function at a single point)."
"We analyze the ability of peer to peer networks to deliver a complete file among the peers. Early on we motivate a broad generalization of network behavior organizing it into one of two successive phases. According to this view the network has two main states: first centralized - few sources (roots) hold the complete file, and next distributed - peers hold some parts (chunks) of the file such that the entire network has the whole file, but no individual has it. In the distributed state we study two scenarios, first, when the peers are ``patient'', i.e, do not leave the system until they obtain the complete file; second, peers are ``impatient'' and almost always leave the network before obtaining the complete file."
"We extend the WKB method for the computation of cosmological perturbations during inflation beyond leading order and provide the power spectra of scalar and tensor perturbations to second order in the slow-roll parameters. Our method does not require that the slow-roll parameters be constant. Although leading and next-to-leading results in the slow-roll parameters depend on the approximation technique used in the computation, we find that the inflationary theoretical predictions obtained may reach the accuracy required by planned observations. In two technical appendices, we compare our techniques and results with previous findings."
We compute the total cross-section for direct production of the pseudoscalar Higgs boson in hadron collisions at next-to-next-to-leading order (NNLO) in perturbative QCD. The O(alpha_s^2) QCD corrections increase the NLO production cross-section by approximately 20-30 per cent.
"We present a parallel FFT algorithm for SIMD systems following the `Transpose Algorithm' approach. The method is based on the assignment of the data field onto a 1-dimensional ring of systolic cells. The systolic array can be universally mapped onto any parallel system. In particular for systems with next-neighbour connectivity our method has the potential to improve the efficiency of matrix transposition by use of hyper-systolic communication. We have realized a scalable parallel FFT on the APE100/Quadrics massively parallel computer, where our implementation is part of a 2-dimensional hydrodynamics code for turbulence studies. A possible generalization to 4-dimensional FFT is presented, having in mind QCD applications."
"We present the current status of the apeNEXT project. Aim of this project is the development of the next generation of APE machines which will provide multi-teraflop computing power. Like previous machines, apeNEXT is based on a custom designed processor, which is specifically optimized for simulating QCD. We discuss the machine design, report on benchmarks, and give an overview on the status of the software development."
"A path integral formalism is developed to study the interaction of an arbitrary curved Dirichlet (D-) string with elementary excitations of the fundumental (F-) string in bosonic string theory. Up to the next to leading order in the derivative expansion, we construct the properly renormalized vertex operator, which generalizes the one previously obtained for a D-particle moving along a curved trajectory. Using this vertex, an attempt is further made to quantize the D-string coordinates and to compute the quantum amplitude for scattering between elementary excitations of the D- and F-strings. By studying the dependence on the Liouville mode for the D-string, it is found that the vertex in our approximation consists of an infinite tower of local vertex operators which are conformally invariant on their respective mass-shell. This analysis indicates that, unlike the D-particle case, an off-shell extension of the interaction vertex would be necessary to compute the full amplitude and that the realization of symmetry can be quite non-trivial when the dual extended objects are simultaneously present. Possible future directions are suggested."
"The exact expression for Wilson loop averages winding n times on a closed contour is obtained in two dimensions for pure U(N) Yang-Mills theory and, rather surprisingly, it displays an interesting duality in the exchange $n \leftrightarrow N$. The large-N limit of our result is consistent with previous computations. Moreover we discuss the limit of small loop area ${\cal A}$, keeping $n^2 {\cal A}$ fixed, and find it coincides with the zero-instanton approximation. We deduce that small loops, both at finite and infinite ""volume"", are blind to instantons. Next we check the non-perturbative result by resumming 't Hooft-CPV and Wu-Mandelstam-Leibbrandt (WML)-prescribed perturbative series, the former being consistent with the exact result, the latter reproducing the zero-instanton contribution. A curious interplay between geometry and algebraic invariants is observed. Finally we compute the spectral density of the Wilson loop operator, at large $N$, via its Fourier representation, both for 't Hooft and WML: for small area they exhibit a gap and coincide when the theory is considered on the sphere $S^2$."
"We show how the holomorphic anomaly found in hep-th/0409245 can be used to efficiently compute certain classes of unitarity cuts of one-loop N=4 amplitudes of gluons. These classes include all cuts of n-gluon one-loop MHV amplitudes and of n-gluon next-to-MHV amplitudes with helicities (1+,2+,3+,4-,..., n-). As an application of this method, we present the explicit computation of the (1,2,3)-cut of the n-gluon one-loop N=4 leading-color amplitude A_{n;1}(1+,2+,3+,4-,..., n-). The answer is given in terms of scalar box functions and provides information about the corresponding amplitudes. A possible way to generalize this method to all kinds of unitarity cuts is also discussed."
"We examine the gyromagnetic ratios of rotating and charged AdS black holes in four and higher spacetime dimensions. We compute the gyromagnetic ratio for Kerr-AdS black holes with an arbitrary electric charge in four dimensions and show that it corresponds to g=2 irrespective of the AdS nature of the spacetime. We also compute the gyromagnetic ratio for Kerr-AdS black holes with a single angular momentum and with a test electric charge in all higher dimensions. The gyromagnetic ratio crucially depends on the dimensionless ratio of the rotation parameter to the curvature radius of the AdS background. At the critical limit, when the boundary Einstein universe is rotating at the speed of light, it exhibits a striking feature leading to g=2 regardless of the spacetime dimension. Next, we extend our consideration to include the exact metric for five-dimensional rotating charged black holes in minimal gauged supergravity. We show that the value of the gyromagnetic ratio found in the ""test-charge"" approach remains unchanged for these black holes."
"It is known that the combinatorial classes in the cohomology of the mapping class group of punctures surfaces defined by Witten and Kontsevich are polynomials in the adjusted Miller-Morita-Mumford classes. The leading coefficient was computed in [Kiyoshi Igusa: Algebr. Geom. Topol. 4 (2004) 473-520]. The next coefficient was computed in [Kiyoshi Igusa: math.AT/0303157, to appear in Topology]. The present paper gives a recursive formula for all of the coefficients. The main combinatorial tool is a generating function for a new statistic on the set of increasing trees on 2n+1 vertices. As we already explained in the last paper cited this verifies all of the formulas conjectured by Arbarello and Cornalba [J. Alg. Geom. 5 (1996) 705--749]. Mondello [math.AT/0303207, to appear in IMRN] has obtained similar results using different methods."
"We study multicellular tumor spheroids by introducing a new three-dimensional agent-based Voronoi/Delaunay hybrid model. In this model, the cell shape varies from spherical in thin solution to convex polyhedral in dense tissues. The next neighbors of the cells are provided by a weighted Delaunay triangulation with in average linear computational complexity. The cellular interactions include direct elastic forces and cell-cell as well as cell-matrix adhesion. The spatiotemporal distribution of two nutrients -- oxygen and glucose -- is described by reaction-diffusion equations. Viable cells consume the nutrients, which are converted into biomass by increasing the cell size during G_1 phase.   We test hypotheses on the functional dependence of the uptake rates and use the computer simulation to find suitable mechanisms for induction of necrosis. This is done by comparing the outcome with experimental growth curves, where the best fit leads to an unexpected ratio of oxygen and glucose uptake rates. The model relies on physical quantities and can easily be generalized towards tissues involving different cell types. In addition, it provides many features that can be directly compared with the experiment."
"It is reasonable to expect the theory of quantum codes to be simplified in the case of codes of minimum distance 2; thus, it makes sense to examine such codes in the hopes that techniques that prove effective there will generalize. With this in mind, we present a number of results on codes of minimum distance 2. We first compute the linear programming bound on the dimension of such a code, then show that this bound can only be attained when the code either is of even length, or is of length 3 or 5. We next consider questions of uniqueness, showing that the optimal code of length 2 or 4 is unique (implying that the well-known one-qubit-in-five single-error correcting code is unique), and presenting nonadditive optimal codes of all greater even lengths. Finally, we compute the full automorphism group of the more important distance 2 codes, allowing us to determine the full automorphism group of any GF(4)-linear code."
"A class of discrete event synthesis problems can be reduced to solving language equations f . X &sube; S, where F is the fixed component and S the specification. Sequential synthesis deals with FSMs when the automata for F and S are prefix closed, and are naturally represented by multi-level networks with latches. For this special case, we present an efficient computation, using partitioned representations, of the most general prefix-closed solution of the above class of language equations. The transition and the output relations of the FSMs for F and S in their partitioned form are represented by the sets of output and next state functions of the corresponding networks. Experimentally, we show that using partitioned representations is much faster than using monolithic representations, as well as applicable to larger problem instances."
"I review recent progress in perturbative QCD on two fronts: extending next-to-next-to-leading order QCD corrections to a broader range of collider processes, and applying twistor-space methods (and related spinoffs) to computations of multi-parton scattering amplitudes."
"As technology scales down, the static power is expected to become a significant fraction of the total power. The exponential dependence of static power with the operating temperature makes the thermal profile estimation of high-performance ICs a key issue to compute the total power dissipated in next-generations. In this paper we present accurate and compact analytical models to estimate the static power dissipation and the temperature of operation of CMOS gates. The models are the fundamentals of a performance estimation tool in which numerical procedures are avoided for any computation to set a faster estimation and optimization. The models developed are compared to measurements and SPICE simulations for a 0.12mm technology showing excellent results."
"XrML is becoming a popular language in industry for writing software licenses. The semantics for XrML is implicitly given by an algorithm that determines if a permission follows from a set of licenses. We focus on a fragment of the language and use it to highlight some problematic aspects of the algorithm. We then correct the problems, introduce formal semantics, and show that our semantics captures the (corrected) algorithm. Next, we consider the complexity of determining if a permission is implied by a set of XrML licenses. We prove that the general problem is undecidable, but it is polynomial-time computable for an expressive fragment of the language. We extend XrML to capture a wider range of licenses by adding negation to the language. Finally, we discuss the key differences between XrML and MPEG-21, an international standard based on XrML."
"The MWA is a next-generation radio interferometer under construction in remote Western Australia. The data rate from the correlator makes storing the raw data infeasible, so the data must be processed in real-time. The processing task is of order ~10 TFLOPS. The remote location of the MWA limits the power that can be allocated to computing. We describe the design and implementation of elements of the MWA real-time data processing system which leverage the computing abilities of modern graphics processing units (GPUs). The matrix algebra and texture mapping capabilities of GPUs are well suited to the majority of tasks involved in real-time calibration and imaging. Considerable performance advantages over a conventional CPU-based reference implementation are obtained."
"Link state routing protocols such as OSPF or IS-IS currently use only best paths to forward IP packets throughout a domain. The optimality of sub-paths ensures consistency of hop by hop forwarding although paths, calculated using Dijkstra algorithm, are recursively composed. According to the link metric, the diversity of existing paths can be underestimated using only best paths. Hence, it reduces potential benefits of multipath applications such as load balancing and fast rerouting. In this paper, we propose a low time complexity multipath computation algorithm able to calculate at least two paths with a different first hop between all pairs of nodes in the network if such next hops exist. Using real and generated topologies, we evaluate and compare the complexity of our proposition with several techniques. Simulation results suggest that the path diversity achieved with our proposition is approximatively the same that the one obtained using consecutive Dijsktra computations, but with a lower time complexity."
"We present trichotomy results characterizing the complexity of reasoning with disjunctive logic programs. To this end, we introduce a certain definition schema for classes of programs based on a set of allowed arities of rules. We show that each such class of programs has a finite representation, and for each of the classes definable in the schema we characterize the complexity of the existence of an answer set problem. Next, we derive similar characterizations of the complexity of skeptical and credulous reasoning with disjunctive logic programs. Such results are of potential interest. On the one hand, they reveal some reasons responsible for the hardness of computing answer sets. On the other hand, they identify classes of problem instances, for which the problem is ""easy"" (in P) or ""easier than in general"" (in NP). We obtain similar results for the complexity of reasoning with disjunctive programs under the supported-model semantics. To appear in Theory and Practice of Logic Programming (TPLP)"
"This paper describes the rationale, curriculum and subject matter of a new MSc module being taught on an MSc Finance and Information Management course at the University of Wales Institute in Cardiff. Academic research on spreadsheet risks now has some penetration in academic literature and there is a growing body of knowledge on the subjects of spreadsheet error, human factors, spreadsheet engineering, ""best practice"", spreadsheet risk management and various techniques used to mitigate spreadsheet errors. This new MSc module in End User Computing Risk Management is an attempt to pull all of this research and practitioner experience together to arm the next generation of finance spreadsheet champions with the relevant knowledge, techniques and critical perspective on an emerging discipline."
"Considerable research has taken place in recent times in the area of parameterization of software defined radio (SDR) architecture. Parameterization decreases the size of the software to be downloaded and also limits the hardware reconfiguration time. The present paper is based on the design and development of a programmable baseband modulator that perform the QPSK modulation schemes and as well as its other three commonly used variants to satisfy the requirement of several established 2G and 3G wireless communication standards. The proposed design has been shown to be capable of operating at a maximum data rate of 77 Mbps on Xilinx Virtex 2-Pro University field programmable gate array (FPGA) board. The pulse shaping root raised cosine (RRC) filter has been implemented using distributed arithmetic (DA) technique in the present work in order to reduce the computational complexity, and to achieve appropriate power reduction and enhanced throughput. The designed multiplier-less programmable 32-tap FIR-based RRC filter has been found to withstand a peak inter-symbol interference (ISI) distortion of -41 dBs"
he correct estimation of thermal properties of ultra-scaled CMOS and thermoelectric semiconductor devices demands for accurate phonon modeling in such structures. This work provides a detailed description of the modified valence force field (MVFF) method to obtain the phonon dispersion in zinc-blende semiconductors. The model is extended from bulk to nanowires after incorporating proper boundary conditions. The computational demands by the phonon calculation increase rapidly as the wire cross-section size increases. It is shown that the nanowire phonon spectrum differ considerably from the bulk dispersions. This manifests itself in the form of different physical and thermal properties in these wires. We believe that this model and approach will prove beneficial in the understanding of the lattice dynamics in the next generation ultra-scaled semiconductor devices.
"Probabilistic B\""uchi Automata (PBA) are randomized, finite state automata that process input strings of infinite length. Based on the threshold chosen for the acceptance probability, different classes of languages can be defined. In this paper, we present a number of results that clarify the power of such machines and properties of the languages they define. The broad themes we focus on are as follows. We present results on the decidability and precise complexity of the emptiness, universality and language containment problems for such machines, thus answering questions central to the use of these models in formal verification. Next, we characterize the languages recognized by PBAs topologically, demonstrating that though general PBAs can recognize languages that are not regular, topologically the languages are as simple as \omega-regular languages. Finally, we introduce Hierarchical PBAs, which are syntactically restricted forms of PBAs that are tractable and capture exactly the class of \omega-regular languages."
"In this work, we suggest that hard-pion chiral perturbation theory may be applicable to the real parts of nonleptonic B^{+} to D^{0}P^{+} and B^{+} to Dbar^{0}P^{+} (P=K,pi) decay amplitudes. These amplitudes play an important role in the extraction of the angle gamma in the b-d unitarity triangle of the CKM matrix, and their real parts can be computed using lattice QCD. We construct the leading-order operator in the chiral expansion for these nonleptonic decays, and discuss the generic features of the next-to-leading-order terms."
"Approximate computing is a computation domain which can be used to trade time and energy with quality and therefore is useful in embedded systems. Energy is the prime resource in battery-driven embedded systems, like robots. Approximate computing can be used as a technique to generate approximate version of the control functionalities of a robot, enabling it to ration energy for computation at the cost of degraded quality. Usually, the programmer of the function specifies the extent of degradation that is safe for the overall safety of the system. However, in a collaborative environment, where several sub-systems co-exist and some of the functionality of each of them have been approximated, the safety of the overall system may be compromised. In this paper, we consider multiple identical robots operate in a warehouse, and the path planning function of the robot is approximated. Although the planned paths are safe for individual robots (i.e. they do not collide with the racks), we show that this leads to a collision among the robots. So, a controlled approximation needs to be carried out in such situations to harness the full power of this new paradigm if it needs to be a mainstream paradigm in future."
"We present results from the first fully differential next-to-next-to leading order NNLO QCD computation of ZZ production, including off-shell effects, the leptonic decay and non-resonant contributions. We compare theoretical predictions to fiducial cross sections and distributions measured by ATLAS and CMS at 8 TeV and also perform a first comparison with early ATLAS measurements at 13 TeV."
"In this article, we first briefly introduce the history of the Weil-\'etale cohomology theory of arithmetic schemes and review some important results established by Lichtenbaum, Flach and Morin. Next we generalize the Weil-etale cohomology to $S$-integers and compute the cohomology for constant sheaves $\mathbb{Z}$ or $\mathbb{R}$. We also define a Weil-\'etale cohomology with compact support $H_c(Y_W, -)$ for $Y=Spec \mathcal{O}_{F,S}$ where $F$ is a number field, and computed them. We verify that these cohomology groups satisfy the axioms state by Lichtenbaum. As an application, we derive a canonical representation of Tate sequence from $RGamma_c(Y_W,\mathbb{Z})$. Motivated by this result, in the final part, we define an \'etale complex $RGm$, such that the complexes $\mathbb{Z}$-dual of the complex $\RG(U_{et},R\Gm),\,\mathbb{Z})[2]$ is canonically quasi-isomorphic to $\tau^{\leq 3}\RG_c(U_W,\mathbb{Z})$ for arbitrary \'etale $U$ over $Spec \mathcal{O}_{F}$. This quasi-isomorphism provides a possible approach to define the Weil-etale cohomology for higher dimensional arithmetic schemes, as the Weil groups are not involved in the definition of $R\Gm$."
"We use a de Sitter breaking graviton propagator to compute the tree order correlator between noncoincident Weyl tensors on a locally de Sitter background. An explicit, and very simple result is obtained, for any spacetime dimension D, in terms of a de Sitter invariant length function and the tensor basis constructed from the metric and derivatives of this length function. Our answer does not agree with the one derived previously by Kouris, but that result must be incorrect because it not transverse and lacks some of the algebraic symmetries of the Weyl tensor. Taking the coincidence limit of our result (with dimensional regularization) and contracting the indices gives the expectation value of the square of the Weyl tensor at lowest order. We propose the next order computation of this as a true test of de Sitter invariance in quantum gravity."
"EKS is a numerical program that predicts differential cross sections for production of single-inclusive hadronic jets and jet pairs at next-to-leading order (NLO) accuracy in a perturbative QCD calculation. We describe MEKS 1.0, an upgraded EKS program with increased numerical precision, suitable for comparisons to the latest experimental data from the Large Hadron Collider and Tevatron. The program integrates the regularized patron-level matrix elements over the kinematical phase space for production of two and three partons using the VEGAS algorithm. It stores the generated weighted events in finely binned two-dimensional histograms for fast offline analysis. A user interface allows one to customize computation of inclusive jet observables. Results of a benchmark comparison of the MEKS program and the commonly used FastNLO program are also documented."
"The mass distribution of jets produced in hard processes at the LHC plays an important role in several jet substructure related studies involving both Standard Model and BSM physics, especially in the context of boosted heavy particle searches. We compute analytically the jet-mass distribution for both Z+jet and dijet processes, for QCD jets defined in the anti-k_t algorithm with an arbitrary radius R, to next-to-leading logarithmic accuracy and match our resummed calculation to full leading-order results. We note the important role played by initial state radiation (ISR) and non-global logarithms explicitly computed here for the first time for hadron collider observables, as well as the jet radius dependence of these effects. We also compare our results to standard Monte Carlo event generators and discuss directions for further studies and phenomenology."
"The high-mass end of the halo mass function is a sensitive probe of primordial non-Gaussianity (NG). In a recent study [9] we have computed the NG halo mass function in the context of the Excursion Set theory and shown that the primordial NG imprint is coupled to that induced by the non-linear collapse of dark matter halos. We also found an excellent agreement with N-body simulation results. Here, we perform a more accurate computation which accounts for the interval validity of the bispectrum expansion to next-to-leading order and extend the calculation to the case of a non-vanishing primordial trispectrum."
"Finite difference discretization schemes preserving a subgroup of the maximal Lie invariance group of the one-dimensional linear heat equation are determined. These invariant schemes are constructed using the invariantization procedure for non-invariant schemes of the heat equation in computational coordinates. We propose a new methodology for handling moving discretization grids which are generally indispensable for invariant numerical schemes. The idea is to use the invariant grid equation, which determines the locations of the grid point at the next time level only for a single integration step and then to project the obtained solution to the regular grid using invariant interpolation schemes. This guarantees that the scheme is invariant and allows one to work on the simpler stationary grids. The discretization errors of the invariant schemes are established and their convergence rates are estimated. Numerical tests are carried out to shed some light on the numerical properties of invariant discretization schemes using the proposed evolution-projection strategy."
"In one of the most important methods in Density Functional Theory - the Full-Potential Linearized Augmented Plane Wave (FLAPW) method - dense generalized eigenproblems are organized in long sequences. Moreover each eigenproblem is strongly correlated to the next one in the sequence. We propose a novel approach which exploits such correlation through the use of an eigensolver based on subspace iteration and accelerated with Chebyshev polynomials. The resulting solver, parallelized using the Elemental library framework, achieves excellent scalability and is competitive with current dense parallel eigensolvers."
"In last decades, it has been realized that the next-to-leading order corrections may become very important, and sometimes requisite, for some processes involving quarkoinum production or decay, e.g., $e^+e^- \to J/\psi + \eta_c$ and $J/\psi \to 3\gamma$. In this article, we review some basic steps to perform automated one-loop computations in quarkonium process within the Non-relativistic Quantum Chromodynamics (NRQCD) factorization framework, and we give an introduction to some related public tools or packages and their usages in each step. We start from generating Feynman diagrams and amplitudes with \textsc{FeynArts} for the quarkonium process, performing Dirac- and Color- algebras simplifications using \textsc{FeynCalc} and \textsc{FeynCalcFormLink}, and then to doing partial fractions on the linear-dependent propagators by \textsc{APart}, and finally to reducing the Tensor Integrals (TI) into Scalar Integrals (SI) or Master Integrals (MI) using Integration-By-Parts (IBP) method with the help of \textsc{Fire}. We will use a simple concrete example to demonstrate the basic usages of the corresponding packages or tools in each step."
"Wireless mesh networks have seen a real progress due of their implementation at a low cost. They present one of Next Generation Networks technologies and can serve as home, companies and universities networks. In this paper, we propose and discuss a new multi-objective model for nodes deployment optimization in Multi-Radio Multi-Channel Wireless Mesh Networks. We exploit the trade-off between network cost and the overall network performance. This optimization problem is solved simultaneously by using a meta-heuristic method that returns a non-dominated set of near optimal solutions. A comparative study was driven to evaluate the efficiency of the proposed model."
"Characterizing noisy or ancient documents is a challenging problem up to now. Many techniques have been done in order to effectuate feature extraction and image indexation for such documents. Global approaches are in general less robust and exact than local approaches. That's why, we propose in this paper, a hybrid system based on global approach(fractal dimension), and a local one based on SIFT descriptor. The Scale Invariant Feature Transform seems to do well with our application since it's rotation invariant and relatively robust to changing illumination.In the first step the calculation of fractal dimension is applied to images in order to eliminate images which have distant features than image request characteristics. Next, the SIFT is applied to show which images match well the request. However the average matching time using the hybrid approach is better than ""fractal dimension"" and ""SIFT descriptor"" if they are used alone."
We consider the inclusive production of vector-boson pairs in hadron collisions. We review the theoretical progress in the computation of radiative corrections to this process up to next-to-next-to-leading order in QCD perturbation theory.
"Implementing a scalable quantum information processor using polar molecules in optical lattices requires precise control over the long-range dipole-dipole interaction between molecules in selected lattice sites. We present here a scheme using trapped open-shell $^2\Sigma$ polar molecules that allows dipolar exchange processes between nearest and next-nearest neighbors to be controlled to construct a generalized transverse Ising spin Hamiltonian with tunable $XX$, $YY$ and $XY$ couplings in the rotating frame of the driving lasers. The scheme requires a moderately strong bias magnetic field with near-infrared light to provide local tuning of the qubit energy gap, and mid-infrared pulses to perform rotational state transfer via stimulated Raman adiabatic passage. No interaction between qubits is present in the absence of the infrared driving. We analyze the fidelity of the resulting two-qubit matchgate, and demonstrate its robustness as a function of the driving parameters. We discuss a realistic application of the system for universal matchgate quantum computing in optical lattices."
"Stable Logic Programming (SLP) is an emergent, alternative style of logic programming: each solution to a problem is represented by a stable model of a deductive database/function-free logic program encoding the problem itself. Several implementations now exist for stable logic programming, and their performance is rapidly improving. To make SLP generally applicable, it should be possible to check for consistency (i.e., existence of stable models) of the input program before attempting to answer queries. In the literature, only rather strong sufficient conditions have been proposed for consistency, e.g., stratification. This paper extends these results in several directions. First, the syntactic features of programs, viz. cyclic negative dependencies, affecting the existence of stable models are characterized, and their relevance is discussed. Next, a new graph representation of logic programs, the Extended Dependency Graph (EDG), is introduced, which conveys enough information for reasoning about stable models (while the traditional Dependency Graph does not). Finally, we show that the problem of the existence of stable models can be reformulated in terms of coloring of the EDG."
"Based on the fundamental constraints of natural way of interacting such as speech, touch, contextual and environmental awareness,immersive 3D experiences-all with a goal of a computer that can see listen, learn talk and act. We drive a set of trends prevailing for the next generation of user interface: Natural User Interface (NUI).New technologies are pushing the boundaries of what is possible without touching or clicking an interface- paving the way of interaction to information visualization and opportunities in human towards more natural interaction than ever before. In this paper we consider the trends in computer interaction through that must be taken into consideration to come up-in the near future with a well-designed-NUI."
"C++ 98/03 already has a reputation for overwhelming complexity compared to other programming languages. The raft of new features in C++ 11/14 suggests that the complexity in the next generation of C++ code bases will overwhelm still further. The planned C++ 17 will probably worsen matters in ways difficult to presently imagine.   Countervailing against this rise in software complexity is the hard de-exponentialisation of computer hardware capacity growth expected no later than 2020, and which will have even harder to imagine consequences on all computer software. WG21 C++ 17 study groups SG2 (Modules), SG7 (Reflection), SG8 (Concepts), and to a lesser extent SG10 (Feature Test) and SG12 (Undefined Behaviour), are all fundamentally about significantly improving complexity management in C++ 17, yet WG21's significant work on improving C++ complexity management is rarely mentioned explicitly.   This presentation pitches a novel implementation solution for some of these complexity scaling problems, tying together SG2 and SG7 with parts of SG3 (Filesystem): a standardised but very lightweight transactional graph database based on Boost.ASIO, Boost.AFIO and Boost.Graph at the very core of the C++ runtime, making future C++ codebases considerably more tractable and affordable to all users of C++."
"We present a new scalable, lightweight algorithm to incrementally construct the BWT and FM-index of large string sets such as those produced by Next Generation Sequencing. The algorithm is designed for massive parallelism and can effectively exploit the combination of low capacity high bandwidth memory and slower external system memory typical of GPU accelerated systems. Particularly, for a string set of n characters from an alphabet with \sigma symbols, it uses a constant amount of high-bandwidth memory and at most 3n log(\sigma) bits of system memory. Given that deep memory hierarchies are becoming a pervasive trait of high performance computing architectures, we believe this to be a relevant feature. The implementation can handle reads of arbitrary length and is up to 2 and respectively 6.5 times faster than state-of-the-art for short and long genomic reads"
"Domain ontologies are important information sources for knowledge-based systems. Yet, building domain ontologies from scratch is known to be a very labor-intensive process. In this study, we present our semi-automatic approach to building an ontology for the domain of wind energy which is an important type of renewable energy with a growing share in electricity generation all over the world. Related Wikipedia articles are first processed in an automated manner to determine the basic concepts of the domain together with their properties and next the concepts, properties, and relationships are organized to arrive at the ultimate ontology. We also provide pointers to other engineering ontologies which could be utilized together with the proposed wind energy ontology in addition to its prospective application areas. The current study is significant as, to the best of our knowledge, it proposes the first considerably wide-coverage ontology for the wind energy domain and the ontology is built through a semi-automatic process which makes use of the related Web resources, thereby reducing the overall cost of the ontology building process."
"The continuous increase of data generated provides enormous possibilities of both public and private companies. The management of this mass of data or big data will play a crucial role in the society of the future, as it finds applications in different fields. There are so much potential and extremely useful insights hidden in the huge volume of data. The advanced analysis techniques available including predictive analytics, text mining, semantic analysis are needed to enable organizations to create a competitive advantage through data analyzed with different levels of sophistication, speed and accuracy previously unavailable. Therefore, is it still possible to have that level of sophistication with the ubiquitous numeric ocean that accompanies use every day via connected devices that invade our lives? However, development of big data requires a good understanding of the issues associated with it. And this is the purpose of this paper, which focuses on giving a close-up view of big data analysis, opportunities and challenges."
"In this paper, we review the parallel and distributed optimization algorithms based on alternating direction method of multipliers (ADMM) for solving ""big data"" optimization problem in smart grid communication networks. We first introduce the canonical formulation of the large-scale optimization problem. Next, we describe the general form of ADMM and then focus on several direct extensions and sophisticated modifications of ADMM from $2$-block to $N$-block settings to deal with the optimization problem. The iterative schemes and convergence properties of each extension/modification are given, and the implementation on large-scale computing facilities is also illustrated. Finally, we numerate several applications in power system for distributed robust state estimation, network energy management and security constrained optimal power flow problem."
"Saliency modeling has been an active research area in computer vision for about two decades. Existing state of the art models perform very well in predicting where people look in natural scenes. There is, however, the risk that these models may have been overfitting themselves to available small scale biased datasets, thus trapping the progress in a local minimum. To gain a deeper insight regarding current issues in saliency modeling and to better gauge progress, we recorded eye movements of 120 observers while they freely viewed a large number of naturalistic and artificial images. Our stimuli includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings. We analyze some basic properties of this dataset and compare some successful models. We believe that our dataset opens new challenges for the next generation of saliency models and helps conduct behavioral studies on bottom-up visual attention."
"Running from October 2011 to June 2015, the aim of the European project Mont-Blanc has been to develop an approach to Exascale computing based on embedded power-efficient technology. The main goals of the project were to i) build an HPC prototype using currently available energy-efficient embedded technology, ii) design a Next Generation system to overcome the limitations of the built prototype and iii) port a set of representative Exascale applications to the system. This article summarises the contributions from the Leibniz Supercomputing Centre (LRZ) and the Juelich Supercomputing Centre (JSC), Germany, to the Mont-Blanc project."
"In this talk, we discuss recent developments in combining parton showers and fixed-order calculations. We focus on the UNNLOPS method for matching next-to-next-to-leading order computations to the parton shower, and we present results from Sherpa for Drell-Yan lepton-pair and Higgs-boson production at the LHC."
"Authentication of individuals via palmprint based biometric system is becoming very popular due to its reliability as it contains unique and stable features. In this paper, we present a novel approach for palmprint recognition and its representation. To extract the palm lines, local thresholding technique Niblack binarization algorithm is adopted. The endpoints of these lines are determined and a connection is created among them using the Delaunay triangulation thereby generating a distinct topological structure of each palmprint. Next, we extract different geometric as well as quantitative features from the triangles of the Delaunay triangulation that assist in identifying different individuals. To ensure that the proposed approach is invariant to rotation and scaling, features were made relative to topological and geometrical structure of the palmprint. The similarity of the two palmprints is computed using the weighted sum approach and compared with the k-nearest neighbor. The experimental results obtained reflect the effectiveness of the proposed approach to discriminate between different palmprint images and thus achieved a recognition rate of 90% over large databases."
"Magnetic skyrmions are promising candidates for next-generation information carriers, owing to their small size, topological stability, and ultralow depinning current density. A wide variety of skyrmionic device concepts and prototypes have been proposed, highlighting their potential applications. Here, we report on a bioinspired skyrmionic device with synaptic plasticity. The synaptic weight of the proposed device can be strengthened/weakened by positive/negative stimuli, mimicking the potentiation/depression process of a biological synapse. Both short-term plasticity(STP) and long-term potentiation(LTP) functionalities have been demonstrated for a spiking time-dependent plasticity(STDP) scheme. This proposal suggests new possibilities for synaptic devices for use in spiking neuromorphic computing applications."
"We present a theory for slicing probabilistic imperative programs -- containing random assignments, and ``observe'' statements (for conditioning) -- represented as probabilistic control-flow graphs (pCFGs) whose nodes modify probability distributions.   We show that such a representation allows direct adaptation of standard machinery such as data and control dependence, postdominators, relevant variables, etc to the probabilistic setting. We separate the specification of slicing from its implementation: first we develop syntactic conditions that a slice must satisfy; next we prove that any such slice is semantically correct; finally we give an algorithm to compute the least slice. To generate smaller slices, we may in addition take advantage of knowledge that certain loops will terminate (almost) always.   A key feature of our syntactic conditions is that they involve two disjoint slices such that the variables of one slice are probabilistically independent of the variables of the other. This leads directly to a proof of correctness of probabilistic slicing. In a companion article we show adequacy of the semantics of pCFGs with respect to the standard semantics of structured probabilistic programs."
"Predicting unseen weather phenomena is an important issue for disaster management. In this paper, we suggest a model for a convolutional sequence-to-sequence autoencoder for predicting undiscovered weather situations from previous satellite images. We also propose a symmetric skip connection between encoder and decoder modules to produce more comprehensive image predictions. To examine our model performance, we conducted experiments for each suggested model to predict future satellite images from historical satellite images. A specific combination of skip connection and sequence-to-sequence autoencoder was able to generate closest prediction from the ground truth image."
"In this paper, considering the probe limit, we analytically study the onset of holographic s-wave condensate in the planar Schwarzschild-AdS background. Inspired by various low energy features of string theory, in the present work we replace the conventional Maxwell action by a (non-linear) Born-Infeld (BI) action which essentially corresponds to the higher derivative corrections of the gauge fields. Based on a variational method, which is commonly known as the Sturm-Liouville (SL) eigenvalue problem and considering a non-trivial asymptotic solution for the scalar field, we compute the critical temperature for the s-wave condensation. The results thus obtained analytically agree well with the numerical findings\cite{hs19}. As a next step, we extend our perturbative technique to compute the order parameter for the condensation. Interestingly our analytic results are found to be of the same order as the numerical values obtained earlier."
"The Dynamic Scalability of resources, a problem in Infrastructure as a Service (IaaS) has been the hotspot for research and industry communities. The heterogeneous and dynamic nature of the Cloud workloads depends on the Quality of Service (QoS) allocation of appropriate workloads to appropriate resources. A workload is an abstraction of work that instance or set of instances that are going to perform. Running a web service or being a Hadoop data node is valid workloads. The efficient management of dynamic nature resources can be done with the help of workloads. Until workload is considered a fundamental capability, the Cloud resources cannot be utilized in an efficient manner. In this paper, different workloads have been identified and categorized along with their characteristics and constraints. The metrics based on Quality of Service (QoS) requirements have been identified for each workload and have been analyzed for creating better application design."
"We present a new framework for computing resummed and matched distributions in processes with many hard QCD jets. The intricate color structure of soft gluon emission at large angles renders resummed calculations highly non-trivial in this case. We automate all ingredients necessary for the color evolution of the soft function at next-to-leading-logarithmic accuracy, namely the selection of the color bases and the projections of color operators and Born amplitudes onto those bases. Explicit results for all QCD processes with up to $2\to 5$ partons are given. We also devise a new tree-level matching scheme for resummed calculations which exploits a quasi-local subtraction based on the Catani-Seymour dipole formalism. We implement both resummation and matching in the Sherpa event generator. As a proof of concept, we compute the resummed and matched transverse-thrust distribution for hadronic collisions."
"In the last decade a number of algorithms and associated software have been developed to align next generation sequencing (NGS) reads with relevant reference genomes. The accuracy of these programs may vary significantly, especially when the NGS reads are quite different from the available reference genome. We propose a benchmark to assess accuracy of short reads mapping based on the pre-computed global alignment of related genome sequences.   In this paper we propose a benchmark to assess accuracy of the short reads mapping based on the pre-computed global alignment of closely related genome sequences. We outline the method and also present a short report of an experiment performed on five popular alignment tools based on the pairwise alignments of Escherichia coli O6 CFT073 genome with genomes of seven other bacteria."
"We consider Langevin dynamics associated with a modified kinetic energy vanishing for small momenta. This allows us to freeze slow particles, and hence avoid the re-computation of inter-particle forces, which leads to computational gains. On the other hand, the statistical error may increase since there are a priori more correlations in time. The aim of this work is first to prove the ergodicity of the modified Langevin dynamics (which fails to be hypoelliptic), and next to analyze how the asymptotic variance on ergodic averages depends on the parameters of the modified kinetic energy. Numerical results illustrate the approach, both for low-dimensional systems where we resort to a Galerkin approximation of the generator, and for more realistic systems using Monte Carlo simulations."
"In this paper, we investigate stability of a class of analytic neural networks with the synaptic feedback via event-triggered rules. This model is general and include Hopfield neural network as a special case. These event-trigger rules can efficiently reduces loads of computation and information transmission at synapses of the neurons. The synaptic feedback of each neuron keeps a constant value based on the outputs of the other neurons at its latest triggering time but changes at its next triggering time, which is determined by certain criterion. It is proved that every trajectory of the analytic neural network converges to certain equilibrium under this event-triggered rule for all initial values except a set of zero measure. The main technique of the proof is the Lojasiewicz inequality to prove the finiteness of trajectory length. The realization of this event-triggered rule is verified by the exclusion of Zeno behaviors. Numerical examples are provided to illustrate the efficiency of the theoretical results."
"The post-Newtonian approximation is still the most widely used approach to obtaining explicit solutions in general relativity, especially for the relativistic two-body problem with arbitrary mass ratio. Within many of its applications, it is often required to use a regularization procedure. Though frequently misunderstood, the regularization is essential for waveform generation without reference to the internal structure of orbiting bodies. In recent years, direct comparison with the self-force approach, constructed specifically for highly relativistic particles in the extreme mass ratio limit, has enabled preliminary confirmation of the foundations of both computational methods, including their very independent regularization procedures, with high numerical precision. In this paper, we build upon earlier work to carry this comparison still further, by examining next-to-next-to-leading order contributions beyond the half integral 5.5PN conservative effect, which arise from terms to cubic and higher orders in the metric and its multipole moments, thus extending scrutiny of the post-Newtonian methods to one of the highest orders yet achieved. We do this by explicitly constructing tail-of-tail terms at 6.5PN and 7.5PN order, computing the redshift factor for compact binaries in the small mass ratio limit, and comparing directly with numerically and analytically computed terms in the self-force approach, obtained using solutions for metric perturbations in the Schwarzschild space-time, and a combination of exact series representations possibly with more typical PN expansions. While self-force results may be relativistic but with restricted mass ratio, our methods, valid primarily in the weak-field slowly-moving regime, are nevertheless in principle applicable for arbitrary mass ratios."
"The string graph for a collection of next-generation reads is a lossless data representation that is fundamental for de novo assemblers based on the overlap-layout-consensus paradigm. In this paper, we explore a novel approach to compute the string graph, based on the FM-index and Burrows-Wheeler Transform. We describe a simple algorithm that uses only the FM-index representation of the collection of reads to construct the string graph, without accessing the input reads. Our algorithm has been integrated into the SGA assembler as a standalone module to construct the string graph.   The new integrated assembler has been assessed on a standard benchmark, showing that FSG is significantly faster than SGA while maintaining a moderate use of main memory, and showing practical advantages in running FSG on multiple threads."
"Rummikub is a tile-based game in which each player starts with a hand of $14$ tiles. A tile has a value and a suit. The players form sets consisting of tiles with the same suit and consecutive values (runs) or tiles with the same value and different suits (groups). The corresponding optimization problem is, given a hand of tiles, to form valid sets such that the score (sum of tile values) is maximized. We first present an algorithm that solves this problem in polynomial time. Next, we analyze the impact on the computational complexity when we generalize over various input parameters. Finally, we attempt to better understand some aspects involved in human play by means of an experiment that considers counting problems related to the number of possible immediately winning hands."
"The supersymmetric completion of higher-derivative operators often requires introducing corrections to the scalar potential. In this paper we study these corrections systematically in the context of theories with $\mathcal{N}=1$ global and local supersymmetry in $D=4$ focusing on ungauged chiral multiplets. In globally supersymmetric theories the most general off-shell effective scalar potential can be captured by a dependence of the K\""{a}hler potential on additional chiral superfields. For supergravity we find a much richer structure of possible corrections. In this context we classify the leading order and next-to-leading order superspace derivative operators and determine the component forms of a subclass thereof. Moreover, we present an algorithm that simplifies the computation of the respective on-shell action. As particular applications we study the structure of the supersymmetric vacua for these theories and comment on the form of the corrections to shift-symmetric no-scale models. These results are relevant for the computation of effective actions for string compactifications and, in turn, for moduli stabilization and string inflation."
"Boundary-engineering in nanostructures has the potential to dramatically impact the development of materials for high-efficiency conversion of thermal energy directly into electricity. In particular, nanostructuring of semiconductors can lead to strong suppression of heat transport with little degradation of electrical conductivity. Although this combination of material properties is promising for thermoelectric materials, it remains largely unexplored. In this work, we introduce a novel concept, the directional phonon suppression function, to unravel boundary-dominated heat transport in unprecedented detail. Using a combination of density functional theory and the Boltzmann transport equation, we compute this quantity for nanoporous silicon materials. We first compute the thermal conductivity for the case with aligned circular pores, confirming a significant thermal transport degradation with respect to the bulk. Then, by analyzing the information on the directionality of phonon suppression in this system, we identify a new structure of rectangular pores with the same porosity that enables a four-fold decrease in thermal transport with respect to the circular pores. Our results illustrate the utility of the directional phonon suppression function, enabling new avenues for systematic thermal conductivity minimization and potentially accelerating the engineering of next-generation thermoelectric devices."
"Fifth generation (5G) dense small cell networks (SCNs) are expected to meet the thousand-fold mobile traffic challenge within the next few years. When developing solution schemes for resource allocation problems in such networks, conventional centralized control is no longer viable due to excessive computational complexity and large signaling overhead caused by the large number of users and network nodes in such a network. Instead, distributed resource allocation (or decision making) methods with low complexity would be desirable to make the network self-organizing and autonomous. Minority game (MG) has recently gained attention of the research community as a tool to model and solve distributed resource allocation problems. The main objective of this article is to study the applicability of the MG to solve the distributed decision making problems in future wireless networks. We present the fundamental theoretical aspects of basic MG, some variants of MG, and the notion of equilibrium. We also study the current state-of-the-art on the applications of MGs in communication networks. Furthermore, we describe an example application of MG to SCNs, where the problem of computation offloading by users in an SCN is modeled and analyzed using MG."
"Next generation radio telescopes, like the Square Kilometre Array, will acquire an unprecedented amount of data for radio astronomy. The development of fast, parallelisable or distributed algorithms for handling such large-scale data sets is of prime importance. Motivated by this, we investigate herein a convex optimisation algorithmic structure, based on primal-dual forward-backward iterations, for solving the radio interferometric imaging problem. It can encompass any convex prior of interest. It allows for the distributed processing of the measured data and introduces further flexibility by employing a probabilistic approach for the selection of the data blocks used at a given iteration. We study the reconstruction performance with respect to the data distribution and we propose the use of nonuniform probabilities for the randomised updates. Our simulations show the feasibility of the randomisation given a limited computing infrastructure as well as important computational advantages when compared to state-of-the-art algorithmic structures."
"We devise an algorithm using a Bayesian optimization framework in conjunction with contextual visual data for the efficient localization of objects in still images. Recent research has demonstrated substantial progress in object localization and related tasks for computer vision. However, many current state-of-the-art object localization procedures still suffer from inaccuracy and inefficiency, in addition to failing to provide a principled and interpretable system amenable to high-level vision tasks. We address these issues with the current research.   Our method encompasses an active search procedure that uses contextual data to generate initial bounding-box proposals for a target object. We train a convolutional neural network to approximate an offset distance from the target object. Next, we use a Gaussian Process to model this offset response signal over the search space of the target. We then employ a Bayesian active search for accurate localization of the target.   In experiments, we compare our approach to a state-of-theart bounding-box regression method for a challenging pedestrian localization task. Our method exhibits a substantial improvement over this baseline regression method."
"We continue the study of four-point correlation functions by the hexagon tessellation approach initiated in 1611.05436 and 1611.05577. We consider planar tree-level correlation functions in $\mathcal{N} = 4$ supersymmetric Yang-Mills theory involving two non-protected operators. We find that, in order to reproduce the field theory result, it is necessary to include $SU(N)$ colour factors in the hexagon formalism; moreover, we find that the hexagon approach as it stands is naturally tailored to the single-trace part of correlation functions, and does not account for multi-trace admixtures. We discuss how to compute correlators involving double-trace operators, as well as more general $1/N$ effects; in particular we compute the whole next-to-leading order in the large-$N$ expansion of tree-level BMN two-point functions by tessellating a torus with punctures. Finally, we turn to the issue of ""wrapping"", L\""uscher-like corrections. We show that $SU(N)$ colour-dressing reproduces an earlier empirical rule for incorporating single-magnon wrapping, and we provide a direct interpretation of such wrapping processes in terms of $\mathcal{N}=2$ supersymmetric Feynman diagrams."
"Mobile IPv6 will be an integral part of the next generation Internet protocol. The importance of mobility in the Internet gets keep on increasing. Current specification of Mobile IPv6 does not provide proper support for reliability in the mobile network and there are other problems associated with it. In this paper, we propose Virtual Private Network (VPN) based Home Agent Reliability Protocol (VHAHA) as a complete system architecture and extension to Mobile IPv6 that supports reliability and offers solutions to the security problems that are found in Mobile IP registration part. The key features of this protocol over other protocols are: better survivability, transparent failure detection and recovery, reduced complexity of the system and workload, secure data transfer and improved overall performance."
"We generalize the method of our recent paper on the large-spin expansions of Gubser-Klebanov-Polyakov (GKP) strings to the large-spin and large-winding expansions of finite-size giant magnons and finite-size single spikes. By expressing the energies of long open strings in RxS2 in terms of Lambert's W-function, we compute the leading, subleading and next-to-subleading series of classical exponential corrections to the dispersion relations of Hofman-Maldacena giant magnons and infinite-winding single spikes. We also compute the corresponding expansions in the doubled regions of giant magnons and single spikes that are respectively obtained when their angular and linear velocities become smaller or greater than unity."
"Over the last decade, the continuing decline in the cost of digital computing technology has brought about a dramatic transformation in how digital instrumentation for radio astronomy is developed and operated. In most cases, it is now possible to interface consumer computing hardware, e.g. inexpensive graphics processing units and storage devices, directly to the raw data streams produced by radio telescopes. Such systems bring with them myriad benefits: straightforward upgrade paths, cost savings through leveraging an economy of scale, and a lowered barrier to entry for scientists and engineers seeking to add new instrument capabilities. Additionally, the typical data-interconnect technology used with general-purpose computing hardware -- Ethernet -- naturally permits multiple subscribers to a single raw data stream. This allows multiple science programs to be conducted in parallel. When combined with broad bandwidths and wide primary fields of view, radio telescopes become capable of achieving many science goals simultaneously. Moreover, because many science programs are not strongly dependent on observing cadence and direction (e.g. searches for extraterrestrial intelligence and radio transient surveys), these so-called ""commensal"" observing programs can dramatically increase the scientific productivity and discovery potential of an observatory. In this whitepaper, we detail a project to add an Ethernet-based commensal observing mode to the Jansky Very Large Array (VLA), and discuss how this mode could be leveraged to conduct a powerful program to constrain the distribution of advanced life in the universe through a search for radio emission indicative of technology. We also discuss other potential science use-cases for the system, and how the system could be used for technology development towards next-generation processing systems for the Next Generation VLA."
"In this paper, we study the problem of `test-driving' a detector, i.e. allowing a human user to get a quick sense of how well the detector generalizes to their specific requirement. To this end, we present the first system that estimates detector performance interactively without extensive ground truthing using a human in the loop. We approach this as a problem of estimating proportions and show that it is possible to make accurate inferences on the proportion of classes or groups within a large data collection by observing only $5-10\%$ of samples from the data. In estimating the false detections (for precision), the samples are chosen carefully such that the overall characteristics of the data collection are preserved. Next, inspired by its use in estimating disease propagation we apply pooled testing approaches to estimate missed detections (for recall) from the dataset. The estimates thus obtained are close to the ones obtained using ground truth, thus reducing the need for extensive labeling which is expensive and time consuming."
"The Internet of Things, or the IoT is a vision for a ubiquitous society wherein people and ""Things"" are connected in an immersively networked computing environment, with the connected ""Things"" providing utility to people/enterprises and their digital shadows, through intelligent social and commercial services. Translating this idea to a conceivable reality is a work in progress for more than a decade. Current IoT architectures are predicated on optimistic assumptions on the evolution and deployment of IoT technologies. We believe many of these assumptions will not be met, consequently impeding the practical and sustainable deployment of IoT. In this article, we explore use-cases across different applications domains that can potentially benefit from an IoT infrastructure, and analyze them in the context of an alternative world-view that is more grounded in reality. Despite this more conservative approach, we argue that adopting certain design paradigms when architecting an IoT ecosystem can achieve much of the promised benefits in a practical and sustainable manner."
"The maximum number of non-crossing straight-line perfect matchings that a set of $n$ points in the plane can have is known to be $O(10.0438^n)$ and $\Omega^*(3^n)$. The lower bound, due to Garc\'ia, Noy, and Tejel (2000) is attained by the double chain, which has $\Theta(3^n n^{O(1)})$ such matchings. We reprove this bound in a simplified way that uses the novel notion of down-free matching, and apply this approach on several other constructions. As a result, we improve the lower bound. First we show that double zigzag chain with $n$ points has $\Theta^*(\lambda^n)$ such matchings with $\lambda \approx 3.0532$. Next we analyze further generalizations of double zigzag chains - double $r$-chains. The best choice of parameters leads to a construction with $\Theta^*(\nu^n)$ matchings, with $\nu \approx 3.0930$. The derivation of this bound requires an analysis of a coupled dynamic-programming recursion between two infinite vectors."
"The Web is becoming more and more a wide software framework on which each one can compose and use contents, software applications and services. It can offer adequate computational resources to manage the complexity implied by the use of the five senses when involved in human machine interaction. The core of the paper describes how SOA (Service Oriented Architecture) can support multimodal interaction by pushing the I/O processing and reasoning to the cloud, improving naturalness. The benefits of cloud computing for multimodal interaction have been identified by emphasizing the flexibility and scalability of a SOA, and its characteristics to provide a more holistic view of interaction according to the variety of situations and users."
"In this work, we study a bipartite system composed by a pair of entangled qudits coupled to an environment. Initially, we derive a master equation and show how the dynamics can be restricted to a ""diagonal"" sector that includes a maximally entangled state (MES). Next, we solve this equation for mixed qutrit pairs and analyze the $I$-concurrence $C(t)$ for the effective state, which is needed to compute the geometric phase when the initial state is pure. Unlike (locally operated) isolated systems, the coupled system leads to a nontrivial time-dependence, with $C(t)$ generally decaying to zero at asymptotic times. However, when the initial condition gets closer to a MES state, the effective concurrence is more protected against the effects of decoherence, signaling a transition to an effective two-qubit MES state at asymptotic times. This transition is also observed in the geometric phase evolution, computed in the kinematic approach. Finally, we explore the system-environment coupling parameter space and show the existence of a Weyl symmetry among the various physical quantities."
"The millimeter wave is a promising technique for the next generation of mobile communication. The large antenna array is able to provide sufficient precoding gain to overcome the high pathloss at millimeter wave band. However, the accurate channel state information is the key for the precoding design. Unfortunately, the channel use overhead and complexity are two major challenges when estimating the channel with high-dimensional array. In this paper, we propose a two-stage approach which reduces the channel use overhead and the computational complexity. Specifically, in the first stage, we estimate the column subspace of the channel matrix. Based on the estimated column subspace, we design the training sounders to acquire the remaining coefficient matrix of the column subspace. By dividing the estimation task into two stages, the training sounders for the second stages are only targeted for the column subspace, which will save the channel uses and the computational complexity as well."
"Connectivity across landscapes influences a wide range of conservation-relevant ecological processes, including species movements, gene flow, and the spread of wildfire, pests, and diseases. Recent improvements in remote sensing data suggest great potential to advance connectivity models, but computational constraints hinder these advances. To address this challenge, we upgraded the widely-used Circuitscape connectivity package to the high performance Julia programming language. Circuitscape.jl allows users to solve problems faster via improved parallel processing and solvers, and supports applications to larger problems (e.g., datasets with hundreds of millions of cells). We document speed improvements of up to 1800\%. We also demonstrate scaling of problem sizes up to 437 million grid cells. These improvements allow modelers to work with higher resolution data, larger landscapes and perform sensitivity analysis effortlessly. These improvements accelerate the pace of innovation, helping modelers address pressing challenges like species range shifts under climate change. Our collaboration between ecologists and computer scientists has led to the use of connectivity models to inform conservation decisions. Further, these next generation connectivity models will produce results faster, facilitating stronger engagement with decision-makers."
"Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures."
"Emerging Big Data analytics and machine learning applications require a significant amount of computational power. While there exists a plethora of large-scale data processing frameworks which thrive in handling the various complexities of data-intensive workloads, the ever-increasing demand of applications have made us reconsider the traditional ways of scaling (e.g., scale-out) and seek new opportunities for improving the performance. In order to prepare for an era where data collection and processing occur on a wide range of devices, from powerful HPC machines to small embedded devices, it is crucial to investigate and eliminate the potential sources of inefficiency in the current state of the art platforms. In this paper, we address the current and upcoming challenges of pervasive data processing and present directions for designing the next generation of large-scale data processing systems."
"We report initial nucleon structure results computed on lattices with 2+1 dynamical M\""obius domain wall fermions at the physical point generated by the RBC and UKQCD collaborations. At this stage, we evaluate only connected quark contributions. In particular, we discuss the nucleon vector and axial-vector form factors, nucleon axial charge and the isovector quark momentum fraction. From currently available statistics, we estimate the stochastic accuracy of the determination of $g_A$ and $<x>_{u-d}$ to be around 10%, and we expect to reduce that to 5% within the next year. To reduce the computational cost of our calculations, we extensively use acceleration techniques such as low-eigenmode deflation and all-mode-averaging (AMA). We present a method for choosing optimal AMA parameters."
"By all measures, wireless networking has seen explosive growth over the past decade. Fourth Generation Long Term Evolution (4G LTE) cellular technology has increased the bandwidth available for smartphones, in essence, delivering broadband speeds to mobile devices. The most recent 5G technology is further enhancing the transmission speeds and cell capacity, as well as, reducing latency through the use of different radio technologies and is expected to provide Internet connections that are an order of magnitude faster than 4G LTE. Technology continues to advance rapidly, however, and the next generation, 6G, is already being envisioned. 6G will make possible a wide range of powerful, new applications including holographic telepresence, telehealth, remote education, ubiquitous robotics and autonomous vehicles, smart cities and communities (IoT), and advanced manufacturing (Industry 4.0, sometimes referred to as the Fourth Industrial Revolution), to name but a few. The advances we will see begin at the hardware level and extend all the way to the top of the software ""stack.""   Artificial Intelligence (AI) will also start playing a greater role in the development and management of wireless networking infrastructure by becoming embedded in applications throughout all levels of the network. The resulting benefits to society will be enormous.   At the same time these exciting new wireless capabilities are appearing rapidly on the horizon, a broad range of research challenges loom ahead. These stem from the ever-increasing complexity of the hardware and software systems, along with the need to provide infrastructure that is robust and secure while simultaneously protecting the privacy of users. Here we outline some of those challenges and provide recommendations for the research that needs to be done to address them."
"We review modeling of astrocyte ion dynamics with a specific focus on the implications of so-called spatial potassium buffering, where excess potassium in the extracellular space (ECS) is transported away to prevent pathological neural spiking. The recently introduced Kirchoff-Nernst-Planck (KNP) scheme for modeling ion dynamics in astrocytes (and brain tissue in general) is outlined and used to study such spatial buffering. We next describe how the ion dynamics of astrocytes may regulate microscopic liquid flow by osmotic effects and how such microscopic flow can be linked to whole-brain macroscopic flow. We thus include the key elements in a putative multiscale theory with astrocytes linking neural activity on a microscopic scale to macroscopic fluid flow."
"We study large $N$ tensor models on the lattice without disorder. We introduce techniques which can be applied to a wide class of models, and illustrate it by studying some specific rank-3 tensor models. In particular, we study Klebanov-Tarnopolsky model on lattice, Gurau-Witten model (by treating it as a tensor model on four sites) and also a new model which interpolates between these two models. In each model, we evaluate various four point functions at large $N$ and strong coupling, and discuss their spectrum and long time behaviors. We find similarities as well as differences from SYK model. We also generalize our analysis to rank-$D$ tensor models where we obtain analogous results as $D=3$ case for the four point functions which we computed. For $D>5$, we are able to compute the next-to-subleading ${1 \over N}$ corrections for a specific four point function."
"Agriculture provides economic opportunity through innovation; helps rural America to thrive; promotes agricultural production that better nourishes Americans; and aims to preserve natural resources through healthy private working lands, conservation, improved watersheds, and restored forests. From agricultural production to food supply, agriculture supports rural and urban economies across the U.S. It accounts for 10% of U.S. jobs and is currently creating new jobs in the growing field of data-driven farming. However, U.S. global competitiveness associated with food and nutrition security is at risk because of accelerated investments by many other countries in agriculture, food, energy, and resource management. To ensure U.S. global competitiveness and long-term food security, it is imperative that we build sustainable physical and cyber infrastructures to enable self-managing and sustainable farming. Such infrastructures should enable next generation precision-farms by harnessing modern and emerging technologies such as small satellites, broadband Internet, tele-operation, augmented reality, advanced data analytics, sensors, and robotics."
"Exascale I/O initiatives will require new and fully integrated I/O models which are capable of providing straightforward functionality, fault tolerance and efficiency. One solution is the Distributed Asynchronous Object Storage (DAOS) technology, which is primarily designed to handle the next generation NVRAM and NVMe technologies envisioned for providing a high bandwidth/IOPS storage tier close to the compute nodes in an HPC system. In conjunction with DAOS, the HDF5 library, an I/O library for scientific applications, will support end-to-end data integrity, fault tolerance, object mapping, index building and querying. This paper details the implementation and performance of the HDF5 library built over DAOS by using three representative scientific application codes."
"The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications."
"Transition Path Theory (TPT) provides a rigorous framework to investigate the dynamics of rare thermally activated transitions. In this theory, a central role is played by the forward committor function q^+(x), which provides the ideal reaction coordinate. Furthermore, the reactive dynamics and kinetics are fully characterized in terms of two time-independent scalar and vector distributions. In this work, we develop a scheme which enables all these ingredients of TPT to be efficiently computed using the short non-equilibrium trajectories generated by means of a specific combination of enhanced path sampling techniques. In particular, first, we further extend the recently introduced Self-Consistent Path Sampling (SCPS) algorithm in order to compute the committor q^+(x). Next, we show how this result can be exploited in order to define efficient algorithms which enable us to directly sample the transition path ensemble."
"In a previous paper the authors argued the case for incorporating ideas from innate immunity into articficial immune systems (AISs) and presented an outline for a conceptual framework for such systems. A number of key general properties observed in the biological innate and adaptive immune systems were hughlighted, and how such properties might be instantiated in artificial systems was discussed in detail. The next logical step is to take these ideas and build a software system with which AISs with these properties can be implemented and experimentally evaluated. This paper reports on the results of that step - the libtissue system."
"We study deterministic extractors for oblivious bit-fixing sources (a.k.a. resilient functions) and exposure-resilient functions with small min-entropy: of the function's n input bits, k << n bits are uniformly random and unknown to the adversary. We simplify and improve an explicit construction of extractors for bit-fixing sources with sublogarithmic k due to Kamp and Zuckerman (SICOMP 2006), achieving error exponentially small in k rather than polynomially small in k. Our main result is that when k is sublogarithmic in n, the short output length of this construction (O(log k) output bits) is optimal for extractors computable by a large class of space-bounded streaming algorithms.   Next, we show that a random function is an extractor for oblivious bit-fixing sources with high probability if and only if k is superlogarithmic in n, suggesting that our main result may apply more generally. In contrast, we show that a random function is a static (resp. adaptive) exposure-resilient function with high probability even if k is as small as a constant (resp. log log n). No explicit exposure-resilient functions achieving these parameters are known."
"We use the finite-entanglement scaling of infinite matrix product states (iMPS) to explore supposedly infinite order transitions. This universal method may have lower computational costs than finite-size scaling. To this end, we study possible MPS-based algorithms to find the ground states of the transverse axial next-nearest-neighbor Ising (ANNNI) model in a spin chain with first and second neighbor interactions and frustration. The ground state has four distinct phases with transitions of second order and one of supposedly infinite order, the Kosterlitz-Thouless transition. To explore phase transitions in the model, we study general quantities such as the correlation length, entanglement entropy and the second derivative of the energy with respect to the external field, and test the finite-entanglement scaling. We propose a scaling ansatz for the correlation length of a non-critical system in order to explore infinite order transitions. This method provides considerably less computational costs compared to the finite-size scaling method in [8], and quantities obtained by applying fixed boundary conditions (such as domain wall energy in [8]) are omitted. The results show good agreement with previous studies of finite-size scaling using DMRG."
"Conversational participants tend to immediately and unconsciously adapt to each other's language styles: a speaker will even adjust the number of articles and other function words in their next utterance in response to the number in their partner's immediately preceding utterance. This striking level of coordination is thought to have arisen as a way to achieve social goals, such as gaining approval or emphasizing difference in status. But has the adaptation mechanism become so deeply embedded in the language-generation process as to become a reflex? We argue that fictional dialogs offer a way to study this question, since authors create the conversations but don't receive the social benefits (rather, the imagined characters do). Indeed, we find significant coordination across many families of function words in our large movie-script corpus. We also report suggestive preliminary findings on the effects of gender and other features; e.g., surprisingly, for articles, on average, characters adapt more to females than to males."
We present a compilation of data on the 22 tentatively identified gamma-ray sources from the Third EGRET Catalog which may be detected by the next-generation imaging atmospheric Cherenkov telescopes.
"Conventional urban traffic control systems have been based on historical traffic data. Later advancements made use of detectors, which enabled the gathering of real time traffic data, in order to reorganize and calibrate traffic signalization programs. Further evolvement provided the ability to forecast traffic conditions, in order to develop traffic signalization programs and strategies precomputed and applied at the most appropriate time frame for the optimal control of the current traffic conditions. We, propose the next generation of traffic control systems based on principles of Artificial Intelligence and Context Awareness. Most of the existing algorithms use average waiting time or length of the queue to assess an algorithms performance. However, a low average waiting time may come at the cost of delaying other vehicles indefinitely. In our algorithm, besides the vehicle queue, we use fairness also as an important performance metric to assess an algorithms performance."
"Recent technological advances in Next Generation Sequencing tools have led to increasing speeds of DNA sample collection, preparation, and sequencing. One instrument can produce over 600 Gb of genetic sequence data in a single run. This creates new opportunities to efficiently handle the increasing workload. We propose a new method of fast genetic sequence analysis using the Dynamic Distributed Dimensional Data Model (D4M) - an associative array environment for MATLAB developed at MIT Lincoln Laboratory. Based on mathematical and statistical properties, the method leverages big data techniques and the implementation of an Apache Acculumo database to accelerate computations one-hundred fold over other methods. Comparisons of the D4M method with the current gold-standard for sequence analysis, BLAST, show the two are comparable in the alignments they find. This paper will present an overview of the D4M genetic sequence algorithm and statistical comparisons with BLAST."
"This paper investigates how concepts from game theory and ICT can contribute to solve challenges in demand side management, an important concept in the upcoming smart grid. Demand side management is about modifying the energy load distribution on the demand side, for example in order to reduce peaks in energy usage. This can be done by shifting energy demands where possible. We start with describing a number of smart grid concepts and assumptions (smart meters, pricing, appliance scheduling) and explain the advantages demand side management has. After the introduction of game theoretic concepts, it becomes possible to mathematically describe the demand side management problem. Next step is to solve the mathematical formulation, and show how complex demand side management becomes if the number of energy users increases. By means of distributed ICT algorithms however, it is possible to still find a solution. Based on existing literature, different algorithms are studied. Though results in literature looked promising, several conclusions on convergence of the algorithm in general, and convergence towards the most optimal results in particular are challenged."
"Due to rapid advancement in high-throughput techniques, such as microarrays and next generation sequencing technologies, biological data are increasing exponentially. The current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge. This review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data, including gene expression discretization, gene co-expression mining, gene expression clustering, finding genes in gene regulatory networks, enzyme/protein classifications, binding site classifications, and so on. It also presents a list of FCA-based software tools applied in biological domain and covers the challenges faced so far."
"In addition to posting news and status updates, many Twitter users post questions that seek various types of subjective and objective information. These questions are often labeled with ""Q&A"" hashtags, such as #lazyweb or #twoogle. We surveyed Twitter users and found they employ these Q&A hashtags both as a topical signifier (this tweet needs an answer!) and to reach out to those beyond their immediate followers (a community of helpful tweeters who monitor the hashtag). However, our log analysis of thousands of hashtagged Q&A exchanges reveals that nearly all replies to hashtagged questions come from a user's immediate follower network, contradicting user's beliefs that they are tapping into a larger community by tagging their question tweets. This finding has implications for designing next-generation social search systems that reach and engage a wide audience of answerers."
"Walrasian prices, if they exist, have the property that one can assign every buyer some bundle in her demand set, such that the resulting assignment will maximize social welfare. Unfortunately, this assumes carefully breaking ties amongst different bundles in the buyer demand set. Presumably, the shopkeeper cleverly convinces the buyer to break ties in a manner consistent with maximizing social welfare. Lacking such a shopkeeper, if buyers arrive sequentially and simply choose some arbitrary bundle in their demand set, the social welfare may be arbitrarily bad. In the context of matching markets, we show how to compute dynamic prices, based upon the current inventory, that guarantee that social welfare is maximized. Such prices are set without knowing the identity of the next buyer to arrive. We also show that this is impossible in general (e.g., for coverage valuations), but consider other scenarios where this can be done. We further extend our results to Bayesian and bounded rationality models."
"Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms perform better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark, containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96% of variables the same as the best available algorithm."
"There is a growing relationship between academic libraries and cloud computing. Therefore, understanding the beginnings and the current use of cloud base services in libraries is important. This will help understand the factors that libraries should consider in the future. The purpose of this paper is to better understand the future implementation of the cloud based software in academic settings. Using cloud based, web based, and other remote services may bring both advantages and disadvantages, some of which this paper will bring out. First, a brief literature review of the academic literature, and a review of available general-purpose cloud-based library products are conducted. Next, a real-life scenario for a mid-sized New Zealand institution of higher education is evaluated. This case involves moving from a locally hosted version of EZproxy to a cloud based version with support from the vendor. As this information system decision is an important one, this paper makes a contribution to the available literature and can be informative for librarians. In conclusion, academic libraries will gradually involve more pervasive use of cloud based systems. The examples of important factors to be considered in future decisions include timing and staffing."
"Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%."
"Resource allocation in High Performance Computing (HPC) settings is still not easy for end-users due to the wide variety of application and environment configuration options. Users have difficulties to estimate the number of processors and amount of memory required by their jobs, select the queue and partition, and estimate when job output will be available to plan for next experiments. Apart from wasting infrastructure resources by making wrong allocation decisions, overall user response time can also be negatively impacted. Techniques that exploit batch scheduler systems to predict waiting time and runtime of user jobs have already been proposed. However, we observed that such techniques are not suitable for predicting job memory usage. In this paper we introduce a tool to help users predict their memory requirements using machine learning. We describe the integration of the tool with a batch scheduler system, discuss how batch scheduler log data can be exploited to generate memory usage predictions through machine learning, and present results of two production systems containing thousands of jobs."
"We review photodetectors used in present running neutrino telescopes. After a brief historical discourse, the photodetector requirements for the next generation deep underwater neutrino telescopes are discussed. It is shown that large area vacuum hybrid phototubes are the closest to the ideal photodetector for such kind of applications when compared with other vacuum phototubes."
"Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted.In the experiments, AMSRN outperformed long short-term memory (LSTM) based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling."
"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.   Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update."
"In this paper we provide a quantum field theoretical study on the shear and bulk relaxation times. First, we find Kubo formulas for the shear and the bulk relaxation times, respectively. They are found by examining response functions of the stress-energy tensor. We use general properties of correlation functions and the gravitational Ward identity to parametrize analytical structures of the Green functions describing both sound and diffusion mode. We find that the hydrodynamic limits of the real parts of the respective energy-momentum tensor correlation functions provide us with the method of computing both the shear and bulk viscosity relaxation times. Next, we calculate the shear viscosity relaxation time using the diagrammatic approach in the Keldysh basis for the massless $\lambda\phi^4$ theory. We derive a respective integral equation which enables us to compute $\eta\tau_\pi$ and then we extract the shear relaxation time. The relaxation time is shown to be inversely related to the thermal width as it should be."
"This article presents the parallel implementation of the coupled harmonic oscillator. From the analytical solution of the coupled harmonic oscillator, the design parameters are obtained. After that, a numerical integration of the system with MATLAB, which is used as a tool of benchmark evaluation, is performed. Next, parallel implementation is performed using a well-known approach like OpenMP and WinAPI. Taking into account the errors of basic parameters of the simulated process, the generated oscillations of the proposed parallel realization are almost identical to the actual solution of the harmonic oscillator model. Test ways to optimize the parallel architecture of computing processes for software implementations of the considered application is carried out. The developed model is used to study a fixed priority scheduling algorithm for real-time parallel threads execution. The proposed parallel implementation of the considered dynamic system has an independent value and can be considered as a test for determining the characteristics of multi-core systems for time-critical simulation problems. Keywords: Harmonic oscillator, model, SMP, parallel programming, OpenMP;"
"We will discuss what it means for a general heat kernel on a metric measure space to be local. We show that the Wiener measure associated to Brownian motion is local. Next we show that locality of the Wiener measure plus a suitable decay bound of the heat kernel implies locality of the heat kernel. We define a class of metric spaces we call manifold-like that satisfy the prerequisites for these theorems. This class includes Riemannian manifolds, metric graphs, products and some quotients of these as well as a number of more singular spaces. There exists a natural Dirichlet form based on the Laplacian on manifold-like spaces and we show that the associated Wiener measure and heat kernel are both local. These results unify and generalise facts known for manifolds and metric graphs. They provide a useful tool for computing heat kernel asymptotics for a large class of metric spaces. As an application we compute the heat kernel asymptotics for two identical particles living on a metric graph."
"Understanding the relationship between the structure of light-harvesting systems and their excitation energy transfer properties is of fundamental importance in many applications including the development of next generation photovoltaics. Natural light harvesting in photosynthesis shows remarkable excitation energy transfer properties, which suggests that pigment-protein complexes could serve as blueprints for the design of nature inspired devices. Mechanistic insights into energy transport dynamics can be gained by leveraging numerically involved propagation schemes such as the hierarchical equations of motion (HEOM). Solving these equations, however, is computationally costly due to the adverse scaling with the number of pigments. Therefore virtual high-throughput screening, which has become a powerful tool in material discovery, is less readily applicable for the search of novel excitonic devices. We propose the use of artificial neural networks to bypass the computational limitations of established techniques for exploring the structure-dynamics relation in excitonic systems. Once trained, our neural networks reduce computational costs by several orders of magnitudes. Our predicted transfer times and transfer efficiencies exhibit similar or even higher accuracies than frequently used approximate methods such as secular Redfield theory"
"One of the major targets for next-generation cosmic microwave background (CMB) experiments is the detection of the primordial B-mode signal. Planning is under way for Stage-IV experiments that are projected to have instrumental noise small enough to make lensing and foregrounds the dominant source of uncertainty for estimating the tensor-to-scalar ratio $r$ from polarization maps. This makes delensing a crucial part of future CMB polarization science. In this paper we present a likelihood method for estimating the tensor-to-scalar ratio $r$ from CMB polarization observations, which combines the benefits of a full-scale likelihood approach with the tractability of the quadratic delensing technique. This method is a pixel space, all order likelihood analysis of the quadratic delensed B modes, and it essentially builds upon the quadratic delenser by taking into account all order lensing and pixel space anomalies. Its tractability relies on a crucial factorization of the pixel space covariance matrix of the polarization observations which allows one to compute the full Gaussian approximate likelihood profile, as a function of $r$, at the same computational cost of a single likelihood evaluation."
"Cascade regression framework has been shown to be effective for facial landmark detection. It starts from an initial face shape and gradually predicts the face shape update from the local appearance features to generate the facial landmark locations in the next iteration until convergence. In this paper, we improve upon the cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF) for simultaneous facial action unit recognition and facial landmark detection, which are two related face analysis tasks, but are seldomly exploited together. In particular, we first learn the relationships among facial action units and face shapes as a constraint. Then, in the proposed constrained joint cascade regression framework, with the help from the constraint, we iteratively update the facial landmark locations and the action unit activation probabilities until convergence. Experimental results demonstrate that the intertwined relationships of facial action units and face shapes boost the performances of both facial action unit recognition and facial landmark detection. The experimental results also demonstrate the effectiveness of the proposed method comparing to the state-of-the-art works."
"Many applications in signal processing require the estimation of some parameters of interest given a set of observed data. More specifically, Bayesian inference needs the computation of {\it a-posteriori} estimators which are often expressed as complicated multi-dimensional integrals. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and Monte Carlo methods are the only feasible approach. A very powerful class of Monte Carlo techniques is formed by the Markov Chain Monte Carlo (MCMC) algorithms. They generate a Markov chain such that its stationary distribution coincides with the target posterior density. In this work, we perform a thorough review of MCMC methods using multiple candidates in order to select the next state of the chain, at each iteration. With respect to the classical Metropolis-Hastings method, the use of multiple try techniques foster the exploration of the sample space. We present different Multiple Try Metropolis schemes, Ensemble MCMC methods, Particle Metropolis-Hastings algorithms and the Delayed Rejection Metropolis technique. We highlight limitations, benefits, connections and differences among the different methods, and compare them by numerical simulations."
"The next generation of PaaS technology accomplishes the true promise of object-oriented and 4GLs development with less effort. Now PaaS is becoming one of the core technical services for application development organizations. PaaS offers a resourceful and agile approach to develop, operate and deploy applications in a cost-effective manner. It is now turning out to be one of the preferred choices throughout the world, especially for globally distributed development environment. However it still lacks the scale of popularity and acceptance which Software-as-a-Service (SaaS) and Infrastructure-as-a-Service (IaaS) have attained. PaaS offers a promising future with novel technology architecture and evolutionary development approach. In this article, we identify the strengths, weaknesses, opportunities and threats for the PaaS industry. We then identify the various issues that will affect the different stakeholders of PaaS industry. This research will outline a set of recommendations for the PaaS practitioners to better manage this technology. For PaaS technology researchers, we also outline the number of research areas that need attention in coming future. Finally, we also included an online survey to outline PaaS technology market leaders. This will facilitate PaaS technology practitioners to have a more deep insight into market trends and technologies."
"Heavy crystal scintillators are used widely in HEP experiments for precision measurements of photons and electrons. Future HEP experiments, however, require crystal scintillators of more bright, more fast, more radiation hard and less cost. This paper discusses several R&D directions for the next generation of crystal detectors for future HEP experiments."
"Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments."
"Background: Since the invention of next-generation RNA sequencing (RNA-seq) technologies, they have become a powerful tool to study the presence and quantity of RNA molecules in biological samples and have revolutionized transcriptomic studies. The analysis of RNA-seq data at four different levels (samples, genes, transcripts, and exons) involve multiple statistical and computational questions, some of which remain challenging up to date.   Results: We review RNA-seq analysis tools at the sample, gene, transcript, and exon levels from a statistical perspective. We also highlight the biological and statistical questions of most practical considerations.   Conclusion: The development of statistical and computational methods for analyzing RNA- seq data has made significant advances in the past decade. However, methods developed to answer the same biological question often rely on diverse statical models and exhibit different performance under different scenarios. This review discusses and compares multiple commonly used statistical models regarding their assumptions, in the hope of helping users select appropriate methods as needed, as well as assisting developers for future method development."
"Results concerning the construction of quantum Bayesian error regions as a means to certify the quality of parameter point estimators have been reported in recent years. This task remains numerically formidable in practice for large dimensions and so far, no analytical expressions of the region size and credibility (probability of any given true parameter residing in the region) are known, which form the two principal region properties to be reported alongside a point estimator obtained from collected data. We first establish analytical formulas for the size and credibility that are valid for a uniform prior distribution over parameters, sufficiently large data samples and general constrained convex parameter-estimation settings. These formulas provide a means to an efficient asymptotic error certification for parameters of arbitrary dimensions. Next, we demonstrate the accuracies of these analytical formulas as compared to numerically computed region quantities with simulated examples in qubit and qutrit quantum-state tomography where computations of the latter are feasible."
"Machine learning models produce state-of-the-art results in many MRI images segmentation. However, most of these models are trained on very large datasets which come from experts manual labeling. This labeling process is very time consuming and costs experts work. Therefore finding a way to reduce this cost is on high demand. In this paper, we propose a segmentation method which exploits MRI images sequential structure to nearly drop out this labeling task. Only the first slice needs to be manually labeled to train the model which then infers the next slice's segmentation. Inference result is another datum used to train the model again. The updated model then infers the third slice and the same process is carried out until the last slice. The proposed model is an combination of two Random Forest algorithms: the classical one and a recent one namely Mondrian Forests. We applied our method on human left ventricle segmentation and results are very promising. This method can also be used to generate labels."
"Finding the music of the moment can often be a challenging problem, even for well-versed music listeners. Musical tastes are constantly in flux, and the problem of developing computational models for musical taste dynamics presents a rich and nebulous problem space. A variety of factors all play some role in determining preferences (e.g., popularity, musicological, social, geographical, generational), and these factors vary across different listeners and contexts. In this paper, we leverage a massive dataset on internet radio station creation from a large music streaming company in order to develop computational models of listener taste evolution. We delve deep into the complexities of this domain, identifying some of the unique challenges that it presents, and develop a model utilizing recurrent neural networks. We apply our model to the problem of next station prediction and show that it not only outperforms several baselines, but excels at long tail music personalization, particularly by learning the long-term dependency structure of listener music preference evolution."
"Provided significant future progress in artificial intelligence and computing, it may ultimately be possible to create multiple Artificial General Intelligences (AGIs), and possibly entire societies living within simulated environments. In that case, it should be possible to improve the problem solving capabilities of the system by increasing the speed of the simulation. If a minimal simulation with sufficient capabilities is created, it might manage to increase its own speed by accelerating progress in science and technology, in a way similar to the Technological Singularity. This may ultimately lead to large simulated civilizations unfolding at extreme temporal speedups, achieving what from the outside would look like a Temporal Singularity. Here we discuss the feasibility of the minimal simulation and the potential advantages, dangers, and connection to the Fermi paradox of the Temporal Singularity. The medium-term importance of the topic derives from the amount of computational power required to start the process, which could be available within the next decades, making the Temporal Singularity theoretically possible before the end of the century."
"Using the formalism of the light-cone wave function in perturbative QCD together with the hybrid factorization, we compute the cross-section for three particle production at forward rapidities in proton-nucleus collisions. In this picture, the three produced partons --- a quark accompanied by a gluon pair, or two quarks plus one antiquark --- are all generated via one or two successive splittings of a quark from the incoming proton, that was originally collinear with the latter. The three partons are put on-shell by their scattering off the nuclear target, described as Lorentz-contracted shockwave. We explicitly compute the three-parton Fock space components of the light-cone wave function of the incoming quark and its outgoing state, which encodes the information both on the evolution in time as well as the scattering process. This outgoing state is also an ingredient for other interesting calculations, like the next-to-leading order correction to the cross-section for the production of a pair of jets."
"Using the formalism of the light-cone wave function in perturbative QCD together with the hybrid factorization, we compute the cross-section for three particle production at forward rapidities in proton-nucleus collisions. We focus on the quark channel, in which the three produced partons -- a quark accompanied by a gluon pair, or two quarks plus one antiquark -- are all generated via two successive splittings starting with a quark that was originally collinear with the proton. The three partons are put on-shell by their scattering off the nuclear target, described as a Lorentz-contracted ""shockwave"". The three-parton component of the quark light-cone wave function that we compute on this occasion is also an ingredient for other interesting calculations, like the next-to-leading order correction to the cross-section for the production of a pair of jets."
"Cosmic Microwave Background experiments from COBE to Planck, have launched cosmology into an era of precision science, where many cosmological parameters are now determined to the percent level. Next generation telescopes, focussing on the cosmological 21cm signal from neutral hydrogen, will probe enormous volumes in the low-redshift Universe, and have the potential to determine dark energy properties and test modifications of Einstein's gravity. We study the 21cm bispectrum due to gravitational collapse as well as the contribution by line of sight perturbations in the form of the lensing-ISW bispectrum at low-redshifts ($z \sim 0.35-3$), targeted by upcoming neutral hydrogen intensity mapping experiments. We compute the expected bispectrum amplitudes and use a Fisher forecast model to compare power spectrum and bispectrum observations of intensity mapping surveys by CHIME, MeerKAT and SKA-mid. We find that combined power spectrum and bispectrum observations have the potential to decrease errors on the cosmological parameters by an order of magnitude compared to Planck. Finally, we compute the contribution of the lensing-ISW bispectrum, and find that, unlike for the cosmic microwave background analyses, it can safely be ignored for 21cm bispectrum observations."
"We compute the radiative quantum corrections to the critical exponents and amplitude ratios for O($N$) $\lambda\phi^{4}$ scalar high energy nonextensive $q$-field theories. We employ the field theoretic renormalization group approach through six methods for evaluating the high energy nonextensive critical exponents up to next-to-leading order while the high energy nonextensive amplitude ratios are computed up to leading level by applying three methods. Later we generalize these high energy nonextensive finite loop order results for any loop level. We find that the high energy nonextensive critical exponents are the same when obtained through all the methods employed. The same fact occurs for the high energy nonextensive amplitude ratios. Furthermore, we show that these high energy nonextensive universal quantities are equal to their low energy extensive counterparts, thus showing that the nonextensivity is broken down at high energies."
We report on recent progress we made towards the four-dimensional formulation of the sector improved residue subtraction. We explain how the subtraction scheme STRIPPER is generalized to higher multiplicities and therefore furnishes a general framework for the calculation of next-to-next-to-leading order cross sections in perturbative QCD.
"Single image super resolution is of great importance as a low-level computer vision task. Recent approaches with deep convolutional neural networks have achieved im-pressive performance. However, existing architectures have limitations due to the less sophisticated structure along with less strong representational power. In this work, to significantly enhance the feature representation, we proposed Triple Attention mixed link Network (TAN) which consists of 1) three different aspects (i.e., kernel, spatial and channel) of attention mechanisms and 2) fu-sion of both powerful residual and dense connections (i.e., mixed link). Specifically, the network with multi kernel learns multi hierarchical representations under different receptive fields. The output features are recalibrated by the effective kernel and channel attentions and feed into next layer partly residual and partly dense, which filters the information and enable the network to learn more powerful representations. The features finally pass through the spatial attention in the reconstruction network which generates a fusion of local and global information, let the network restore more details and improves the quality of reconstructed images. Thanks to the diverse feature recalibrations and the advanced information flow topology, our proposed model is strong enough to per-form against the state-of-the-art methods on the bench-mark evaluations."
"Making a simple model by choosing a limited number of features with the purpose of reducing the computational complexity of the algorithms involved in classification is one of the main issues in machine learning and data mining. The aim of Feature Selection (FS) is to reduce the number of redundant and irrelevant features and improve the accuracy of classification in a data set. We propose an efficient ISPSO-GLOBAL (Improved Seeding Particle Swarm Optimization GLOBAL) method which investigates the specified iterations to produce prominent features and store them in storage list. The goal is to find informative features based on its iteration frequency with favorable fitness for the next generation and high exploration. Our method exploits of a new initialization strategy in PSO which improves space search and utilizes chaos theory to enhance the population initialization, then we offer a new formula to determine the features size used in proposed method. Our experiments with real-world data sets show that the performance of the ISPSO-GLOBAL is superior comparing with state-of-the-art methods in most of the data sets."
"Millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) has been regarded to be an emerging solution for the next generation of communications, in which hybrid analog and digital precoding is an important method for reducing the hardware complexity and energy consumption associated with mixed signal components. However, the fundamental limitations of the existing hybrid precoding schemes is that they have high computational complexity and fail to fully exploit the spatial information. To overcome these limitations, this paper proposes, a deep-learning-enabled mmWave massive MIMO framework for effective hybrid precoding, in which each selection of the precoders for obtaining the optimized decoder is regarded as a mapping relation in the deep neural network (DNN). Specifically, the hybrid precoder is selected through training based on the DNN for optimizing precoding process of the mmWave massive MIMO. Additionally, we present extensive simulation results to validate the excellent performance of the proposed scheme. The results exhibit that the DNN-based approach is capable ofminimizing the bit error ratio (BER) and enhancing spectrum efficiency of the mmWave massive MIMO, which achieves better performance in hybrid precoding compared with conventional schemes while substantially reducing the required computational complexity."
"Virtual and augmented reality (VR/AR) systems are emerging technologies requiring data rates of multiple Gbps. Existing high quality VR headsets require connections through HDMI cables to a computer rendering rich graphic contents to meet the extremely high data transfer rate requirement. Such a cable connection limits the VR user's mobility and interferes with the VR experience. Current wireless technologies such as WiFi cannot support the multi-Gbps graphics data transfer. Instead, we propose to use visible light communication (VLC) for establishing high speed wireless links between a rendering computer and a VR headset. But, VLC transceivers are highly directional with narrow beams and require constant maintenance of line-of-sight (LOS) alignment between the transmitter and the receiver. Thus, we present a novel multi-detector hemispherical VR headset design to tackle the beam misalignment problem caused by the VR user's random head orientation. We provide detailed analysis on how the number of detectors on the headset can be minimized while maintaining the required beam alignment and providing high quality VR experience."
"The ability to accurately predict the surrounding environment is a foundational principle of intelligence in biological and artificial agents. In recent years, a variety of approaches have been proposed for learning to predict the physical dynamics of objects interacting in a visual scene. Here we conduct a systematic empirical evaluation of several state-of-the-art unsupervised deep learning models that are considered capable of learning the spatio-temporal structure of a popular dataset composed by synthetic videos of bouncing objects. We show that most of the models indeed obtain high accuracy on the standard benchmark of predicting the next frame of a sequence, and one of them even achieves state-of-the-art performance. However, all models fall short when probed with the more challenging task of generating multiple successive frames. Our results show that the ability to perform short-term predictions does not imply that the model has captured the underlying structure and dynamics of the visual environment, thereby calling for a careful rethinking of the metrics commonly adopted for evaluating temporal models. We also investigate whether the learning outcome could be affected by the use of curriculum-based teaching."
"This paper presents an accelerated composite gradient (ACG) variant, referred to as the AC-ACG method, for solving nonconvex smooth composite minimization problems. As opposed to well-known ACG variants that are either based on a known Lipschitz gradient constant or a sequence of maximum observed curvatures, the current one is based on the average of all past observed curvatures. More specifically, AC-ACG uses a positive multiple of the average of all observed curvatures until the previous iteration as a way to estimate the ""function curvature"" at the current point and then two resolvent evaluations to compute the next iterate. In contrast to other variable Lipschitz estimation variants, e.g., the ones based on the maximum curvature, AC-ACG always accepts the aforementioned iterate regardless how poor the Lipschitz estimation turns out to be. Finally, computational results are presented to illustrate the efficiency of AC-ACG on both randomly generated and real-world problem instances."
"We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image, and develop two approaches for efficiently computing the resulting adversarial examples. Finally, we demonstrate that adversarial training using our new attack yields image classification models that exhibit high robustness against the physically realizable attacks we study, offering the first effective generic defense against such attacks."
"Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies."
"Cyber-physical systems (CPSs) are important whenever computer technology interfaces with the physical world as it does in self-driving cars or aircraft control support systems. Due to their many subtleties, controllers for cyber-physical systems deserve to be held to the highest correctness standards. Their correct functioning is crucial, which explains the broad interest in safety analysis technology for their mathematical models, which are called hybrid systems because they combine discrete dynamics with continuous dynamics. Differential dynamic logic (dL) provides logical specification and rigorous reasoning techniques for hybrid systems. The logic dL is implemented in the theorem prover KeYmaera X, which has been instrumental in verifying ground robot controllers, railway systems, and the next-generation airborne collision avoidance system ACAS X. This chapter provides an informal overview of this logical approach to CPS safety that is detailed in a recent textbook on Logical Foundations of Cyber-Physical Systems. It also explains how safety guarantees obtained in the land of verified models reach the level of CPS execution unharmed."
"We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
"We address the problem of upper bounding the mean square error of MCMC estimators. Our analysis is nonasymptotic. We first establish a general result valid for essentially all ergodic Markov chains encountered in Bayesian computation and a possibly unbounded target function $f$. The bound is sharp in the sense that the leading term is exactly $\sigma_{\mathrm {as}}^2(P,f)/n$, where $\sigma_{\mathrm{as}}^2(P,f)$ is the CLT asymptotic variance. Next, we proceed to specific additional assumptions and give explicit computable bounds for geometrically and polynomially ergodic Markov chains under quantitative drift conditions. As a corollary, we provide results on confidence estimation."
"We propose a cost-optimal antenna vs. spectrum resource allocation strategy for mobile 5G MD-MIMO over Next-Generation PONs. Comparing wavelength overlay and shared wavelength approaches, split-PHY leads to solutions with higher mobile capacity than fronthaul."
"Outlier detection (also known as anomaly detection or deviation detection) is a process of detecting data points in which their patterns deviate significantly from others. It is common to have outliers in industry applications, which could be generated by different causes such as human error, fraudulent activities, or system failure. Recently, density-based methods have shown promising results, particularly among which Local Outlier Factor (LOF) is arguably dominating. However, one of the major drawbacks of LOF is that it is computationally expensive. Motivated by the mentioned problem, this research presents a novel pruning-based procedure in which the execution time of LOF is reduced while the performance is maintained. A novel Prune-based Local Outlier Factor (PLOF) approach is proposed, in which prior to employing LOF, outlierness of each data instance is measured. Next, based on a threshold, data instances that require further investigation are separated and LOF score is only computed for these points. Extensive experiments have been conducted and results are promising. Comparison experiments with the original LOF and two state-of-the-art variants of LOF have shown that PLOF produces higher accuracy and precision while reducing execution time."
"Effectively capturing graph node sequences in the form of vector embeddings is critical to many applications. We achieve this by (i) first learning vector embeddings of single graph nodes and (ii) then composing them to compactly represent node sequences. Specifically, we propose SENSE-S (Semantically Enhanced Node Sequence Embedding - for Single nodes), a skip-gram based novel embedding mechanism, for single graph nodes that co-learns graph structure as well as their textual descriptions. We demonstrate that SENSE-S vectors increase the accuracy of multi-label classification tasks by up to 50% and link-prediction tasks by up to 78% under a variety of scenarios using real datasets. Based on SENSE-S, we next propose generic SENSE to compute composite vectors that represent a sequence of nodes, where preserving the node order is important. We prove that this approach is efficient in embedding node sequences, and our experiments on real data confirm its high accuracy in node order decoding."
"We consider a generalization of the third degree price discrimination problem studied in Bergemann et al. (2015), where an intermediary between the buyer and the seller can design market segments to maximize any linear combination of consumer surplus and seller revenue. Unlike in Bergemann et al. (2015), we assume that the intermediary only has partial information about the buyer's value. We consider three different models of information, with increasing order of difficulty. In the first model, we assume that the intermediary's information allows him to construct a probability distribution of the buyer's value. Next we consider the sample complexity model, where we assume that the intermediary only sees samples from this distribution. Finally, we consider a bandit online learning model, where the intermediary can only observe past purchasing decisions of the buyer, rather than her exact value. For each of these models, we present algorithms to compute optimal or near optimal market segmentation."
"Designing efficient cooling systems for integrated circuits (ICs) relies on a deep understanding of the electro-thermal properties of transistors. To shed light on this issue in currently fabricated FinFETs, a quantum mechanical solver capable of revealing atomically-resolved electron and phonon transport phenomena from first-principles is required. In this paper, we consider a global, data-centric view of a state-of-the-art quantum transport simulator to optimize its execution on supercomputers. The approach yields coarse- and fine-grained data-movement characteristics, which are used for performance and communication modeling, communication-avoidance, and data-layout transformations. The transformations are tuned for the Piz Daint and Summit supercomputers, where each platform requires different caching and fusion strategies to perform optimally. The presented results make ab initio device simulation enter a new era, where nanostructures composed of over 10,000 atoms can be investigated at an unprecedented level of accuracy, paving the way for better heat management in next-generation ICs."
"Numerical simulations of high-speed forming and welding are of significant interest to industry, but are challenging due to the coupled physics and dynamic nature of the processes. With the advancement in hardware and computational capabilities, the next generation of computational methods, so called meshless methods, have received significant attention. Among all meshless methods, smoothed particle hydrodynamics (SPH) has received major consideration. The main advantage of the SPH method is to bypass the requirement for a numerical grid to calculate spatial derivatives. This avoids severe problems associated with mesh tangling and distortion which usually occur in Lagrangian analysis involving high-strain-rate events. In this study to better understand the effects of oxide layer, coating and diffused materials on weldability, a novel hybrid SPH platform was developed. Then, the high-speed impact between Steel/Steel, Copper/Titanium, and Aluminum/Steel were simulated. To experimentally validate the numerical efforts, results were compared to vaporizing foil actuator welding and explosive welding tests. Good agreement between the numerical simulations and experimental results provided confidence in the numerical modeling."
"As an emerging technology, blockchain has achieved great success in numerous application scenarios, from intelligent healthcare to smart cities. However, a long-standing bottleneck hindering its further development is the massive resource consumption attributed to the distributed storage and computation methods. This makes blockchain suffer from insufficient performance and poor scalability. Here, we analyze the recent blockchain techniques and demonstrate that the potential of widely-adopted consensus-based scaling is seriously limited, especially in the current era when Moore's law-based hardware scaling is about to end. We achieve this by developing an open-source benchmarking tool, called Prism, for investigating the key factors causing low resource efficiency and then discuss various topology and hardware innovations which could help to scale up blockchain. To the best of our knowledge, this is the first in-depth study that explores the next-generation scaling strategies by conducting large-scale and comprehensive benchmarking."
"Millimeter-wave/Terahertz (mmW/THz) communications have shown great potential for wideband massive access in next-generation cellular internet of things (IoT) networks. To decrease the length of pilot sequences and the computational complexity in wideband massive access, this paper proposes a novel joint activity detection and channel estimation (JADCE) algorithm. Specifically, after formulating JADCE as a problem of recovering a simultaneously sparse-group and low rank matrix according to the characteristics of mmW/THz channel, we prove that jointly imposing $l_1$ norm and low rank on such a matrix can achieve a robust recovery under sufficient conditions, and verify that the number of measurements derived for the mmW/THz wideband massive access system is significantly smaller than currently known measurements bound derived for the conventional simultaneously sparse and low-rank recovery. Furthermore, we propose a multi-rank aware method by exploiting the quotient geometry of product of complex rank-$L$ matrices with the number of scattering clusters $L$. Theoretical analysis and simulation results confirm the superiority of the proposed algorithm in terms of computational complexity, detection error rate, and channel estimation accuracy."
"Clustering consists of partitioning data objects into subsets called clusters according to some similarity criteria. This paper addresses a generalization called quasi-clustering that allows overlapping of clusters, and which we link to biclustering. Biclustering simultaneously groups the objects and features so that a specific group of objects has a special group of features. In recent years, biclustering has received a lot of attention in several practical applications. In this paper we consider a bi-objective optimization of biclustering problem with binary data. First we present an integer programing formulations for the bi-objective optimization biclustering. Next we propose a constructive heuristic based on the set intersection operation and its efficient implementation for solving a series of mono-objective problems used inside the Epsilon-constraint method (obtained by keeping only one objective function and the other objective function is integrated into constraints). Finally, our experimental results show that using CPLEX solver as an exact algorithm for finding an optimal solution drastically increases the computational cost for large instances, while our proposed heuristic provides very good results and significantly reduces the computational expense."
"In a recent series of papers it has been established that variants of Gradient Descent/Ascent and Mirror Descent exhibit last iterate convergence in convex-concave zero-sum games. Specifically, \cite{DISZ17, LiangS18} show last iterate convergence of the so called ""Optimistic Gradient Descent/Ascent"" for the case of \textit{unconstrained} min-max optimization. Moreover, in \cite{Metal} the authors show that Mirror Descent with an extra gradient step displays last iterate convergence for convex-concave problems (both constrained and unconstrained), though their algorithm does not follow the online learning framework; it uses extra information rather than \textit{only} the history to compute the next iteration. In this work, we show that ""Optimistic Multiplicative-Weights Update (OMWU)"" which follows the no-regret online learning framework, exhibits last iterate convergence locally for convex-concave games, generalizing the results of \cite{DP19} where last iterate convergence of OMWU was shown only for the \textit{bilinear case}. We complement our results with experiments that indicate fast convergence of the method."
"In order to ensure the robust actuation of a plan, execution must be adaptable to unexpected situations in the world and to exogenous events. This is critical in domains in which committing to a wrong ordering of actions can cause the plan failure, even when all the actions succeed. We propose an approach to the execution of a task plan that permits some adaptability to unexpected observations of the state while maintaining the validity of the plan through online reasoning.   Our approach computes an adaptable, partially-ordered plan from a given totally-ordered plan. The partially-ordered plan is adaptable in that it can exploit beneficial differences between the world and what was expected. The approach is general in that it can be used with any task planner that produces either a totally or a partially-ordered plan. We propose a plan execution algorithm that computes online the complete set of valid totally-ordered plans described by an adaptable partially-ordered plan together with the probability of success for each of them. This set is then used to choose the next action to execute."
"In this paper we show that QUASAR-370 large area hybrid phototube developed for and successfully used in a number of astroparticle physics experiments, the lake Baikal deep underwater neutrino experiment among them, could be used as a prototype of a photodetector for the next generation of giant neutrino telescopes."
"Connected vehicular network is one of the key enablers for next generation cloud/fog-supported autonomous driving vehicles. Most connected vehicular applications require frequent status updates and Age of Information (AoI) is a more relevant metric to evaluate the performance of wireless links between vehicles and cloud/fog servers. This paper introduces a novel proactive and data-driven approach to optimize the driving route with a main objective of guaranteeing the confidence of AoI. In particular, we report a study on three month measurements of a multi-vehicle campus shuttle system connected to cloud/fog servers via a commercial LTE network. We establish empirical models for AoI in connected vehicles and investigate the impact of major factors on the performance of AoI. We also propose a Deep Q-Learning Netwrok (DQN)-based algorithm to decide the optimal driving route for each connected vehicle with maximized confidence level. Numerical results show that the proposed approach can lead to a significant improvement on the AoI confidence for various types of services supported."
"Non-orthogonal multiple access (NOMA) has been considered one of the most promising radio access techniques for next-generation cellular networks. In this paper, we study the joint user and power scheduling for downlink NOMA over fading channels. Specifically, we focus on a stochastic optimization problem to maximize the weighted average sum rate while ensuring given minimum average data rates of users. To address this problem, we first develop an opportunistic user and power scheduling algorithm (OUPS) based on the duality and stochastic optimization theory. By OUPS, the stochastic problem is transformed into a series of deterministic ones for the instantaneous weighted sum rate maximization for each slot. Thus, we additionally develop a heuristic algorithm with very low computational complexity, called user selection and power allocation algorithm (USPA), for the instantaneous weighted sum rate maximization problem. Via simulation results, we demonstrate that USPA provides near-optimal performance with very low computational complexity, and OUPS well guarantees given minimum average data rates."
"Counter machines have achieved a newfound relevance to the field of natural language processing (NLP): recent work suggests some strong-performing recurrent neural networks utilize their memory as counters. Thus, one potential way to understand the success of these networks is to revisit the theory of counter computation. Therefore, we study the abilities of real-time counter machines as formal grammars, focusing on formal properties that are relevant for NLP models. We first show that several variants of the counter machine converge to express the same class of formal languages. We also prove that counter languages are closed under complement, union, intersection, and many other common set operations. Next, we show that counter machines cannot evaluate boolean expressions, even though they can weakly validate their syntax. This has implications for the interpretability and evaluation of neural network systems: successfully matching syntactic patterns does not guarantee that counter memory accurately encodes compositional semantics. Finally, we consider whether counter languages are semilinear. This work makes general contributions to the theory of formal languages that are of potential interest for understanding recurrent neural networks."
"For 35 years, {\it ab initio} molecular dynamics (AIMD) has been the method of choice for modeling complex atomistic phenomena from first principles. However, most AIMD applications are limited by computational cost to systems with thousands of atoms at most. We report that a machine learning-based simulation protocol (Deep Potential Molecular Dynamics), while retaining {\it ab initio} accuracy, can simulate more than 1 nanosecond-long trajectory of over 100 million atoms per day, using a highly optimized code (GPU DeePMD-kit) on the Summit supercomputer. Our code can efficiently scale up to the entire Summit supercomputer, attaining $91$ PFLOPS in double precision ($45.5\%$ of the peak) and {$162$/$275$ PFLOPS in mixed-single/half precision}. The great accomplishment of this work is that it opens the door to simulating unprecedented size and time scales with {\it ab initio} accuracy. It also poses new challenges to the next-generation supercomputer for a better integration of machine learning and physical modeling."
"Fast cryogenic switches with ultra-low power dissipation are highly sought-after for control electronics of quantum computers, space applications and next generation logic circuits. However, existing high-frequency switches are often bulky, lossy or require large source-drain and gate currents for operation, making them unsuitable for many applications and difficult to interface to semiconducting devices. Here we present an electrically controlled superconducting switch based on a metallic nanowire. Transition from superconducting to resistive state is realized by tunneling of high-energy electrons from a gate contact through an insulating barrier. Operating gate currents are several orders of magnitude smaller than the nanowire critical source-drain current, effectively resulting in a voltage-controlled device. This superconducting switch is fast, self-resets from normal to superconducting state, and can operate in large magnetic fields, making it an ideal component for low-power cryogenic applications and quantum computing architectures."
"Name entity recognition in noisy user-generated texts is a difficult task usually enhanced by incorporating an external resource of information, such as gazetteers. However, gazetteers are task-specific, and they are expensive to build and maintain. This paper adopts and improves the approach of Aguilar et al. by presenting a novel feature, called Local Distance Neighbor, which substitutes gazetteers. We tested the new approach on the W-NUT-2017 dataset, obtaining state-of-the-art results for the Group, Person and Product categories of Named Entities. Next, we added 851 manually labeled samples to the W-NUT-2017 dataset to account for named entities in the Tor Darknet related to weapons and drug selling. Finally, our proposal achieved an entity and surface F1 scores of 52.96% and 50.57% on this extended dataset, demonstrating its usefulness for Law Enforcement Agencies to detect named entities in the Tor hidden services."
"Development of next-generation electronic devices for applications call for the discovery of quantum materials hosting novel electronic, magnetic, and topological properties. Traditional electronic structure methods require expensive computation time and memory consumption, thus a fast and accurate prediction model is desired with increasing importance. Representing the interactions among atomic orbitals in any material, a material Hamiltonian provides all the essential elements that control the structure-property correlations in inorganic compounds. Effective learning of material Hamiltonian by developing machine learning methodologies therefore offers a transformative approach to accelerate the discovery and design of quantum materials. With this motivation, we present and compare several different graph convolution networks that are able to predict the band gap for inorganic materials. The models are developed to incorporate two different features: the information of each orbital itself and the interaction between each other. The information of each orbital includes the name, relative coordinates with respect to the center of super cell and the atom number, while the interaction between orbitals are represented by the Hamiltonian matrix. The results show that our model can get a promising prediction accuracy with cross-validation."
"Recently, many Convolution Neural Networks (CNN) have been successfully employed in bitemporal SAR image change detection. However, most of the existing networks are too heavy and occupy a large volume of memory for storage and calculation. Motivated by this, in this paper, we propose a lightweight neural network to reduce the computational and spatial complexity and facilitate the change detection on an edge device. In the proposed network, we replace normal convolutional layers with bottleneck layers that keep the same number of channels between input and output. Next, we employ dilated convolutional kernels with a few non-zero entries that reduce the running time in convolutional operators. Comparing with the conventional convolutional neural network, our light-weighted neural network will be more efficient with fewer parameters. We verify our light-weighted neural network on four sets of bitemporal SAR images. The experimental results show that the proposed network can obtain better performance than the conventional CNN and has better model generalization, especially on the challenging datasets with complex scenes."
"In single-channel speech enhancement, methods based on full-band spectral features have been widely studied. However, only a few methods pay attention to non-full-band spectral features. In this paper, we explore a knowledge distillation framework based on sub-band spectral mapping for single-channel speech enhancement. Specifically, we divide the full frequency band into multiple sub-bands and pre-train an elite-level sub-band enhancement model (teacher model) for each sub-band. These teacher models are dedicated to processing their own sub-bands. Next, under the teacher models' guidance, we train a general sub-band enhancement model (student model) that works for all sub-bands. Without increasing the number of model parameters and computational complexity, the student model's performance is further improved. To evaluate our proposed method, we conducted a large number of experiments on an open-source data set. The final experimental results show that the guidance from the elite-level teacher models dramatically improves the student model's performance, which exceeds the full-band model by employing fewer parameters."
"Massive exploitation of next-generation sequencing technologies requires dealing with both: huge amounts of data and complex bioinformatics pipelines. Computing architectures have evolved to deal with these problems, enabling approaches that were unfeasible years ago: accelerators and Non-Volatile Memories (NVM) are becoming widely used to enhance the most demanding workloads. However, bioinformatics workloads are usually part of bigger pipelines with different and dynamic needs in terms of resources. The introduction of Software Defined Infrastructures (SDI) for data centers provides roots to dramatically increase the efficiency in the management of infrastructures. SDI enables new ways to structure hardware resources through disaggregation, and provides new hardware composability and sharing mechanisms to deploy workloads in more flexible ways. In this paper we study a state-of-the-art genomics application, SMUFIN, aiming to address the challenges of future HPC facilities."
"This paper presents an overview of the topology of D-Wave's next-generation quantum processors. It provides examples of minor embeddings and discusses performance of embedding algorithms for the new topology compared to the existing Chimera topology. It also presents some initial performance results for simple, standard Ising model classes of problems."
"Dynamical properties of a resonator can be analyzed using the Rayleigh-Lorentz invariant which is not an exact constant but varies more or less over time. We investigate the time behavior of this invariant for a flux qubit resonator in order for better understanding of qubit-information detection with the resonator. Flux qubit resonators can be utilized in implementing diverse next generation nano-optic and nano-electronic devices such as quantum computing systems. Through the analyses of the temporal evolution of the invariant, we derive a condition for optimal adiabatic qubit-information detection with the resonator. This condition is helpful for controlling the dynamics of qubit resonators over long periods of time. It is necessary to consider it when designing a nano-resonator used for quantum nondemolition readouts of qubit states, crucial in quantum computation."
"We present Tactician, a tactic learner and prover for the Coq Proof Assistant. Tactician helps users make tactical proof decisions while they retain control over the general proof strategy. To this end, Tactician learns from previously written tactic scripts and gives users either suggestions about the next tactic to be executed or altogether takes over the burden of proof synthesis. Tactician's goal is to provide users with a seamless, interactive, and intuitive experience together with robust and adaptive proof automation. In this paper, we give an overview of Tactician from the user's point of view, regarding both day-to-day usage and issues of package dependency management while learning in the large. Finally, we give a peek into Tactician's implementation as a Coq plugin and machine learning platform."
"Neutrino experiments study the least understood of the Standard Model particles by observing their direct interactions with matter or searching for ultra-rare signals. The study of neutrinos typically requires overcoming large backgrounds, elusive signals, and small statistics. The introduction of state-of-the-art machine learning tools to solve analysis tasks has made major impacts to these challenges in neutrino experiments across the board. Machine learning algorithms have become an integral tool of neutrino physics, and their development is of great importance to the capabilities of next generation experiments. An understanding of the roadblocks, both human and computational, and the challenges that still exist in the application of these techniques is critical to their proper and beneficial utilization for physics applications. This review presents the current status of machine learning applications for neutrino physics in terms of the challenges and opportunities that are at the intersection between these two fields."
"Strongly-interacting nanomagnetic systems are pivotal across next-generation technologies including reconfigurable magnonics and neuromorphic computation. Controlling magnetisation state and local coupling between neighbouring nanoelements allows vast reconfigurable functionality and a host of associated functionalities. However, existing designs typically suffer from an inability to tailor inter-element coupling post-fabrication and nanoelements restricted to a pair of Ising-like magnetisation states. Here, we propose a new class of reconfigurable magnonic crystal incorporating nanodisks as the functional element. Magnetic nanodisks are crucially bistable in macrospin and vortex states, allowing inter-element coupling to be selectively activated (macrospin) or deactivated (vortex). Through microstate engineering, we leverage the distinct coupling behaviours and magnonic band structures of bistable nanodisks to achieve reprogrammable magnonic waveguiding, bending, gating and phase-shifting across a 2D network. The potential of nanodisk-based magnonics for wave-based computation is demonstrated via an all-magnon interferometer exhibiting XNOR logic functionality. Local microstate control is achieved here via topological magnetic writing using a magnetic force microscope tip."
"We argue that the next frontier in natural language understanding (NLU) and generation (NLG) will include models that can efficiently access external structured knowledge repositories. In order to support the development of such models, we release the VisualSem knowledge graph (KG) which includes nodes with multilingual glosses and multiple illustrative images and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any (neural network) model pipeline and we encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. VisualSem as well as the multi-modal retrieval model are publicly available and can be downloaded in: https://github.com/iacercalixto/visualsem."
"This work addresses inverse linear optimization where the goal is to infer the unknown cost vector of a linear program. Specifically, we consider the data-driven setting in which the available data are noisy observations of optimal solutions that correspond to different instances of the linear program. We introduce a new formulation of the problem that, compared to other existing methods, allows the recovery of a less restrictive and generally more appropriate admissible set of cost estimates. It can be shown that this inverse optimization problem yields a finite number of solutions, and we develop an exact two-phase algorithm to determine all such solutions. Moreover, we propose an efficient decomposition algorithm to solve large instances of the problem. The algorithm extends naturally to an online learning environment where it can be used to provide quick updates of the cost estimate as new data becomes available over time. For the online setting, we further develop an effective adaptive sampling strategy that guides the selection of the next samples. The efficacy of the proposed methods is demonstrated in computational experiments involving two applications, customer preference learning and cost estimation for production planning. The results show significant reductions in computation and sampling efforts."
"Optics is a promising platform in which to help realise the next generation of fast, parallel and energy-efficient computation. We demonstrate a reconfigurable free-space optical multiplier that is capable of over 3000 computations in parallel, using spatial light modulators with a pixel resolution of only 340x340. This enables vector-matrix multiplication and parallel vector-vector multiplication with vector size of up to 56. Our design is the first to simultaneously support optical implementation of reconfigurable, large-size and real-valued linear algebraic operations. Such an optical multiplier can serve as a building block of special-purpose optical processors such as optical neural networks and optical Ising machines."
"Mass spectrometry based omics data analysis require significant time and resources. To date, few parallel algorithms have been proposed for deducing peptides from mass spectrometry based data. However, these parallel algorithms were designed, and developed when the amount of data that needed to be processed was smaller in scale. In this paper, we prove that the communication bound that is reached by the \emph{existing} parallel algorithms is $\Omega(mn+2r\frac{q}{p})$, where $m$ and $n$ are the dimensions of the theoretical database matrix, $q$ and $r$ are dimensions of spectra, and $p$ is the number of processors. We further prove that communication-optimal strategy with fast-memory $\sqrt{M} = mn + \frac{2qr}{p}$ can achieve $\Omega({\frac{2mnq}{p}})$ but is not achieved by any existing parallel proteomics algorithms till date. To further validate our claim, we performed a meta-analysis of published parallel algorithms, and their performance results. We show that sub-optimal speedups with increasing number of processors is a direct consequence of not achieving the communication lower-bounds proved in this paper. Consequently, we assert that next-generation of provable, and demonstrated superior parallel algorithms are urgently needed for MS based large systems-biology studies especially for meta-proteomics, protegenomics, microbiome, and proteomics for non-model organisms."
"Metabolic heterogeneity is widely recognised as the next challenge in our understanding of non-genetic variation. A growing body of evidence suggests that metabolic heterogeneity may result from the inherent stochasticity of intracellular events. However, metabolism has been traditionally viewed as a purely deterministic process, on the basis that highly abundant metabolites tend to filter out stochastic phenomena. Here we bridge this gap with a general method for prediction of metabolite distributions across single cells. By exploiting the separation of time scales between enzyme expression and enzyme kinetics, our method produces estimates for metabolite distributions without the lengthy stochastic simulations that would be typically required for large metabolic models. The metabolite distributions take the form of Gaussian mixture models that are directly computable from single-cell expression data and standard deterministic models for metabolic pathways. The proposed mixture models provide a systematic method to predict the impact of biochemical parameters on metabolite distributions. Our method lays the groundwork for identifying the molecular processes that shape metabolic heterogeneity and its functional implications in disease."
"Scheduling plays a pivotal role in multi-user wireless communications, since the quality of service of various users largely depends upon the allocated radio resources. In this paper, we propose a novel scheduling algorithm with contiguous frequency-domain resource allocation (FDRA) based on deep reinforcement learning (DRL) that jointly selects users and allocates resource blocks (RBs). The scheduling problem is modeled as a Markov decision process, and a DRL agent determines which user and how many consecutive RBs for that user should be scheduled at each RB allocation step. The state space, action space, and reward function are delicately designed to train the DRL network. More specifically, the originally quasi-continuous action space, which is inherent to contiguous FDRA, is refined into a finite and discrete action space to obtain a trade-off between the inference latency and system performance. Simulation results show that the proposed DRL-based scheduling algorithm outperforms other representative baseline schemes while having lower online computational complexity."
"I consider the problem of computing the mass of a charged, gravitating particle in quantum field theory. It is shown how solving for the first quantized propagator of a particle in the presence of its own potentials reproduces the gauge and general coordinate invariant sum over an infinite class of diagrams. The distinguishing feature of this class of diagrams is that all closed loops contain part of the continuous matter line running from early to late times. The next order term would have one closed loop external to the continuous matter line, and so on. I argue that the gravitational potentials in the 0-th order term may permit the formation of bound states, which would then dominate the propagator. It is conceivable that this provides an tractable technique for computing the masses of fundamental particles from first principles. It is also conceivable that the expansion in external loops permits gravity to regulate certain ultraviolet divergences."
"While deep learning (DL) architectures like convolutional neural networks (CNNs) have enabled effective solutions in image denoising, in general their implementations overly rely on training data, lack interpretability, and require tuning of a large parameter set. In this paper, we combine classical graph signal filtering with deep feature learning into a competitive hybrid design -- one that utilizes interpretable analytical low-pass graph filters and employs 80% fewer network parameters than state-of-the-art DL denoising scheme DnCNN. Specifically, to construct a suitable similarity graph for graph spectral filtering, we first adopt a CNN to learn feature representations per pixel, and then compute feature distances to establish edge weights. Given a constructed graph, we next formulate a convex optimization problem for denoising using a graph total variation (GTV) prior. Via a $l_1$ graph Laplacian reformulation, we interpret its solution in an iterative procedure as a graph low-pass filter and derive its frequency response. For fast filter implementation, we realize this response using a Lanczos approximation. Experimental results show that in the case of statistical mistmatch, our algorithm outperformed DnCNN by up to 3dB in PSNR."
"Real-time object detection in videos using lightweight hardware is a crucial component of many robotic tasks. Detectors using different modalities and with varying computational complexities offer different trade-offs. One option is to have a very lightweight model that can predict from all modalities at once for each frame. However, in some situations (e.g., in static scenes) it might be better to have a more complex but more accurate model and to extrapolate from previous predictions for the frames coming in at processing time. We formulate this task as a sequential decision making problem and use reinforcement learning (RL) to generate a policy that decides from the RGB input which detector out of a portfolio of different object detectors to take for the next prediction. The objective of the RL agent is to maximize the accuracy of the predictions per image. We evaluate the approach on the Waymo Open Dataset and show that it exceeds the performance of each single detector."
"VVC is the next generation video coding standard, offering coding capability beyond HEVC standard. The high computational complexity of the latest video coding standards requires high-level parallelism techniques, in order to achieve real-time and low latency encoding and decoding. HEVC and VVC include tile grid partitioning that allows to process simultaneously rectangular regions of a frame with independent threads. The tile grid may be further partitioned into a horizontal sub-grid of Rectangular Slices (RSs), increasing the partitioning flexibility. The dynamic Tile and Rectangular Slice (TRS) partitioning solution proposed in this paper benefits from this flexibility. The TRS partitioning is carried-out at the frame level, taking into account both spatial texture of the content and encoding times of previously encoded frames. The proposed solution searches the best partitioning configuration that minimizes the trade-off between multi-thread encoding time and encoding quality loss. Experiments prove that the proposed solution, compared to uniform TRS partitioning, significantly decreases multi-thread encoding time, with slightly better encoding quality."
"Wireless edge caching is a popular strategy to avoid backhaul congestion in the next generation networks, where the content is cached in advance at the base stations to fulfil the redundant requests during peak periods. In the edge caching data, the missing observations are inevitable due to dynamic selective popularity. Among the completion methods, the tensor-based models have been shown to be the most advantageous for missing data imputation. Also, since the observations are correlated across time, files, and base stations, in this paper, we formulate the caching, prediction and recommendation problem as a fourth-order tensor completion problem. Since the content library can be large leading to a large dimension tensor, we modify the latent norm-based Frank-Wolfe (FW) algorithm with tensor-ring decomposition towards a lower time complexity using random mode selection. Analyzing the time and space complexity of the algorithm shows $N$-times reduction in computational time where $N$ is the order of tensor. Simulations with MovieLens dataset shows the approximately similar reconstruction errors for the presented FW algorithm as compared to that of the recent FW algorithm, albeit with lower computation overhead. It is also demonstrated that the completed tensor improves normalized cache hit rates for linear prediction schemes."
"Speech emotion recognition is a vital contributor to the next generation of human-computer interaction (HCI). However, current existing small-scale databases have limited the development of related research. In this paper, we present LSSED, a challenging large-scale english speech emotion dataset, which has data collected from 820 subjects to simulate real-world distribution. In addition, we release some pre-trained models based on LSSED, which can not only promote the development of speech emotion recognition, but can also be transferred to related downstream tasks such as mental health analysis where data is extremely difficult to collect. Finally, our experiments show the necessity of large-scale datasets and the effectiveness of pre-trained models. The dateset will be released on https://github.com/tobefans/LSSED."
"We go beyond a systematic review of the semiclassical approaches for determining the scaling dimensions of fixed-charge operators in $U(1)$ and $O(N)$ models by introducing a general strategy apt at determining the relation between a given charge configuration and the associated operators for more involved symmetry groups such as the $U(N) \times U(M)$. We show how, varying the charge configuration, it is possible to access anomalous dimensions of different operators transforming according to a variety of irreducible representations of the non-abelian symmetry group without the aid of diagrammatical computations. We illustrate our computational strategy by determining the anomalous dimensions of several composite operators to the next-to-leading order in the semiclassical expansion for the $U(N) \times U(M)$ conformal field theory (CFT) in $4-\epsilon$ dimensions. Thanks to the powerful interplay between semiclassical methods and group theory we can, for the first time, extract scaling dimensions for a wide range of operators."
"The Massive Multiple Input Multiple Output (MIMO) system is a core technology of the next generation communication. With the growing complexity of CSI, CSI feedback in massive MIMO system has become a bottleneck problem, the traditional compressive sensing based CSI feedback approaches have limited performance. Recently, numerous deep learning based CSI feedback approaches demonstrate their efficiency and potential. However, most existing methods improve accuracy at the cost of computational complexity and the accuracy decreases significantly as the CSI compression rate increases. This paper presents a novel neural network CLNet tailored for CSI feedback problem based on the intrinsic properties of CSI. The experiment result shows that CLNet outperforms the state-of-the-art method by average accuracy improvement of 5.41% in both outdoor and indoor scenarios with average 24.1% less computational overhead. Codes are available at GitHub."
"We study the gravitational radiation emitted during the scattering of two spinless bodies in the post-Minkowskian Effective Field Theory approach. We derive the conserved stress-energy tensor linearly coupled to gravity and the classical probability amplitude of graviton emission at leading and next-to-leading order in the Newton's constant $G$. The amplitude can be expressed in compact form as one-dimensional integrals over a Feynman parameter involving Bessel functions. We use it to recover the leading-order radiated angular momentum expression. Upon expanding it in the relative velocity between the two bodies $v$, we compute the total four-momentum radiated into gravitational waves at leading-order in $G$ and up to an order $v^8$, finding agreement with what was recently computed using scattering amplitude methods. Our results also allow us to investigate the zero frequency limit of the emitted energy spectrum."
"Checkpointing large amounts of related data concurrently to stable storage is a common I/O pattern of many HPC applications. However, such a pattern frequently leads to I/O bottlenecks that lead to poor scalability and performance. As modern HPC infrastructures continue to evolve, there is a growing gap between compute capacity vs. I/O capabilities. Furthermore, the storage hierarchy is becoming increasingly heterogeneous: in addition to parallel file systems, it comprises burst buffers, key-value stores, deep memory hierarchies at node level, etc. In this context, state of art is insufficient to deal with the diversity of vendor APIs, performance and persistency characteristics. This extended abstract presents an overview of VeloC (Very Low Overhead Checkpointing System), a checkpointing runtime specifically design to address these challenges for the next generation Exascale HPC applications and systems. VeloC offers a simple API at user level, while employing an advanced multi-level resilience strategy that transparently optimizes the performance and scalability of checkpointing by leveraging heterogeneous storage."
"This paper proposes a method to relax the conditional independence assumption of connectionist temporal classification (CTC)-based automatic speech recognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC losses in intermediate layers in addition to the original CTC loss in the last layer. During both training and inference, each generated prediction in the intermediate layers is summed to the input of the next layer to condition the prediction of the last layer on those intermediate predictions. Our method is easy to implement and retains the merits of CTC-based ASR: a simple model architecture and fast decoding speed. We conduct experiments on three different ASR corpora. Our proposed method improves a standard CTC model significantly (e.g., more than 20 % relative word error rate reduction on the WSJ corpus) with a little computational overhead. Moreover, for the TEDLIUM2 corpus and the AISHELL-1 corpus, it achieves a comparable performance to a strong autoregressive model with beam search, but the decoding speed is at least 30 times faster."
"We compute the order-$\as$ corrections to the total cross section and to jet rates for the process $\epem\to\epem +$ hadrons, where the hadrons are produced through crossed-channel quark exchange in the hard scattering of two off-shell photons originating from the incoming leptons. We use a next-to-leading order general-purpose partonic Monte Carlo event generator that allows the computation of a rate differential in the produced leptons and hadrons. We compare our results with the available experimental data for $\epem\to\epem +$ hadrons at LEP2."
"In this study, we build upon a previously proposed neuroevolution framework to evolve deep convolutional models. Specifically, the genome encoding and the crossover operator are extended to make them applicable to layered networks. We also propose a convolutional layer layout which allows kernels of different shapes and sizes to coexist within the same layer, and present an argument as to why this may be beneficial. The proposed layout enables the size and shape of individual kernels within a convolutional layer to be evolved with a corresponding new mutation operator. The proposed framework employs a hybrid optimisation strategy involving structural changes through epigenetic evolution and weight update through backpropagation in a population-based setting. Experiments on several image classification benchmarks demonstrate that the crossover operator is sufficiently robust to produce increasingly performant offspring even when the parents are trained on only a small random subset of the training dataset in each epoch, thus providing direct confirmation that learned features and behaviour can be successfully transferred from parent networks to offspring in the next generation."
"In recent years, with the advancement of computer-aided diagnosis (CAD) technology and whole slide image (WSI), histopathological WSI has gradually played a crucial aspect in the diagnosis and analysis of diseases. To increase the objectivity and accuracy of pathologists' work, artificial neural network (ANN) methods have been generally needed in the segmentation, classification, and detection of histopathological WSI. In this paper, WSI analysis methods based on ANN are reviewed. Firstly, the development status of WSI and ANN methods is introduced. Secondly, we summarize the common ANN methods. Next, we discuss publicly available WSI datasets and evaluation metrics. These ANN architectures for WSI processing are divided into classical neural networks and deep neural networks (DNNs) and then analyzed. Finally, the application prospect of the analytical method in this field is discussed. The important potential method is Visual Transformers."
"We introduce a unified framework for the study of multilevel mixed integer linear optimization problems and multistage stochastic mixed integer linear optimization problems with recourse. The framework highlights the common mathematical structure of the two problems and allows for the development of a common algorithmic framework. Focusing on the two-stage case, we investigate, in particular, the nature of the value function of the second-stage problem, highlighting its connection to dual functions and the theory of duality for mixed integer linear optimization problems, and summarize different reformulations. We then present two main solution techniques, one based on a Benders-like decomposition to approximate either the risk function or the value function, and the other one based on cutting plane generation."
"In a complex community, species continuously adapt to each other. On rare occasions, the adaptation of a species can lead to the extinction of others, and even its own. ""Adaptive dynamics"" is the standard mathematical framework to describe evolutionary changes in community interactions, and in particular, predict adaptation driven extinction. Unfortunately, most authors implement the equations of adaptive dynamics through computer simulations, that require assuming a large number of questionable parameters and fitness functions. In this study we present analytical solutions to adaptive dynamics equations, thereby clarifying how outcomes depend on any computational input. We develop general formulas that predict equilibrium abundances over evolutionary time scales. Additionally, we predict which species will go extinct next, and when this will happen."
"Massive upgrades to science infrastructure are driving data velocities upwards while stimulating adoption of increasingly data-intensive analytics. While next-generation exascale supercomputers promise strong support for I/O-intensive workflows, HPC remains largely untapped by live experiments, because data transfers and disparate batch-queueing policies are prohibitive when faced with scarce instrument time. To bridge this divide, we introduce Balsam: a distributed orchestration platform enabling workflows at the edge to securely and efficiently trigger analytics tasks across a user-managed federation of HPC execution sites. We describe the architecture of the Balsam service, which provides a workflow management API, and distributed sites that provision resources and schedule scalable, fault-tolerant execution. We demonstrate Balsam in efficiently scaling real-time analytics from two DOE light sources simultaneously onto three supercomputers (Theta, Summit, and Cori), while maintaining low overheads for on-demand computing, and providing a Python library for seamless integration with existing ecosystems of data analysis tools."
"Topological phases of matter have revolutionized the fundamental understanding of band theory and hold great promise for next-generation technologies such as low-power electronics or quantum computers. Single-gap topologies have been extensively explored, and a large number of materials have been theoretically proposed and experimentally observed. These ideas have recently been extended to multi-gap topologies, characterized by invariants that arise by the momentum space braiding of band nodes that carry non-Abelian charges. However, the constraints placed by the Fermi-Dirac distribution to electronic systems have so far prevented the experimental observation of multi-gap topologies in real materials. Here, we show that multi-gap topologies and the accompanying phase transitions driven by braiding processes can be readily observed in the bosonic phonon spectra of known monolayer silicates. The associated braiding process can be controlled by means of an electric field and epitaxial strain, and involves, for the first time, more than three bands. Finally, we propose that these conversion processes can be tracked by following the evolution of the Raman spectrum, providing a clear signature for the experimental verification of multi-gap topologies."
"An accurate force field is the key to the success of all molecular mechanics simulations on organic polymers and biomolecules. Accuracy beyond density functional theory is often needed to describe the intermolecular interactions, while most correlated wavefunction (CW) methods are prohibitively expensive for large molecules. Therefore, it posts a great challenge to develop an extendible ab initio force field for large flexible organic molecules at CW level of accuracy. In this work, we face this challenge by combining the physics-driven nonbonding potential with a data-driven subgraph neural network bonding model (named sGNN). Tests on polyethylene glycol polymer chains show that our strategy is highly accurate and robust for molecules of different sizes. Therefore, we can develop the force field from small molecular fragments (with sizes easily accessible to CW methods) and safely transfer it to large polymers, thus opening a new path to the next-generation organic force fields."
"In this article, we develop a formula for an inverse Riemann zeta function such that for $w=\zeta(s)$ we have $s=\zeta^{-1}(w)$ for real and complex domains $s$ and $w$. The presented work is based on extending the analytical recurrence formulas for trivial and non-trivial zeros to solve an equation $\zeta(s)-w=0$ for a given $w$-domain using logarithmic differentiation and zeta recursive root extraction methods. We further explore formulas for trivial and non-trivial zeros of the Riemann zeta function in greater detail, and next, we introduce an expansion of the inverse zeta function by its singularities, and study its properties and develop many identities that emerge from them. In the last part, we extend the presented results as a general method for finding zeros and inverses of many other functions, such as the gamma function, the Bessel function of the first kind, or finite/infinite degree polynomials and rational functions, etc. We further compute all the presented formulas numerically to high precision and show that these formulas do indeed converge to the inverse of the Riemann zeta function and the related results. We also develop a fast algorithm to compute $\zeta^{-1}(w)$ for complex $w$."
"We propose {\rm \texttt{ResIST}}, a novel distributed training protocol for Residual Networks (ResNets). {\rm \texttt{ResIST}} randomly decomposes a global ResNet into several shallow sub-ResNets that are trained independently in a distributed manner for several local iterations, before having their updates synchronized and aggregated into the global model. In the next round, new sub-ResNets are randomly generated and the process repeats. By construction, per iteration, {\rm \texttt{ResIST}} communicates only a small portion of network parameters to each machine and never uses the full model during training. Thus, {\rm \texttt{ResIST}} reduces the communication, memory, and time requirements of ResNet training to only a fraction of the requirements of previous methods. In comparison to common protocols like data-parallel training and data-parallel training with local SGD, {\rm \texttt{ResIST}} yields a decrease in wall-clock training time, while being competitive with respect to model performance."
"We design and develop a new Particle-in-Cell (PIC) method for plasma simulations using Deep-Learning (DL) to calculate the electric field from the electron phase space. We train a Multilayer Perceptron (MLP) and a Convolutional Neural Network (CNN) to solve the two-stream instability test. We verify that the DL-based MLP PIC method produces the correct results using the two-stream instability: the DL-based PIC provides the expected growth rate of the two-stream instability. The DL-based PIC does not conserve the total energy and momentum. However, the DL-based PIC method is stable against the cold-beam instability, affecting traditional PIC methods. This work shows that integrating DL technologies into traditional computational methods is a viable approach for developing next-generation PIC algorithms."
"We propose a method for matching the next-to-leading order (NLO) calculation of a given QCD process with a parton shower Monte Carlo (MC) simulation. The method has the following features: fully exclusive events are generated, with hadronization according to the MC model; total exclusive rates are accurate to NLO; NLO results for distributions are recovered upon expansion in $\alpha_S$; hard emissions are treated as in NLO computations while soft/collinear emissions are handled by the MC simulation, with the same logarithmic accuracy as the MC; and matching between the hard- and soft/collinear-emission regions is smooth. A fraction of events with negative weight is generated, but unweighting remains possible with reasonable efficiency. The method is clarified using a simple toy model, and illustrated by application to the hadroproduction of W$^+$W$^-$ pairs."
I describe how to calculate cross sections for hard-scattering processes in high energy collisions at next to leading order in QCD. I consider infrared-safe quantities and I assume that the scattering amplitudes are known in analytic form up to next-to-leading order. The main topic is the description of the algorithm for the analytic cancellation of the soft and collinear singularities in the loop and bremsstrahlung contributions. The method is systematic and general. It allows the construction of an analytic expression for finite next-to-leading order hard scattering cross sections suitable for numerical evaluation.
We report the results of a next-to-leading order event generator of purely gluonic jet production. This calculation is the first step in the construction of a full next-to-leading order calculation of three jet production at hadron colliders. Several jet-algorithms commonly used in experiments are implemented and their numerical stability is investigated.
I present results from a next-to-leading order event generator of purely gluonic jet production. This calculation is the first step in the construction of a full next-to-leading order calculation of three jet production at hadron colliders. Several jet algorithms commonly used in experiments are implemented and their numerical stability is investigated. A numerical instability is found in the iterative cone algorithm which makes it inappropriate for use in fixed order calculations beyond leading order.
We study the effects of gluon interference in the production and semi-leptonic decay of a t-tbar pair above threshold at the Next Linear Collider (NLC). We calculate all matrix elements to next-to-leading order and use the resulting expressions for the development of a Monte Carlo event generator. Our results show effects at the level of 10% in differential cross-sections. We thus extend previous results obtained by analytical means in the soft-gluon limit.
"I review the status of fixed order jet Monte Carlos and briefly discuss the prospects for next-to-next-to-leading order calculations. I present a general purpose next-to-leading order Monte Carlo program for four jet event shape observables in electron-positron annihilation and present some estimates of the light jet mass and narrow jet broadening distributions. Finally, I discuss an estimate of the strong coupling constant using the measured energy evolution of the average value of event shape variables such as thrust and heavy jet mass."
"We present a new QCD event generator for hadron collider which can calculate one-, two- and three-jet cross sections at next-to-leading order accuracy. In this letter we study the transverse energy spectrum of three-jet hadronic events using the kT algorithm. We show that the next-to-leading order correction significantly reduces the renormalization and factorization scale dependence of the three-jet cross section."
"We present the results of a next-to-leading order analysis of single top production including the decay of the top quark. Radiative effects are included both in the production and decay stages, using a general subtraction method. This calculation gives a good treatment of the jet activity associated with single top production. We perform an analysis of the single top search at the Tevatron, including a consideration of the main backgrounds, many of which are also calculated at next-to-leading order."
"We study resummation of transverse-momentum-related large logarithms generated from soft-gluon radiations in soft-collinear effective field theory. The anomalous dimensions of the effective quark and gluon currents, an important ingredient for the resummation, are calculated to two-loop order. the result at next-to-leading-log reproduces that obtained using the standard method for deep-inelastic scattering, Drell-Yan process, and Higgs production through gluon-gluon fusion. We comment on the extension of the calculation to next-to-next-to-leading logarithms."
"We demonstrate that a tight-binding Hamiltonian with nearest- and next-nearest-neighbor hopping integrals can be decomposed into bulk and boundary parts in a general lattice system. The Hamiltonian decomposition reveals that next nearest-neighbor hopping causes sizable changes in the energy spectrum of surface states even if the correction to the energy spectrum of bulk states is negligible. By applying the Hamiltonian decomposition to edge states in graphene systems, we show that the next nearest-neighbor hopping stabilizes the edge states."
"We take into account the dynamics of a complete third post-Newtonian conservative Hamiltonian of two spinning black holes, where the orbital part arrives at the third post-Newtonian precision level and the spin-spin part with the spin-orbit part includes the leading-order and next-to-leading-order contributions. It is shown through numerical simulations that the next-to-leading order spin-spin couplings play an important role in chaos. A dynamical sensitivity to the variation of single parameter is also investigated. In particular, there are a number of \textit{observable} orbits whose initial radii are large enough and which become chaotic before coalescence."
"We consider the QCD corrections to the electric dipole moment of the neutron in the Minimal Supersymmetric Standard Model. We provide a master formula for the Wilson coefficients at the low energy scale including for the first time the mixing between the electric and chromoelectric operators and correcting widely used previous LO estimates. We show that, because of the mixing between the electric and chromoelectric operators, the neutralino contribution is always strongly suppressed. We find that, in general, the effect of the QCD corrections is to reduce the amount of CP violation generated at the high scale. We discuss the perturbative uncertainties of the LO computation, which are particularly large for the gluino-mediated contribution. This motivates our Next-to-Leading order analysis. We compute for the first time the order alpha_s corrections to the Wilson coefficients for the gluino contributions, and recompute the two-loop anomalous dimension for the dipole operators. We show that the large LO uncertainty disappears once NLO corrections are taken into account."
"We examine properties of small-radius jets, focusing on phenomenological applications to the inclusive jet spectrum. We match fixed-order calculations with the leading-logarithmic resummation of the jet radius (LL$_R$), and propose a new prescription to evaluate theoretical uncertainties for next-to-leading order (NLO) predictions. We also examine the $R$-dependent next-to-next-to-leading order (NNLO) corrections, and include them in our calculation. We discuss hadronisation corrections, which are derived from Monte Carlo generators. Finally, we assemble these elements and compare the ratio of the inclusive jet spectra at two $R$ values obtained from our matched (N)NLO+LL$_R$ predictions to data from ALICE and ATLAS, finding improved agreement."
"Planning high-energy collision experiments for the next few decades requires extensive Monte Carlo simulations in order to accomplish physics goals of these experiments. Such simulations are essential for understanding fundamental physics processes, as well as for setting up the detector parameters that help establish R&D projects required over the next few decades. This paper describes a public repository with Monte Carlo event samples before and after detector-response simulation. The goal of this repository is to facilitate the accomplishment of many goals in planning a next generation of particle experiments."
"We derive the chiral Lagrangian at next-to-next-to-next-to-leading order (NNNLO) for a general number $N_f$ of light quark flavours as well as for $N_f=2,3$. We enumerate the contact terms separately. We also discuss the cases where some of the external fields are not included. An example of a choice of Lagrangian is given in the supplementary material."
"We have developed a framework for automated transverse momentum resummation for arbitrary electroweak final states based on reweighting tree-level events. It is fully differential in the kinematics of the electroweak final states, which facilitates a straightforward analysis of arbitrary observables in the small transverse momentum region. We have implemented the resummation at next-to-next-to-leading logarithmic accuracy and match to next-to-leading fixed-order results using the event generator MadGraph5_aMC@NLO. Results for $Z$ and $W$ boson production with leptonic decay as well as $WZ$ production are presented. We compare to experimental measurements for the transverse momentum and the angular observable $\phi^*$."
"We developed new parameterizations of local regularized finite-range pseudopotentials up to next-to-next-to-next-to-leading order (N3LO), used as generators of nuclear density functionals. When supplemented with zero-range spin-orbit and density-dependent terms, they provide a correct single-reference description of binding energies and radii of spherical and deformed nuclei. We compared the obtained results to experimental data and discussed benchmarks against the standard well-established Gogny D1S functional."
"We present a calculation of the fully differential cross section for Higgs boson production in the gluon fusion channel through next-to-next-to-leading order in perturbative QCD. We apply the method introduced in \cite{Anastasiou:2003gr} to compute double real emission corrections. Our calculation permits arbitrary cuts on the final state in the reaction $hh \to H + X$. It can be easily extended to include decays of the Higgs boson into observable final states. In this Letter, we discuss the most important features of the calculation, and present some examples of physical applications that illustrate the range of observables that can be studied using our result. We compute the NNLO rapidity distribution of the Higgs boson, and also calculate the NNLO rapidity distribution with a veto on jet activity."
"We describe a calculation of the fully differential cross section for Higgs boson production in the gluon fusion channel through next-to-next-to-leading order (NNLO) in perturbative QCD. The decay of the Higgs boson into two photons is included. Technical aspects of the computation are discussed in detail. The implementation of the calculation into a numerical code, called FEHiP, is described. The NNLO K-factors for completely realistic photon acceptances and isolation cuts, including those employed by the ATLAS and CMS collaborations, are computed. We study various distributions of the photons from Higgs decay through NNLO."
"We introduce an improved version of the simulation code FEWZ (Fully Exclusive W and Z Production) for hadron collider production of lepton pairs through the Drell-Yan process at next-to-next-to-leading-order (NNLO) in the strong coupling constant. The program is fully differential in the phase space of leptons and additional hadronic radiation. The new version offers users significantly more options for customization. FEWZ now bins multiple, user-selectable histograms during a single run, and produces parton distribution function (PDF) errors automatically. It also features a signifcantly improved integration routine, and can take advantage of multiple processor cores locally or on the Condor distributed computing system. We illustrate the new features of FEWZ by presenting numerous phenomenological results for LHC physics. We compare NNLO QCD with initial ATLAS and CMS results, and discuss in detail the effects of detector acceptance on the measurement of angular quantities associated with Z-boson production. We address the issue of technical precision in the presence of severe phase-space cuts."
"Jet substructure has emerged as a critical tool for LHC searches, but studies so far have relied heavily on shower Monte Carlo simulations, which formally approximate QCD at leading-log level. We demonstrate that systematic higher-order QCD computations of jet substructure can be carried out by boosting global event shapes by a large momentum Q, and accounting for effects due to finite jet size, initial-state radiation (ISR), and the underlying event (UE) as 1/Q corrections. In particular, we compute the 2-subjettiness substructure distribution for boosted Z -> q qbar events at the LHC at next-to-next-to-next-to-leading-log order. The calculation is greatly simplified by recycling the known results for the thrust distribution in e+ e- collisions. The 2-subjettiness distribution quickly saturates, becoming Q independent for Q > 400 GeV. Crucially, the effects of jet contamination from ISR/UE can be subtracted out analytically at large Q, without knowing their detailed form. Amusingly, the Q=infinity and Q=0 distributions are related by a scaling by e, up to next-to-leading-log order."
"We compute the Next-to-Next-to-Leading Order (NNLO) QCD corrections to the partonic reaction that dominates top-pair production at the Tevatron. This is the first ever NNLO calculation of an observable with more than two colored partons, and/or massive fermions, at hadron colliders. Augmenting our fixed order calculation with soft-gluon resummation through Next-to-Next-to-Leading Logarithmic (NNLL) accuracy, we observe that the predicted total inclusive cross-section exhibits a very small perturbative uncertainty, estimated at +-2.7%. We expect that once all sub-dominant partonic reactions are accounted for, and work in this direction is ongoing, the perturbative theoretical uncertainty for this observable could drop below +-2%. Our calculation demonstrates the power of our computational approach and proves it can be successfully applied to all processes at hadron colliders for which high-precision analyses are needed."
"In this article we are concerned with how to compute the cohomology ring of a symplectic quotient by a circle action using the information we have about the cohomology of the original manifold and some data at the fixed point set of the action. Our method is based on the Tolman-Weitsman theorem which gives a characterization of the kernel of the Kirwan map. First we compute a generating set for the kernel of the Kirwan map for the case of product of compact connected manifolds such that the cohomology ring of each of them is generated by a degree two class. We assume the fixed point set is isolated; however the circle action only needs to be ``formally Hamiltonian''. By identifying the kernel, we obtain the cohomology ring of the symplectic quotient. Next we apply this result to some special cases and in particular to the case of products of two dimensional spheres. We show that the results of Kalkman and Hausmann-Knutson are special cases of our result."
"We consider direct diphoton production in hadron collisions. We compute the next-to-next-to-leading order (NNLO) QCD radiative corrections at the fully-differential level. Our calculation uses the qT subtraction formalism and it is implemented in a parton level Monte Carlo program, which allows the user to apply arbitrary kinematical cuts on the final-state photons and the associated jet activity, and to compute the corresponding distributions in the form of bin histograms. We present selected numerical results related to Higgs boson searches at the LHC and the Tevatron, and we show how the NNLO corrections to diphoton production are relevant to understand the main background of the decay channel H -> \gamma \gamma of the Higgs boson H."
"We compute the next-to-next-to-leading order (NNLO) QCD correction to the total cross-section for the reaction gg \to t\bar t + X. Together with the partonic channels we computed previously, the result derived in this letter completes the set of NNLO QCD corrections to the total top pair production cross-section at hadron colliders. Supplementing the fixed order results with soft-gluon resummation with next-to-next-to-leading logarithmic accuracy we estimate that the theoretical uncertainty of this observable due to unknown higher order corrections is about 3% at the LHC and 2.2% at the Tevatron. We observe a good agreement between the Standard Model predictions and the available experimental measurements. The very high theoretical precision of this observable allows a new level of scrutiny in parton distribution functions and new physics searches."
"We extend the recent computation of Higgs boson production in association with a jet through next-to-next-to-leading order in perturbative QCD by including decays of the Higgs boson to electroweak vector bosons. This allows us to compute fiducial cross sections and kinematic distributions including realistic selection criteria for the Higgs boson decay products. As an illustration, we present results for $pp \to H + j \to \gamma \gamma + j$ closely following the ATLAS 8 TeV analysis and for $pp \to H+ j \to W W + j \to e^+ \mu^- \nu \bar \nu + j $ in a CMS-like 13 TeV setup."
"We compute the ${\cal O}(\alpha^2\alpha_s^2)$ perturbative corrections to inclusive jet production in electron-nucleon collisions. This process is of particular interest to the physics program of a future Electron Ion Collider (EIC). We include all relevant partonic processes, including deep-inelastic scattering contributions, photon-initiated corrections, and parton-parton scattering terms that first appear at this order. Upon integration over the final-state hadronic phase space we validate our results for the deep-inelastic corrections against the known next-to-next-to-leading order (NNLO) structure functions. Our calculation uses the $N$-jettiness subtraction scheme for performing higher-order computations, and allows for a completely differential description of the deep-inelastic scattering process. We describe the application of this method to inclusive jet production in detail, and present phenomenological results for the proposed EIC. The NNLO corrections have a non-trivial dependence on the jet kinematics and arise from an intricate interplay between all contributing partonic channels."
"The problem of finding a next best viewpoint for 3D modeling or scene mapping has been explored in computer vision over the last decade. This paper tackles a similar problem, but with different characteristics. It proposes a method for dynamic next best viewpoint recovery of a target point while avoiding possible occlusions. Since the environment can change, the method has to iteratively find the next best view with a global understanding of the free and occupied parts.   We model the problem as a set of possible viewpoints which correspond to the centers of the facets of a virtual tessellated hemisphere covering the scene. Taking into account occlusions, distances between current and future viewpoints, quality of the viewpoint and joint constraints (robot arm joint distances or limits), we evaluate the next best viewpoint. The proposal has been evaluated on 8 different scenarios with different occlusions and a short 3D video sequence to validate its dynamic performance."
"We report on a new calculation of the next-to-next-to-leading order (NNLO) QCD radiative corrections to the inclusive production of top-quark pairs at hadron colliders. The calculation is performed by using the $q_T$ subtraction formalism to handle and cancel infrared singular contributions at intermediate stages of the computation. We present numerical results for the total cross section in $pp$ collisions at $\sqrt{s}=8$ TeV and $13$ TeV, and we compare them with those obtained by using the publicly available numerical program Top++. Our computation represents the first complete application of the $q_T$ subtraction formalism to the hadroproduction of a colourful high-mass system at NNLO."
"We develop further an approach to computing energy-energy correlations (EEC) directly from finite correlation functions. In this way, one completely avoids infrared divergences. In maximally supersymmetric Yang-Mills theory ($\mathcal{N}=4$ sYM), we derive a new, extremely simple formula relating the EEC to a triple discontinuity of a four-point correlation function. We use this formula to compute the EEC in $\mathcal{N}=4$ sYM at next-to-next-to-leading order in perturbation theory. Our result is given by a two-fold integral representation that is straightforwardly evaluated numerically. We find that some of the integration kernels are equivalent to those appearing in sunrise Feynman integrals, which evaluate to elliptic functions. Finally, we use the new formula to provide the expansion of the EEC in the back-to-back and collinear limits."
"The calculations for the production of heavy quarks in deeply inelastic scattering (DIS) can be seen as an illustrative example in the framework of perturbative Quantum Chromo Dynamics (pQCD). In this thesis, I give an overview of all steps that are required for this computation. First, I give an introduction to the fundamental theoretical basics. Next, I present the calculation of the matrix elements which are a main part of the work. The next-to-leading order (NLO) pQCD corrections for polarized DIS have been obtained here for the first time. Then, I discuss two different phase space decompositions which provide another important ingredient to the performed study. Practical results of the computations are shown first on a partonic level and, next, I present some phenomenological applications on a hadronic level. These results shed a first light onto the physical opportunities at a future Electron-Ion Collider (EIC)."
"We assess how well next generation neutrinoless double beta decay and normal neutrino beta decay experiments can answer four fundamental questions. 1) If neutrinoless double beta decay searches do not detect a signal, and if the spectrum is known to be inverted hierarchy, can we conclude that neutrinos are Dirac particles? 2) If neutrinoless double beta decay searches are negative and a next generation ordinary beta decay experiment detects the neutrino mass scale, can we conclude that neutrinos are Dirac particles? 3) If neutrinoless double beta decay is observed with a large neutrino mass element, what is the total mass in neutrinos? 4) If neutrinoless double beta decay is observed but next generation beta decay searches for a neutrino mass only set a mass upper limit, can we establish whether the mass hierarchy is normal or inverted? We base our answers on the expected performance of next generation neutrinoless double beta decay experiments and on simulations of the accuracy of calculations of nuclear matrix elements."
"In this paper, we proved P(n,3), which is an important part of the DDVV conjecture. The general case will be treated in the next version of the paper."
"The immune system provides an ideal metaphor for anomaly detection in general and computer security in particular. Based on this idea, artificial immune systems have been used for a number of years for intrusion detection, unfortunately so far with little success. However, these previous systems were largely based on immunological theory from the 1970s and 1980s and over the last decade our understanding of immunological processes has vastly improved. In this paper we present two new immune inspired algorithms based on the latest immunological discoveries, such as the behaviour of Dendritic Cells. The resultant algorithms are applied to real world intrusion problems and show encouraging results. Overall, we believe there is a bright future for these next generation artificial immune algorithms."
We introduce concept of next generation shape invariance and show that the process of shape invariant extension can be continued indefinitely.
"Wireless Sensor Networks (WSNs) have gained worldwide attention in recent years, particularly with the proliferation in Micro-Electro-Mechanical Systems (MEMS) technology which has facilitated the development of smart sensors. The paper discusses about classification of WSN and challenges of the Next Generation WSN. One of the major challenges of Next Generation WSN is reduction of power consumption. The two approaches are discussed: Ultra-Low-Power Networks and Energy Harvesting. The paper also discusses about some major applications as designing low cost secured Intelligent Buildings, In-Home Health care and Agriculture."
"For a topological dynamical system $(X, T)$ we define a uniform generator as a finite measurable partition such that the symmetric cylinder sets in the generated process shrink in diameter uniformly to zero. The problem of existence and optimal cardinality of uniform generators has lead us to new challenges in the theory of symbolic extensions. At the beginning we show that uniform generators can be identified with so-called symbolic extensions with an embedding, i.e., symbolic extensions admitting an equivariant measurable selector from preimages. From here we focus on such extensions and we strive to characterize the collection of the corresponding extension entropy functions on invariant measures. For aperiodic zero-dimensional systems we show that this collection coincides with that of extension entropy functions in arbitrary symbolic extensions, which, by the general theory of symbolic extensions, is known to coincide with the collection of all affine superenvelopes of the entropy structure of the system. In particular, we recover, after [Bu16], that an aperiodic zero-dimensional system is asymptotically h- expansive if and only if it admits an isomorphic symbolic extension. Next we pass to systems with periodic points, and we introduce the notion of a period tail structure, which captures the local growth rate of periodic orbits. Finally, we succeed in precisely identifying the wanted collection of extension entropy functions in symbolic extensions with an embedding: these are all the affine superenvelopes of the usual entropy structure which lie above certain threshold function determined by the period tail structure. This characterization allows us, among other things, to give estimates (and in examples to compute precisely) the optimal cardinality of a uniform generator."
"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data."
"Under special circumstances, summaries should conform to a particular style with patterns, such as court judgments and abstracts in academic papers. To this end, the prototype document-summary pairs can be utilized to generate better summaries. There are two main challenges in this task: (1) the model needs to incorporate learned patterns from the prototype, but (2) should avoid copying contents other than the patternized words---such as irrelevant facts---into the generated summaries. To tackle these challenges, we design a model named Prototype Editing based Summary Generator (PESG). PESG first learns summary patterns and prototype facts by analyzing the correlation between a prototype document and its summary. Prototype facts are then utilized to help extract facts from the input document. Next, an editing generator generates new summary based on the summary pattern or extracted facts. Finally, to address the second challenge, a fact checker is used to estimate mutual information between the input document and generated summary, providing an additional signal for the generator. Extensive experiments conducted on a large-scale real-world text summarization dataset show that PESG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations."
"Generative seq2seq dialogue systems are trained to predict the next word in dialogues that have already occurred. They can learn from large unlabeled conversation datasets, build a deep understanding of conversational context, and generate a wide variety of responses. This flexibility comes at the cost of control. Undesirable responses in the training data will be reproduced by the model at inference time, and longer generations often don't make sense. Instead of generating responses one word at a time, we train a classifier to choose from a predefined list of full responses. The classifier is trained on (conversation context, response class) pairs, where each response class is a noisily labeled group of interchangeable responses. At inference, we generate the exemplar response associated with the predicted response class. Experts can edit and improve these exemplar responses over time without retraining the classifier or invalidating old training data. Human evaluation of 775 unseen doctor/patient conversations shows that this tradeoff improves responses. Only 12% of our discriminative approach's responses are worse than the doctor's response in the same conversational context, compared to 18% for the generative model. A discriminative model trained without any manual labeling of response classes achieves equal performance to the generative model."
"Machine reading comprehension methods that generate answers by referring to multiple passages for a question have gained much attention in AI and NLP communities. The current methods, however, do not investigate the relationships among multiple passages in the answer generation process, even though topics correlated among the passages may be answer candidates. Our method, called neural answer Generation through Unified Memories over Multiple Passages (GUM-MP), solves this problem as follows. First, it determines which tokens in the passages are matched to the question. In particular, it investigates matches between tokens in positive passages, which are assigned to the question, and those in negative passages, which are not related to the question. Next, it determines which tokens in the passage are matched to other passages assigned to the same question and at the same time it investigates the topics in which they are matched. Finally, it encodes the token sequences with the above two matching results into unified memories in the passage encoders and learns the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism. As a result, GUM-MP can generate answers by pointing to important tokens present across passages. Evaluations indicate that GUM-MP generates much more accurate results than the current models do."
"Reproduction numbers, like the basic reproduction number $\mathcal{R}_0$, play an important role in the analysis and application of dynamic models, including contagion models and ecological population models. One difficulty in deriving these quantities is that they must be computed on a model-by-model basis, since it is typically impractical to obtain general reproduction number expressions applicable to a family of related models, especially if these are of different dimensions. For example, this is typically the case for SIR-type infectious disease models derived using the linear chain trick (LCT). Here we show how to find general reproduction number expressions for such models families (which vary in their number of state variables) using the next generation operator approach in conjunction with the generalized linear chain trick (GLCT). We further show how the GLCT enables modelers to draw insights from these results by leveraging theory and intuition from continuous time Markov chains (CTMCs) and their absorption time distributions (i.e., phase-type probability distributions). To do this, we first review the GLCT and other connections between mean-field ODE model assumptions, CTMCs, and phase-type distributions. We then apply this technique to find reproduction numbers for two sets of models: a family of generalized SEIRS models of arbitrary finite dimension, and a generalized family of finite dimensional predator-prey (Rosenzweig-MacArthur type) models. These results highlight the utility of the GLCT for the derivation and analysis of mean field ODE models, especially when used in conjunction with theory from CTMCs and their associated phase-type distributions."
"We apply the derivative expansion of the effective action in the exact renormalization group equation up to fourth order to the $Z_2$ and $O(N)$ symmetric scalar models in $d=3$ Euclidean dimensions. We compute the critical exponents $\nu$, $\eta$ and $\omega$ using polynomial expansion in the field. We obtain our predictions for the exponents employing two regulators widely used in ERG computations. We apply Wynn's epsilon algorithm to improve the predictions for the critical exponents, extrapolating beyond the next-to-next-to-leading order prediction of the derivative expansion."
"We present an analytical description of top-antitop pair production near the threshold in $e^+e^-$ annihilation and $\g\g$ collisions. A set of basic observables considered includes the total cross sections, forward-backward asymmetry and top quark polarization. The threshold effects relevant for the basic observables are described by three universal functions related to S wave production, P wave production and S-P interference. These functions are computed analytically up to the next-to-next-to-leading order of NRQCD. The total $e^+e^-\to t\bar t$ cross section near the threshold is obtained in the next-to-next-to-leading order in the closed form including the contribution due to the axial coupling of top quark and mediated by the Z-boson. The effects of the running of the strong coupling constant and of the finite top quark width are taken into account analytically for the P wave production and S-P wave interference."
"In this paper, we use semi-definite programming and generalized principal component analysis (GPCA) to distinguish between two or more different facial expressions. In the first step, semi-definite programming is used to reduce the dimension of the image data and ""unfold"" the manifold which the data points (corresponding to facial expressions) reside on. Next, GPCA is used to fit a series of subspaces to the data points and associate each data point with a subspace. Data points that belong to the same subspace are claimed to belong to the same facial expression category. An example is provided."
"A measurement of the production cross section for two isolated photons in proton-proton collisions at a center-of-mass energy of $\sqrt{s}=8$ TeV is presented. The results are based on an integrated luminosity of 20.2 fb$^{-1}$ recorded by the ATLAS detector at the Large Hadron Collider. The measurement considers photons with pseudorapidities satisfying $|\eta^{\gamma}|<1.37$ or ${1.56<|\eta^{\gamma}|<2.37}$ and transverse energies of respectively $E_{\mathrm{T,1}}^{\gamma}>40$ GeV and $E_{\mathrm{T,2}}^{\gamma}>30$ GeV for the two leading photons ordered in transverse energy produced in the interaction.The background due to hadronic jets and electrons is subtracted using data-driven techniques. The fiducial cross sections are corrected for detector effects and measured differentially as a function of six kinematic observables. The measured cross section integrated within the fiducial volume is $16.8 \pm 0.8$ pb. The data are compared to fixed-order QCD calculations at next-to-leading-order and next-to-next-to-leading-order accuracy as well as next-to-leading-order computations including resummation of initial-state gluon radiation at next-to-next-to-leading logarithm or matched to a parton shower, with relative uncertainties varying from 5% to 20%."
"We perform the all orders resummation of threshold enhanced contributions for the Higgs boson pair production cross section via gluon fusion, including finite top quark mass ($M_t$) effects. We present results for the total cross section and Higgs pair invariant mass ($M_{hh}$) distribution. We obtain results at next-to-leading logarithmic accuracy (NLL) which retain the full $M_t$ dependence, and are matched to the full next-to-leading order (NLO) prediction. Our NLL+NLO results represent the most advanced prediction with full $M_t$ dependence for this process, and produce an increase of about 4% in the total cross section with respect to the NLO result for LHC energies, and for a central scale $\mu_0 = M_{hh}/2$. We also consistently combine the full NLL with the next-to-next-to-leading logarithmically (NNLL) accurate resummation computed in the Born-improved large-$M_t$ limit, and match it to the next-to-next-to-leading order approximation of Ref. \cite{Grazzini:2018bsd}, so called NNLO_FTa. We find that the resummation effects are very small at NNLL for $\mu_0 = M_{hh}/2$, in particular below 1% at 13 TeV, indicating that the perturbative expansion is under control. In all cases the resummation effects are found to be substantially larger for the central scale $\mu_0 = M_{hh}$, resulting in a more stable cross section with respect to scale variations than the fixed order calculation."
"Given the ever increasing bandwidth of the visual information available to many intelligent systems, it is becoming essential to endow them with a sense of what is worthwhile their attention and what can be safely disregarded. This article presents a general mathematical framework to efficiently allocate the available computational resources to process the parts of the input that are relevant to solve a given perceptual problem. By this we mean to find the hypothesis H (i.e., the state of the world) that maximizes a function L(H), representing how well each hypothesis ""explains"" the input. Given the large bandwidth of the sensory input, fully evaluating L(H) for each hypothesis H is computationally infeasible (e.g., because it would imply checking a large number of pixels). To address this problem we propose a mathematical framework with two key ingredients. The first one is a Bounding Mechanism (BM) to compute lower and upper bounds of L(H), for a given computational budget. These bounds are much cheaper to compute than L(H) itself, can be refined at any time by increasing the budget allocated to a hypothesis, and are frequently enough to discard a hypothesis. To compute these bounds, we develop a novel theory of shapes and shape priors. The second ingredient is a Focus of Attention Mechanism (FoAM) to select which hypothesis' bounds should be refined next, with the goal of discarding non-optimal hypotheses with the least amount of computation. The proposed framework: 1) is very efficient since most hypotheses are discarded with minimal computation; 2) is parallelizable; 3) is guaranteed to find the globally optimal hypothesis; and 4) its running time depends on the problem at hand, not on the bandwidth of the input. We instantiate the proposed framework for the problem of simultaneously estimating the class, pose, and a noiseless version of a 2D shape in a 2D image."
"The success of the abstract model of computation, in terms of bits, logical operations, programming language constructs, and the like, makes it easy to forget that computation is a physical process. Our cherished notions of computation and information are grounded in classical mechanics, but the physics underlying our world is quantum. In the early 80s researchers began to ask how computation would change if we adopted a quantum mechanical, instead of a classical mechanical, view of computation. Slowly, a new picture of computation arose, one that gave rise to a variety of faster algorithms, novel cryptographic mechanisms, and alternative methods of communication. Small quantum information processing devices have been built, and efforts are underway to build larger ones. Even apart from the existence of these devices, the quantum view on information processing has provided significant insight into the nature of computation and information, and a deeper understanding of the physics of our universe and its connections with computation.   We start by describing aspects of quantum mechanics that are at the heart of a quantum view of information processing. We give our own idiosyncratic view of a number of these topics in the hopes of correcting common misconceptions and highlighting aspects that are often overlooked. A number of the phenomena described were initially viewed as oddities of quantum mechanics. It was quantum information processing, first quantum cryptography and then, more dramatically, quantum computing, that turned the tables and showed that these oddities could be put to practical effect. It is these application we describe next. We conclude with a section describing some of the many questions left for future work, especially the mysteries surrounding where the power of quantum information ultimately comes from."
"Research has shown assistance can provide many benefits to novices lacking the mental models needed for problem solving in a new domain. However, varying approaches to assistance, such as subgoals and next-step hints, have been implemented with mixed results. Next-Step hints are common in data-driven tutors due to their straightforward generation from historical student data, as well as research showing positive impacts on student learning. However, there is a lack of research exploring the possibility of extending data-driven methods to provide higher-level assistance. Therefore, we modified our data-driven Next-Step hint generator to provide Waypoints, hints that are a few steps ahead, representing problem-solving subgoals. We hypothesized that Waypoints would benefit students with high prior knowledge, and that Next-Step hints would most benefit students with lower prior knowledge. In this study, we investigated the influence of data-driven hint type, Waypoints versus Next-Step hints, on student learning in a logic proof tutoring system, Deep Thought, in a discrete mathematics course. We found that Next-Step hints were more beneficial for the majority of students in terms of time, efficiency, and accuracy on the posttest. However, higher totals of successfully used Waypoints were correlated with improvements in efficiency and time in the posttest. These results suggest that Waypoint hints could be beneficial, but more scaffolding may be needed to help students follow them."
"SpinQ Gemini is a commercial desktop quantum computer designed and manufactured by SpinQ Technology. It is an integrated hardware-software system. The first generation product with two qubits was launched in January 2020. The hardware is based on NMR spectrometer, with permanent magnets providing $\sim 1$ T magnetic field. SpinQ Gemini operates under room temperature ($0$-$30^{\circ}$C), highlighting its lightweight (55 kg with a volume of $70\times 40 \times 80$ cm$^3$), cost-effective (under $50$k USD), and maintenance-free. SpinQ Gemini aims to provide real-device experience for quantum computing education for K-12 and at the college level. It also features quantum control design capabilities that benefit the researchers studying quantum control and quantum noise. Since its first launch, SpinQ Gemini has been shipped to institutions in Canada, Taiwan and Mainland China. This paper introduces the system of design of SpinQ Gemini, from hardware to software. We also demonstrate examples for performing quantum computing tasks on SpinQ Gemini, including one task for a variational quantum eigensolver of a two-qubit Heisenberg model. The next generations of SpinQ quantum computing devices will adopt models of more qubits, advanced control functions for researchers with comparable cost, as well as simplified models for much lower cost (under $5$k USD) for K-12 education. We believe that low-cost portable quantum computer products will facilitate hands-on experience for teaching quantum computing at all levels, well-prepare younger generations of students and researchers for the future of quantum technologies."
"Innovations in Next-Generation Sequencing are enabling generation of DNA sequence data at ever faster rates and at very low cost. Large sequencing centers typically employ hundreds of such systems. Such high-throughput and low-cost generation of data underscores the need for commensurate acceleration in downstream computational analysis of the sequencing data. A fundamental step in downstream analysis is mapping of the reads to a long reference DNA sequence, such as a reference human genome. Sequence mapping is a compute-intensive step that accounts for more than 30% of the overall time of the GATK workflow. BWA-MEM is one of the most widely used tools for sequence mapping and has tens of thousands of users.   In this work, we focus on accelerating BWA-MEM through an efficient architecture aware implementation, while maintaining identical output. The volume of data requires distributed computing environment, usually deploying multicore processors. Since the application can be easily parallelized for distributed memory systems, we focus on performance improvements on a single socket multicore processor. BWA-MEM run time is dominated by three kernels, collectively responsible for more than 85% of the overall compute time. We improved the performance of these kernels by 1) improving cache reuse, 2) simplifying the algorithms, 3) replacing small fragmented memory allocations with a few large contiguous ones, 4) software prefetching, and 5) SIMD utilization wherever applicable - and massive reorganization of the source code enabling these improvements.   As a result, we achieved nearly 2x, 183x, and 8x speedups on the three kernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end compute time over the original BWA-MEM on single thread and single socket of Intel Xeon Skylake processor. To the best of our knowledge, this is the highest reported speedup over BWA-MEM."
"The notion of generations of monic polynomials such that the coefficients of the polynomials of the next generation coincide with the zeros of the polynomials of the current generation is introduced, and its relevance to the identification of endless sequences of new solvable many-body problems of ""goldfish type"" is demonstrated."
"Photons are natural carriers of quantum information due to their ease of distribution and long lifetime. This thesis concerns various related aspects of quantum information processing with single photons. Firstly, we demonstrate N-photon entanglement generation through a generalised N X N symmetric beam splitter known as the Bell multiport. A wide variety of 4-photon entangled states as well as the N-photon W-state can be generated with an unexpected non-monotonic decreasing probability of success with N. We also show how the same setup can be used to generate multiatom entanglement. A further study of multiports also leads us to a multiparticle generalisation of the Hong-Ou-Mandel dip which holds for all Bell multiports of even number of input ports. Next, we demonstrate a generalised linear optics based photon filter that has a constant success probability regardless of the number of photons involved. This filter has the highest reported success probability and is interferometrically robust. Finally, we demonstrate how repeat-until-success quantum computing can be performed with two distant nodes with unit success probability using only linear optics resource. We further show that using non-identical photon sources, robustness can still be achieved, an illustration of the nature and advantages of measurement-based quantum computation. A direct application to the same setup leads naturally to arbitrary multiphoton state generation on demand. Finally, we demonstrate how polarisation entanglement of photons can be detected from the emission of two atoms in a Young's double-slit type experiment without linear optics, resulting in both atoms being also maximally entangled."
"We study linear series on a general curve of genus $g$, whose images are exceptional with regard to their secant planes. Working in the framework of an extension of Brill-Noether theory to pairs of linear series, we prove that a general curve has no linear series with exceptional secant planes, in a very precise sense, whenever the total number of series is finite. Next, we partially solve the problem of computing the number of linear series with exceptional secant planes in a one-parameter family in terms of tautological classes associated with the family, by evaluating our hypothetical formula along judiciously-chosen test families. As an application, we compute the number of linear series with exceptional secant planes on a general curve equipped with a one-dimensional family of linear series. We pay special attention to the extremal case of $d$-secant $(d-2)$-planes to $(2d-1)$-dimensional series, which appears in the study of Hilbert schemes of points on surfaces. In that case, our formula may be rewritten in terms of hypergeometric series, which allows us both to prove that it is nonzero and to deduce its asymptotics in $d$."
"This thesis contains contributions to the theory of quantum computation.   We first define a new method to efficiently approximate special unitary operators. Specifically, given a special unitary U and a precision {\epsilon} > 0, we show how to efficiently find a sequence of Clifford+V or Clifford+T operators whose product approximates U up to {\epsilon} in the operator norm. In the general case, the length of the approximating sequence is asymptotically optimal. If the unitary to approximate is diagonal then our method is optimal: it yields the shortest sequence approximating U up to {\epsilon}.   Next, we introduce a mathematical formalization of a fragment of the Quipper quantum programming language. We define a typed lambda calculus called Proto-Quipper which formalizes a restricted but expressive fragment of Quipper. The type system of Proto-Quipper is based on intuitionistic linear logic and prohibits the duplication of quantum data, in accordance with the no-cloning property of quantum computation. We prove that Proto-Quipper is type-safe in the sense that it enjoys the subject reduction and progress properties."
"The increase in dissipated power per unit area of electronic components sets higher demands on the performance of the heat sink. Also if we continue at our current rate of miniaturisation, laptops and other electronic devices can get heated up tremendously. Hence we require a better heat dissipating system to overcome the excess heat generating problem of using nanoelectronics, which is expected to power the next generation of computers. To handle the excessive and often unpredictable heating up of high performance electronic components like microprocessors, we need to predict the temperature profile of the heat sink used. This also helps us to select the best heat sink for the operating power range of any microprocessor. Understanding the temperature profile of a heat sink and a microprocessor helps us to handle its temperature efficiently for a range of loads. In this work, a method to estimate the normal response of a heat sink to various loads of a microprocessor is explained."
"Maximum likelihood estimation (MLE) is a well-known estimation method used in many robotic and computer vision applications. Under Gaussian assumption, the MLE converts to a nonlinear least squares (NLS) problem. Efficient solutions to NLS exist and they are based on iteratively solving sparse linear systems until convergence. In general, the existing solutions provide only an estimation of the mean state vector, the resulting covariance being computationally too expensive to recover. Nevertheless, in many simultaneous localisation and mapping (SLAM) applications, knowing only the mean vector is not enough. Data association, obtaining reduced state representations, active decisions and next best view are only a few of the applications that require fast state covariance recovery. Furthermore, computer vision and robotic applications are in general performed online. In this case, the state is updated and recomputed every step and its size is continuously growing, therefore, the estimation process may become highly computationally demanding. This paper introduces a general framework for incremental MLE called SLAM++, which fully benefits from the incremental nature of the online applications, and provides efficient estimation of both the mean and the covariance of the estimate. Based on that, we propose a strategy for maintaining a sparse and scalable state representation for large scale mapping, which uses information theory measures to integrate only informative and non-redundant contributions to the state representation. SLAM++ differs from existing implementations by performing all the matrix operations by blocks. This led to extremely fast matrix manipulation and arithmetic operations. Even though this paper tests SLAM++ efficiency on SLAM problems, its applicability remains general."
"An open-source middleware EigenKernel was developed for use with parallel generalized eigenvalue solvers or large-scale electronic state calculation to attain high scalability and usability. The middleware enables the users to choose the optimal solver, among the three parallel eigenvalue libraries of ScaLAPACK, ELPA, EigenExa and hybrid solvers constructed from them, according to the problem specification and the target architecture. The benchmark was carried out on the Oakforest-PACS supercomputer and reveals that ELPA, EigenExa and their hybrid solvers show better performance, when compared with pure ScaLAPACK solvers. The benchmark on the K computer is also used for discussion. In addition, a preliminary research for the performance prediction was investigated, so as to predict the elapsed time T as the function of the number of used nodes P (T=T(P)). The prediction is based on Bayesian inference using the Markov Chain Monte Carlo (MCMC) method and the test calculation indicates that the method is applicable not only to performance interpolation but also to extrapolation. Such a middleware is of crucial importance for application-algorithm-architecture co-design among the current, next-generation (exascale), and future-generation (post-Moore era) supercomputers."
"Learned image compression has recently shown the potential to outperform the standard codecs. State-of-the-art rate-distortion (R-D) performance has been achieved by context-adaptive entropy coding approaches in which hyperprior and autoregressive models are jointly utilized to effectively capture the spatial dependencies in the latent representations. However, the latents are feature maps of the same spatial resolution in previous works, which contain some redundancies that affect the R-D performance. In this paper, we propose the first learned multi-frequency image compression and entropy coding approach that is based on the recently developed octave convolutions to factorize the latents into high and low frequency (resolution) components, where the low frequency is represented by a lower resolution. Therefore, its spatial redundancy is reduced, which improves the R-D performance. Novel generalized octave convolution and octave transposed-convolution architectures with internal activation layers are also proposed to preserve more spatial structure of the information. Experimental results show that the proposed scheme not only outperforms all existing learned methods as well as standard codecs such as the next-generation video coding standard VVC (4:2:0) on the Kodak dataset in both PSNR and MS-SSIM. We also show that the proposed generalized octave convolution can improve the performance of other auto-encoder-based computer vision tasks such as semantic segmentation and image denoising."
"Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques."
"Generalization in deep learning has been the topic of much recent theoretical and empirical research. Here we introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Such predictions should 1) scale correctly with data complexity; 2) scale correctly with training set size; 3) capture differences between architectures; 4) capture differences between optimization algorithms; 5) be quantitatively not too far from the true error (in particular, be non-vacuous); 6) be efficiently computable; and 7) be rigorous. We focus on generalization error upper bounds, and introduce a categorisation of bounds depending on assumptions on the algorithm and data. We review a wide range of existing approaches, from classical VC dimension to recent PAC-Bayesian bounds, commenting on how well they perform against the desiderata.   We next use a function-based picture to derive a marginal-likelihood PAC-Bayesian bound. This bound is, by one definition, optimal up to a multiplicative constant in the asymptotic limit of large training sets, as long as the learning curve follows a power law, which is typically found in practice for deep learning problems. Extensive empirical analysis demonstrates that our marginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results for 6 and 7 are promising, but not yet fully conclusive, while only desideratum 4 is currently beyond the scope of our bound. Finally, we comment on why this function-based bound performs significantly better than current parameter-based PAC-Bayes bounds."
"We study the impact of a non-vanishing (positive) cosmological constant on the innermost and outermost stable circular orbits (ISCOs and OSCOs, respectively) within massive gravity in four dimensions. The gravitational field generated by a point-like object within this theory is known, generalizing the usual Schwarzschild--de Sitter geometry of General Relativity. In the non-relativistic limit, the gravitational potential differs by the one corresponding to the Schwarzschild--de Sitter geometry by a term that is linear in the radial coordinate with some prefactor $\gamma$, which is the only free parameter. Starting from the geodesic equations for massive test particles and the corresponding effective potential, we obtain a polynomial of fifth order that allows us to compute the innermost and outermost stable circular orbits. Next, we numerically compute the real and positive roots of the polynomial for several different structures (from the hydrogen atom to stars and globular clusters to galaxies and galaxy clusters) considering three distinct values of the parameter $\gamma$, determined using physical considerations, such as galaxy rotation curves and orbital precession. Similarly to the Kottler spacetime, both ISCOs and OSCOs appear. Their astrophysical relevance as well as the comparison with the Kottler spacetime are briefly discussed."
"In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence. We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework. Bayesian schemes, on the other hand, often make unrealistic demands for structure. The propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done. We would like to emphasize that the basic idea of local computation for propagating probabilities is due to Judea Pearl. It is a very innovative idea; we do not believe that it can be found in the Bayesian literature prior to Pearl's work. We see our contribution as extending the usefulness of Pearl's idea by generalizing it from Bayesian probabilities to belief functions. In the next section, we give a brief introduction to belief functions. The notions of qualitative independence for partitions and a qualitative Markov tree are introduced in Section III. Finally, in Section IV, we describe a scheme for propagating belief functions in qualitative Markov trees."
"Large scale structure surveys promise to be the next leading probe of cosmological information. It is therefore crucial to reliably predict their observables. The Effective Field Theory of Large Scale Structures (EFTofLSS) provides a manifestly convergent perturbation theory for the weakly non-linear regime of dark matter, where correlation functions are computed in an expansion of the wavenumber k of a mode over the wavenumber associated with the non-linear scale k_nl. Since most of the information is contained at high wavenumbers, it is necessary to compute higher order corrections to correlation functions. After the one-loop correction to the matter power spectrum, we estimate that the next leading one is the two-loop contribution, which we compute here. At this order in k/k_nl, there is only one counterterm in the EFTofLSS that must be included, though this term contributes both at tree-level and in several one-loop diagrams. We also discuss correlation functions involving the velocity and momentum fields. We find that the EFTofLSS prediction at two loops matches to percent accuracy the non-linear matter power spectrum at redshift zero up to k~0.6 h/Mpc, requiring just one unknown coefficient that needs to be fit to observations. Given that Standard Perturbation Theory stops converging at redshift zero at k~0.1 h/Mpc, our results demonstrate the possibility of accessing a factor of order 200 more dark matter quasi-linear modes than naively expected. If the remaining observational challenges to accessing these modes can be addressed with similar success, our results show that there is tremendous potential for large scale structure surveys to explore the primordial universe."
"The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibility, supporting only a limited set of optimizations.   This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a scheduling language. The algorithm language simplifies expressing the algorithms. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together optimizations. We also built an autotuner to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\times$, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework."
"As nanoelectronics approaches the nanometer scale, a massive effort is underway to identify the next scalable logic technology beyond Complementary Metal Oxide Semiconductor (CMOS) computing. Such computing technology needs to improve switching energy & delay at reduced dimensions, allow improved interconnects and provide a complete logic/memory family. However, a viable beyond-CMOS logic technology has remained elusive. Here, we propose a scalable spintronic logic device which operates via spin-orbit transduction combined with magneto-electric switching. The proposed Magneto-Electric Spin Orbit (MESO) logic enables a new paradigm to continue scaling of logic performance to near thermodynamic limits for GHz logic (100 kT switching energy at 100 ps delay). The proposed MESO devices scale strongly and favorably with critical dimensions of the device, showing a cubic dependence of switching energy on size, (E_m proportional to W^3), and square dependence on voltage (E_m proportional to V^2). The excellent scaling is obtained thanks to the properties of the spin orbit effects (e.g. Inverse Spin Hall Effect (ISHE) and Inverse Rashba-Edelstein Effect (IREE)) and the dependence of capacitance on size. The operating voltages for these devices are predicted to be < 100 mV allowing a significant jump ahead of historic trends of scaling voltage with size and corresponding reduction of energy. Interconnect resistance is a critical obstacle for scaling beyond 10 nm dimensions. We project a less detrimental impact of interconnect resistance and show that MESO logic is amenable for highly resistive interconnects (100 uOhm.cm-1 mOhm.cm) which opens a possibility to use nano-metallic (width < bulk electron mean free path) or doped semiconducting wires (width<5 nm). A scalable, CMOS compatible, non-volatile logic family proposed here may enable the next multi-generational scaling of computing devices."
"This research addresses the multiprocessor scheduling problem of hard real-time systems, and it especially focuses on optimal and global schedulers when practical constraints are taken into account. First, we propose an improvement of the optimal algorithm BF. We formally prove that our adaptation is (i) optimal, i.e., it always generates a feasible schedule as long as such a schedule exists, and (ii) valid, i.e., it complies with the all the requirements. We also show that it outperforms BF by providing a computing complexity of O(n), where n is the number of tasks to be scheduled. Next, we propose a schedulability analysis which indicates a priori whether the real-time application can be scheduled by our improvement of BF without missing any deadline. This analysis is, to the best of our knowledge, the first such test for multiprocessors that takes into account all the main overheads generated by the Operating System."
"We computer model a free-standing vitreous silica bilayer which has recently been synthesized and characterized experimentally in landmark work. Here we model the bilayer using a computer assembly procedure that starts from a single layer of amorphous graphene, generated using a bond switching algorithm from an initially crystalline graphene structure. Next each bond is decorated with an oxygen atom and the carbon atoms are relabeled as silicon. This monolayer can be now thought of as a two dimensional network of corner sharing triangles. Next each triangle is made into a tetrahedron, by raising the silicon atom above each triangle and adding an additional singly coordinated oxygen atom at the apex. The final step is to mirror reflect this layer to form a second layer and then attach the two layers together to form the bilayer.   We show that this vitreous silica bilayer has the additional macroscopic degrees of freedom to easily form a network of identical corner sharing tetrahedra if there is a symmetry plane through the center of the bilayer going through the layer of oxygen ions that join the upper and lower layers. This has the consequence that the upper rings lie exactly above the lower rings, which are tilted in general. The assumption of a network of perfect corner sharing tetrahedra leads to a range of possible densities that we have previously characterized in three dimensional zeolites as a flexibility window. Finally, using a realistic potential, we have relaxed the bilayer to determine the density, and other structural characteristics such as the Si-Si pair distribution functions and the Si-O-Si bond angle distribution, which are compared to the experimental results obtained by direct imaging."
"Quantum annealing (QA) has been proposed as a quantum enhanced optimization heuristic exploiting tunneling. Here, we demonstrate how finite range tunneling can provide considerable computational advantage. For a crafted problem designed to have tall and narrow energy barriers separating local minima, the D-Wave 2X quantum annealer achieves significant runtime advantages relative to Simulated Annealing (SA). For instances with 945 variables, this results in a time-to-99%-success-probability that is $\sim 10^8$ times faster than SA running on a single processor core. We also compared physical QA with Quantum Monte Carlo (QMC), an algorithm that emulates quantum tunneling on classical processors. We observe a substantial constant overhead against physical QA: D-Wave 2X again runs up to $\sim 10^8$ times faster than an optimized implementation of QMC on a single core. We note that there exist heuristic classical algorithms that can solve most instances of Chimera structured problems in a timescale comparable to the D-Wave 2X. However, we believe that such solvers will become ineffective for the next generation of annealers currently being designed. To investigate whether finite range tunneling will also confer an advantage for problems of practical interest, we conduct numerical studies on binary optimization problems that cannot yet be represented on quantum hardware. For random instances of the number partitioning problem, we find numerically that QMC, as well as other algorithms designed to simulate QA, scale better than SA. We discuss the implications of these findings for the design of next generation quantum annealers."
"We consider collocated wireless sensor networks, where each node has a Boolean measurement and the goal is to compute a given Boolean function of these measurements. We first consider the worst case setting and study optimal block computation strategies for computing symmetric Boolean functions. We study three classes of functions: threshold functions, delta functions and interval functions. We provide exactly optimal strategies for the first two classes, and a scaling law order-optimal strategy with optimal preconstant for interval functions. We also extend the results to the case of integer measurements and certain integer-valued functions. We use lower bounds from communication complexity theory, and provide an achievable scheme using information theoretic tools.   Next, we consider the case where nodes measurements are random and drawn from independent Bernoulli distributions. We address the problem of optimal function computation so as to minimize the expected total number of bits that are transmitted. In the case of computing a single instance of a Boolean threshold function, we show the surprising result that the optimal order of transmissions depends in an extremely simple way on the values of previously transmitted bits, and the ordering of the marginal probabilities of the Boolean variables.   The approach presented can be generalized to the case where each node has a block of measurements, though the resulting problem is somewhat harder, and we conjecture the optimal strategy. We further show how to generalize to a pulse model of communication. One can also consider the related problem of approximate computation given a fixed number of bits. In this case, the optimal strategy is significantly different, and lacks an elegant characterization. However, for the special case of the parity function, we show that the greedy strategy is optimal."
This paper describes a programme to study the computing model in CMS after the next long shutdown near the end of the decade.
"We continue the investigation of locally testable codes, i.e., error-correcting codes for whom membership of a given word in the code can be tested probabilistically by examining it in very few locations. We give two general results on local testability: First, motivated by the recently proposed notion of {\em robust} probabilistically checkable proofs, we introduce the notion of {\em robust} local testability of codes. We relate this notion to a product of codes introduced by Tanner, and show a very simple composition lemma for this notion. Next, we show that codes built by tensor products can be tested robustly and somewhat locally, by applying a variant of a test and proof technique introduced by Raz and Safra in the context of testing low-degree multivariate polynomials (which are a special case of tensor codes).   Combining these two results gives us a generic construction of codes of inverse polynomial rate, that are testable with poly-logarithmically many queries. We note these locally testable tensor codes can be obtained from {\em any} linear error correcting code with good distance. Previous results on local testability, albeit much stronger quantitatively, rely heavily on algebraic properties of the underlying codes."
"Catalog Services play a vital role on Data Grids by allowing users and applications to discover and locate the data needed. On large Data Grids, with hundreds of geographically distributed sites, centralized Catalog Services do not provide the required scalability, performance or fault-tolerance. In this article, we start by presenting and discussing the general requirements on Grid Catalogs of applications being developed by the EGEE user community. This provides the motivation for the second part of the article, where we present the replication and distribution mechanisms we have designed and implemented into the AMGA Metadata Catalog, which is part of the gLite software stack being developed for the EGEE project. Implementing these mechanisms in the catalog itself has the advantages of not requiring any special support from the relational database back-end, of being database independent, and of allowing tailoring the mechanisms to the specific requirements and characteristics of Metadata Catalogs."
"In the last twenty years, several approaches to higher-order rewriting have been proposed, among which Klop's Combinatory Rewrite Systems (CRSs), Nipkow's Higher-order Rewrite Systems (HRSs) and Jouannaud and Okada's higher-order algebraic specification languages, of which only the last one considers typed terms. The later approach has been extended by Jouannaud, Okada and the present author into Inductive Data Type Systems (IDTSs). In this paper, we extend IDTSs with the CRS higher-order pattern-matching mechanism, resulting in simply-typed CRSs. Then, we show how the termination criterion developed for IDTSs with first-order pattern-matching, called the General Schema, can be extended so as to prove the strong normalization of IDTSs with higher-order pattern-matching. Next, we compare the unified approach with HRSs. We first prove that the extended General Schema can also be applied to HRSs. Second, we show how Nipkow's higher-order critical pair analysis technique for proving local confluence can be applied to IDTSs."
"We study spontaneous scalarization in Scalar-Tensor boson stars. We find that scalarization does not occur in stars whose bosons have no self-interaction. We introduce a quartic self-interaction term into the boson Lagrangian and show that when this term is large, scalarization does occur. Strong self-interaction leads to a large value of the compactness (or sensitivity) of the boson star, a necessary condition for scalarization to occur, and we derive an analytical expression for computing the sensitivity of a boson star in Brans-Dicke theory from its mass and particle number. Next we comment on how one can use the sensitivity of a star in any Scalar-Tensor theory to determine how its mass changes when it undergoes gravitational evolution. Finally, in the Appendix, we derive the most general form of the boson wavefunction that minimises the energy of the star when the bosons carry a U(1) charge."
"Using the general framework of quantum field theory, we derive basic equations of quantum field kinetics. The main goal of this approach is to compute the observables associated with a quark-gluon plasma at different stages of its evolution. We start by rewriting the integral equations for the field correlators in different forms, depending on the relevant dynamical features at each different stage. Next, two versions of perturbation expansion are considered. The first is best suited for the calculation of electromagnetic emission from chaotic, but not equilibrated, strongly interacting matter. The second version allows one to derive evolution equations, which are generalizations of the familiar QCD evolution equations, and provide a basis for the calculation of the initial quark and gluon distributions after the first hard interaction of the heavy ions."
"Physics beyond the Standard Model could be measured indirectly, through its effects on Standard Model observables. One place to look for such effects is in the semileptonic decays of B mesons. In order to constrain the possible role of new physics on semileptonic B decays, we present the most general low energy effective Lagrangian constructed from all dimension six four-fermion operators that contribute to the process b-> c l- nu. We then use it to compute the corrections due to new physics to the differential decay rates for the exclusive processes B-> D(*) l- nu, as well as the inclusive decays B-> X l- nu. Both inclusive and exclusive rates are expressed in terms of a set of parameters that characterize the types of four-fermion interactions that can be induced by physics beyond the Standard Model. Although it is not particularly useful to carry out a full analysis until data from the next generation of B factories becomes available, here we illustrate how the existing experimental results can be used to constrain some of these parameters."
"Albertson has defined the irregularity of a simple undirected graph $G=(V,E)$ as $ \irr(G) = \sum_{uv\in E}|d_G(u)-d_G(v)|,$ where $d_G(u)$ denotes the degree of a vertex $u \in V$. Recently, this graph invariant gained interest in the chemical graph theory, where it occured in some bounds on the first and the second Zagreb index, and was named the third Zagreb index Fath-Tabar. For general graphs with $n$ vertices, Albertson has obtained an asymptotically tight upper bound on the irregularity of $4 n^3 /27.$ Here, by exploiting a different approach than in Albertson, we show that for general graphs with $n$ vertices the upper bound $\lfloor \frac{n}{3} \rfloor \lceil \frac{2 n}{3} \rceil (\lceil \frac{2 n}{3} \rceil -1)$ is sharp. Next, we determine $k$-cyclic graphs with maximal irregularity. We also present some bounds on the maximal/minimal irregularity of graphs with fixed minimal and/or maximal vertex degrees, and consider an approximate computation of the irregularity of a graph."
"We calculate the baryon asymmetry generated at the electroweak phase transition in the minimal supersymmetric standard model, using a new method to compute the CP-violating asymmetry in the Higgsino flux reflected into the unbroken phase. The method is based on a Higgs insertion expansion. We find that the CP asymmetry at leading order is proportional to the change in $\tan \beta$ in the bubble wall, which is at most of order $10^{-2}$, while at next-to-leading order this suppression factor disappears. This result may enhance the final baryon asymmetry generated during the electroweak phase transition for small $\Delta \beta (< 10^{-3}$)."
"Next-to-leading logarithmic final-state resummed predictions have traditionally been calculated, manually, separately for each observable. In this article we derive NLL resummed results for generic observables. We highlight and discuss the conditions that the observable should satisfy for the approach to be valid, in particular continuous globalness and recursive infrared and collinear safety. The resulting resummation formula is expressed in terms of certain well-defined characteristics of the observable. We have written a computer program, CAESAR, which, given a subroutine for an arbitrary observable, determines those characteristics, enabling full automation of a large class of final-state resummations, in a range of processes."
"We investigate $N=2$ extended superconformal symmetry, using the half-twisted Landau-Ginzburg models. The first example is the $D_{2n+2}$ -type minimal model. It has been conjectured that this model has a spin $n$ super $W$ current. We checked this by the direct computations of the BRS cohomology class up to $n=4$. We observe for $n\le 3$ the super W currents generate the ring isomorphic to the chiral ring of the model with respect to the classical product. We thus conjecture that this isomorphism holds for any $n$. The next example is $ CP_{n}$ coset model. In this case we find a sort of Miura transformation which gives the simple formula for the super W currents of spin \{1,2,...,n\} in terms of the chiral superfields. Explicit form of the super W currents and their Poisson brackets are obtained for $CP_{2},CP_{3}$ case. We also conjecture that as long as the classical product is concerned, these super W currents generate the ring isomorphic to the chiral ring of the model and this is checked for $CP_2$ model."
"The determination of the critical exponents by means of the Exact Renormalizion Group approach is still a topic of debate. The general flow equation is by construction scheme independent, but the use of the truncated derivative expansion generates a model dependence in the determination of the universal quantities. We derive new nonperturbative flow equations for the one-component, $Z_2$ symmetric scalar field to the next-to-leading order of the derivative expansion by means of a class of proper time regulators. The critical exponents $\eta$, $\nu$ and $\omega$ for the Wilson-Fisher fixed point are computed by numerical integration of the flow equations, without resorting to polynomial truncations. We show that by reducing the width of the cut-off employed, the critical exponents become rapidly insensitive to the cut-off width and their values are in good agreement with the results of entirely different approaches."
"We revisit the general topic of thermodynamical stability and critical phenomena in black hole physics, analyzing in detail the phase diagram of the five dimensional rotating black hole and the black rings discovered by Emparan and Reall. First we address the issue of microcanonical stability of these spacetimes and its relation to thermodynamics by using the so-called Poincare (or ""turning point"") method, which we review in detail. We are able to prove that one of the black ring branches is always locally unstable, showing that there is a change of stability at the point where the two black ring branches meet. Next we study divergence of fluctuations, the geometry of the thermodynamic state space (Ruppeiner geometry) and compute the appropriate critical exponents and verify the scaling laws familiar from RG theory in statistical mechanics. We find that, at extremality, the behaviour of the system is formally very similar to a second order phase transition."
The 1/Nc expansion is formulated for the baryon wave function in terms of a specially constructed generating functional. The leading order of this 1/Nc expansion is universal for all low-lying baryons [including the O(1/Nc) and O(Nc^0) excited resonances] and for baryon-meson scattering states. A nonlinear evolution equation of Hamilton-Jacobi type is derived for the generating functional describing the baryon distribution amplitude in the large-Nc limit. In the asymptotic regime this nonlinear equation is solved analytically. The anomalous dimensions of the leading-twist baryon operators diagonalizing the evolution are computed analytically up to the next-to-leading order of the 1/Nc expansion.
"In this dissertation we study Courant algebroids, objects that first appeared in the work of T. Courant on Dirac structures; they were later studied by Liu, Weinstein and Xu who used Courant algebroids to generalize the notion of the Drinfeld double to Lie bialgebroids. As a first step towards understanding the complicated properties of Courant algebroids, we interpret them by associating to each Courant algebroid a strongly homotopy Lie algebra in a natural way. Next, we propose an alternative construction of the double of a Lie bialgebroid as a homological hamiltonian vector field on an even symplectic supermanifold. The classical BRST complex and the Weil algebra arise as special cases. We recover the Courant algebroid via the derived bracket construction and give a simple proof of the doubling theorem of Liu, Weinstein and Xu. We also introduce a generalization, quasi-Lie bialgebroids, analogous to Drinfeld's quasi-Lie bialgebras; we show that the derived bracket construction in this case also yields a Courant algebroid. Finally, we compute the Poisson cohomology of a one-parameter family of SU(2)- covariant Poisson structures on S^2. As an application, we show that these structures are non-trivial deformations of each other, and that they do not admit rescaling."
"The Alekseev-Gr{\""o}bner lemma is combined with the theory of modified equations to obtain an \emph{a priori} estimate for the global error of numerical integrators. This estimate is correct up to a remainder term of order $h^{2p}$, where $h$ denotes the step size and $p$ the order of the method. It is applied to a class of nonautonomous linear oscillatory equations, which includes the Airy equation, thereby improving prior work which only gave the $h^p$ term.   Next, nonlinear oscillators whose behaviour is described by the Emden-Fowler equation $y'' + t^\nu y^n = 0$ are considered, and global errors committed by Runge-Kutta methods are calculated. Numerical experiments show that the resulting estimates are generally accurate. The main conclusion is that we need to do a full calculation to obtain good estimates: the behaviour is different from the linear case, it is not sufficient to look only at the leading term, and merely considering the local error does not provide an accurate picture either."
"Every directed graph defines a Hilbert space and a family of weighted shifts that act on the space. We identify a natural notion of periodicity for such shifts and study their C*-algebras. We prove the algebras generated by all shifts of a fixed period are of Cuntz-Krieger and Toeplitz-Cuntz-Krieger type. The limit C*-algebras determined by an increasing sequence of positive integers, each dividing the next, are proved to be isomorphic to Cuntz-Pimsner algebras and the linking maps are shown to arise as factor maps. We derive a characterization of simplicity and compute the $K$-groups for these algebras. We prove a classification theorem for the class of algebras generated by simple loop graphs."
"We review the construction of the Dirac operator and its properties in Riemannian geometry and show how the asymptotic expansion of the trace of the heat kernel determines the spectral invariants of the Dirac operator and its index. We also point out that the Einstein-Hilbert functional can be obtained as a linear combination of the first two spectral invariants of the Dirac operator. Next, we report on our previous attempts to generalize the notion of the Dirac operator to the case of Matrix Geometry, when instead of a Riemannian metric there is a matrix valued self-adjoint symmetric two-tensor that plays a role of a ``non-commutative'' metric. We construct invariant first-order and second-order self-adjoint elliptic partial differential operators, that can be called ``non-commutative'' Dirac operator and non-commutative Laplace operator. We construct the corresponding heat kernel for the non-commutative Laplace type operator and compute its first two spectral invariants. A linear combination of these two spectral invariants gives a functional that can be considered as a non-commutative generalization of the Einstein-Hilbert action."
"Deployment of distributed systems sets high requirements for procedures and tools for the complex testing of these systems. This work introduces a formal four-layered model for test generation mission on the basis of the component-based approach and the concept of layered networks. Based on this model, we describe the test generation strategy that covers every interaction from the end-user requirements on all coexisting architectural layers, and checks the internal consistency of the system technical specifications with respect to the end-user requirements. The next step introduces the Prolog-based approach to representing this model and the requirements-coverage strategy"
"In this paper we present the applications of methods from wavelet analysis to polynomial approximations for a number of accelerator physics problems. In the general case we have the solution as a multiresolution expansion in the base of compactly supported wavelet basis. The solution is parametrized by the solutions of two reduced algebraical problems, one is nonlinear and the second is some linear problem, which is obtained from one of the next wavelet constructions: Fast Wavelet Transform, Stationary Subdivision Schemes, the method of Connection Coefficients. According to the orbit method and by using construction from the geometric quantization theory we construct the symplectic and Poisson structures associated with generalized wavelets by using metaplectic structure. We consider wavelet approach to the calculations of Melnikov functions in the theory of homoclinic chaos in perturbed Hamiltonian systems and for parametrization of Arnold-Weinstein curves in Floer variational approach."
"A general-purpose, self-adapting Monte Carlo (MC) algorithm implemented in the program {\tt Foam} is described. The high efficiency of the MC, that is small maximum weight or variance of the MC weight is achieved by means of dividing the integration domain into small cells. The cells can be $n$-dimensional simplices, hyperrectangles or a Cartesian product of them. The grid of cells, ``foam'', is produced in the process of the binary split of the cells. The next cell to be divided and the position/direction of the division hyperplane is chosen by the algorithm which optimizes the ratio of the maximum weight to the average weight or (optionally) the total variance. The algorithm is able to deal, in principle, with an arbitrary pattern of the singularities in the distribution."
"The first generation of European e-passports will be issued in 2006. We discuss how borders are crossed regarding the security and privacy erosion of the proposed schemes, and show which borders need to be crossed to improve the security and the privacy protection of the next generation of e-passports. In particular we discuss attacks on Basic Access Control due to the low entropy of the data from which the access keys are derived, we sketch the European proposals for Extended Access Control and the weaknesses in that scheme, and show how fundamentally different design decisions can make e-passports more secure."
"Embedded real-time applications in communication systems require high processing power. Manual scheduling devel-oped for single-processor applications is not suited to multi-core architectures. The Algorithm Architecture Matching (AAM) methodology optimizes static application implementation on multi-core architectures. The Random Access Channel Preamble Detection (RACH-PD) is an algorithm for non-synchronized access of Long Term Evolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency of the next generation cellular system. This paper de-scribes a complete methodology for implementing the RACH-PD. AAM prototyping is applied to the RACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient implemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then explained. Benchmarks for the solution are given."
"In Mod. Phys. Lett. A 9, 3119 (1994), one of us (R.D.S) investigated a formulation of quantum mechanics as a generalized measure theory. Quantum mechanics computes probabilities from the absolute squares of complex amplitudes, and the resulting interference violates the (Kolmogorov) sum rule expressing the additivity of probabilities of mutually exclusive events. However, there is a higher order sum rule that quantum mechanics does obey, involving the probabilities of three mutually exclusive possibilities. We could imagine a yet more general theory by assuming that it violates the next higher sum rule. In this paper, we report results from an ongoing experiment that sets out to test the validity of this second sum rule by measuring the interference patterns produced by three slits and all the possible combinations of those slits being open or closed. We use attenuated laser light combined with single photon counting to confirm the particle character of the measured light."
"Graph transformation is concerned with the manipulation of graphs by means of rules. Graph grammars have been traditionally studied using techniques from category theory. In previous works, we introduced Matrix Graph Grammars (MGGs) as a purely algebraic approach for the study of graph grammars and graph dynamics, based on the representation of graphs by means of their adjacency matrices. MGGs have been succesfully applied to problems such as applicability of rule sequences, sequentialization and reachability, providing new analysis techniques and generalizing and improving previous results.   Our next objective is to generalize MGGs in order to approach computational complexity theory and ""static"" properties of graphs out of the ""dynamics"" of certain grammars. In the present work, we start building bridges between MGGs and complexity by introducing what we call ""Monotone Complex Logic"", which allows establishing a (bijective) link between MGGs and complex analysis. We use this logic to recast the formulation and basic building blocks of MGGs as more proper geometric and analytic concepts (scalar products, norms, distances). MGG rules can also be interpreted - via operators - as complex numbers. Interestingly, the subset they define can be characterized as the Sierpinski gasket."
"We summarise the motivation for, and the status of, the tools developed by CEDAR/MCnet for validating and tuning Monte Carlo event generators for the LHC against data from previous colliders. We then present selected preliminary results from studies of event shapes and hadronisation observables from e+e- colliders, and of minimum bias and underlying event observables from the Tevatron, and comment on the approach needed with early LHC data to best exploit the potential for new physics discoveries at the LHC in the next few years."
"We revisit the old (fourth-order or quadratically generated) gravity model of Starobinsky in four space-time dimensions, and derive the (inflaton) scalar potential in the equivalent scalar-tensor gravity model via a Legendre-Weyl transform. The inflaton scalar potential is used to compute the (CMB) observables of inflation associated with curvature perturbations (namely, the scalar and tensor spectral indices, and the tensor-to-scalar ratio), including the new next-to-leading-order terms with respect to the inverse number of e-foldings. The results are compared to the recent (WMAP5) experimental bounds. We confirm both mathematical and physical equivalence between f(R) gravity theories and the corresponding scalar-tensor gravity theories."
"It was pointed out by Eliashberg in his ICM 2006 plenary talk that the integrable systems of rational Gromov-Witten theory very naturally appear in the rich algebraic formalism of symplectic field theory (SFT). Carefully generalizing the definition of gravitational descendants from Gromov-Witten theory to SFT, one can assign to every contact manifold a Hamiltonian system with symmetries on SFT homology and the question of its integrability arises. While we have shown how the well-known string, dilaton and divisor equations translate from Gromov-Witten theory to SFT, the next step is to show how genus-zero topological recursion translates to SFT. Compatible with the example of SFT of closed geodesics, it turns out that the corresponding localization theorem requires a non-equivariant version of SFT, which is generated by parametrized instead of unparametrized closed Reeb orbits. Since this non-equivariant version is so far only defined for cylindrical contact homology, we restrict ourselves to this special case. As an important result we show that, as in rational Gromov-Witten theory, all descendant invariants can be computed from primary invariants, i.e. without descendants."
"Radio astronomy has benefited greatly from advances in technology and will continue to do so in the future. In fact, we are experiencing a revolution in the way radio astronomy is conducted as our instruments allow us now to directly ""digitize"" our photons. This has enormous consequences, since we can greatly benefit from the continuing advances in digital electronics, telecommunication and computing. The results are dramatic increase in observable bandwidths, FoVs, frequency coverage and collecting area. The global efforts will culminate in the construction of the SKA as the world's largest and most powerful telescope. On the way projects like LOFAR, LEAP and others will revolutionize many areas of astrophysics and fundamental physics. Observations of pulsars will play a central role in these scientific endeavours. We briefly summarize here some recent scientific developments that help us in defining our expectations for the the new generation of radio telescopes for pulsar astrophysics."
"Recently, the new Kinect One has been issued by Microsoft, providing the next generation of real-time range sensing devices based on the Time-of-Flight (ToF) principle. As the first Kinect version was using a structured light approach, one would expect various differences in the characteristics of the range data delivered by both devices. This paper presents a detailed and in-depth comparison between both devices. In order to conduct the comparison, we propose a framework of seven different experimental setups, which is a generic basis for evaluating range cameras such as Kinect. The experiments have been designed with the goal to capture individual effects of the Kinect devices as isolatedly as possible and in a way, that they can also be adopted, in order to apply them to any other range sensing device. The overall goal of this paper is to provide a solid insight into the pros and cons of either device. Thus, scientists that are interested in using Kinect range sensing cameras in their specific application scenario can directly assess the expected, specific benefits and potential problem of either device."
"This report aims to present the main ideas of Regge calculus necessary to understand the basic premise of CDT. Next, the main strategy of the CDT approach is introduced in general terms. The main focus of this report is the 2-D model of CDT. The section on the 2-D model closely follows a single paper (\cite{ambjorn98}). While the 4-D or even 3-D case will behave very differently from the 2-D model, 2-D CDT can be solved exactly, and as such offers a better introductory exposition of CDT's methods. Higher-dimensional CDT requires a lot of computer simulation, and lies outside the scope of this report.   All derivations carried out explicitly are the result of the author's independent work in attempting to find and prove how the results presented were obtained by CDT authors. Because these derivations were made explicit by the author, this paper can act as a guide to those who are new to CDT."
"We present predictions for the production cross section of a Standard Model Z0-boson in association with a top-antitop pair at the next-to-leading order accuracy in QCD, matched with shower Monte Carlo programs to evolve the system down to the hadronization energy scale. We adopt a framework based on three well established numerical codes, namely the POWHEG-BOX, used for computing the cross section, HELAC-NLO, which generates all necessary input matrix elements, and finally a parton shower program, such as PYTHIA or HERWIG, which allows for including t-quark and Z0-boson decays at the leading order accuracy and generates shower emissions, hadronization and hadron decays."
"A state-dependent degraded broadcast diamond channel is studied where the source-to-relays cut is modeled with two noiseless, finite-capacity digital links with a degraded broadcasting structure, while the relays-to-destination cut is a general multiple access channel controlled by a random state. It is assumed that the source has non-causal channel state information and the relays have no state information. Under this model, first, the capacity is characterized for the case where the destination has state information, i.e., has access to the state sequence. It is demonstrated that in this case, a joint message and state transmission scheme via binning is optimal. Next, the case where the destination does not have state information, i.e., the case with state information at the source only, is considered. For this scenario, lower and upper bounds on the capacity are derived for the general discrete memoryless model. Achievable rates are then computed for the case in which the relays-to-destination cut is affected by an additive Gaussian state. Numerical results are provided that illuminate the performance advantages that can be accrued by leveraging non-causal state information at the source."
"The asynchronous systems are the non-deterministic models of the asynchronous circuits from the digital electrical engineering, where non-determinism is a consequence of the fact that modelling is made in the presence of unknown and variable parameters. Such a system is a multi-valued function f that assigns to an (admissible) input u:R{\to}{0,1}^{m} a set f(u) of (possible) states x:R{\to}{0,1}^{n}. When this assignment is defined by making use of a so-called generator function {\Phi}:{0,1}^{n}{\times}{0,1}^{m}{\to}{0,1}^{n}, then the asynchronous system f is called regular. The generator function {\Phi} acts in this asynchronous framework similarly with the next state function from a synchronous framework. The parallel connection of the asynchronous systems f' and f"" is the asynchronous system (f'||f"")(u)=f'(u){\times}f""(u). The purpose of the paper is to give the circumstances under which a regular asynchronous system f may be written as a parallel connection of regular asynchronous systems."
"With the LHC close to complete its 8 TeV run, the experimental searches have already started to probe the vast beyond-the-standard Model scenery. Providing next-to-leading order (NLO) predictions for the major new physics discovery channels is therefore a most pressing request to particle phenomenologists these days. MadGolem is a new computational tool that automates NLO calculations of generic 2->2 new physics processes in the MadGraph/Golem framework. In this contribution we concisely describe the structure and performance of the code, with particular focus on the generation of the renormalized one-loop amplitudes and the automatized subtraction of infrared and on-shell divergences. We briefly survey the many dedicated tests of all these aspects and outline some applications to LHC phenomenology."
"Risk is unavoidable in business and risk management is needed amongst others to set up good security policies. Once the risks are evaluated, the next step is to decide how they should be treated. This involves managers making decisions on proper countermeasures to be implemented to mitigate the risks. The countermeasure expenditure, together with its ability to mitigate risks, is factors that affect the selection. While many approaches have been proposed to perform risk analysis, there has been less focus on delivering the prescriptive and specific information that managers require to select cost-effective countermeasures. This paper proposes a generic approach to integrate the cost assessment into risk analysis to aid such decision making. The approach makes use of a risk model which has been annotated with potential countermeasures, estimates for their cost and effect. A calculus is then employed to reason about this model in order to support decision in terms of decision diagrams. We exemplify the instantiation of the generic approach in the CORAS method for security risk analysis."
"Background: Computational analysis of next-generation sequencing data is outpaced by data generation in many cases. In one such case, paired-end reads can be produced from the Illumina sequencing method faster than they can be overlapped by downstream analysis. The advantages in read length and accuracy provided by overlapping paired-end reads demonstrates the necessity for software to efficiently solve this problem.   Results: XORRO is an extremely efficient paired-end read overlapping program. XORRO can overlap millions of short paired-end reads in a few minutes. It uses 64-bit registers with a two bit alphabet to represent sequences and does comparisons using low-level logical operations like XOR, AND, bitshifting and popcount.   Conclusions: As of the writing of this manuscript, XORRO provides the fastest solution to the paired-end read overlap problem. XORRO is available for download at: sourceforge.net/projects/xorro-overlap/"
"We compute the one-loop electroweak Sudakov corrections to the production process Z (nu bar{nu}) + n jets, with n = 1,2,3, in p p collisions at the LHC. It represents the main irreducible background to new physics searches at the energy frontier. The results are obtained at the leading and next-to-leading logarithmic accuracy by implementing the general algorithm of Denner-Pozzorini in the event generator for multiparton processes ALPGEN. For the standard selection cuts used by ATLAS and CMS collaborations, we show that the Sudakov corrections to the relevant observables can grow up to - 40% at sqrt{s} = 14 TeV. We also include the contribution due to undetected real radiation of massive gauge bosons, to show to what extent the partial cancellation with the large negative virtual corrections takes place in realistic event selections."
"Question answering (QA) system aims at retrieving precise information from a large collection of documents against a query. This paper describes the architecture of a Natural Language Question Answering (NLQA) system for a specific domain based on the ontological information, a step towards semantic web question answering. The proposed architecture defines four basic modules suitable for enhancing current QA capabilities with the ability of processing complex questions. The first module was the question processing, which analyses and classifies the question and also reformulates the user query. The second module allows the process of retrieving the relevant documents. The next module processes the retrieved documents, and the last module performs the extraction and generation of a response. Natural language processing techniques are used for processing the question and documents and also for answer extraction. Ontology and domain knowledge are used for reformulating queries and identifying the relations. The aim of the system is to generate short and specific answer to the question that is asked in the natural language in a specific domain. We have achieved 94 % accuracy of natural language question answering in our implementation."
"In this note, we calculate the $S^3$ free energy $F$ of 3-d ${\cal N}\geq 4$ supersymmetric gauge theories with $U(N)$, $O(N)$, and $USp(2N)$ gauge groups and matter hypermultiplets in the fundamental and two-index tensor representations. Supersymmetric localization reduces the computation of $F$ to a matrix model that we solve in the large $N$ limit using two different methods. The first method is a saddle point approximation first introduced in arXiv:1011.5487, which we extend to next-to-leading order in $1/N$. The second method generalizes the Fermi gas approach of arXiv:1110.4066 to theories with symplectic and orthogonal gauge groups, and yields an expression for $F$ valid to all orders in $1/N$. In developing the second method, we use a non-trivial generalization of the Cauchy determinant formula."
"We present a multi-contact walking pattern generator based on preview-control of the 3D acceleration of the center of mass (COM). A key point in the design of our algorithm is the calculation of contact-stability constraints. Thanks to a mathematical observation on the algebraic nature of the frictional wrench cone, we show that the 3D volume of feasible COM accelerations is a always a downward-pointing cone. We reduce its computation to a convex hull of (dual) 2D points, for which optimal O(n log n) algorithms are readily available. This reformulation brings a significant speedup compared to previous methods, which allows us to compute time-varying contact-stability criteria fast enough for the control loop. Next, we propose a conservative trajectory-wide contact-stability criterion, which can be derived from COM-acceleration volumes at marginal cost and directly applied in a model-predictive controller. We finally implement this pipeline and exemplify it with the HRP-4 humanoid model in multi-contact dynamically walking scenarios."
"Motivated by the Lee--Yang approach to phase transitions, we study the partition function of the Generalized Random Energy Model (GREM) at complex inverse temperature $\beta$. We compute the limiting log-partition function and describe the fluctuations of the partition function. For the GREM with $d$ levels, in total, there are $\frac 12 (d+1)(d+2)$ phases, each of which can symbolically be encoded as $G^{d_1}F^{d_2}E^{d_3}$ with $d_1,d_2,d_3\in\mathbb{N}_0$ such that $d_1+d_2+d_3=d$. In phase $G^{d_1}F^{d_2}E^{d_3}$, the first $d_1$ levels (counting from the root of the GREM tree) are in the glassy phase (G), the next $d_2$ levels are dominated by fluctuations (F), and the last $d_3$ levels are dominated by the expectation (E). Only the phases of the form $G^{d_1}E^{d_3}$ intersect the real $\beta$ axis. We describe the limiting distribution of the zeros of the partition function in the complex $\beta$ plane (= Fisher zeros). It turns out that the complex zeros densely touch the positive real axis at $d$ points at which the GREM is known to undergo phase transitions. Our results confirm rigorously and considerably extend the replica-method predictions from the physics literature."
"The classical parking functions, counted by the Cayley number (n+1)^(n-1), carry a natural permutation representation of the symmetric group S_n in which the number of orbits is the n'th Catalan number. In this paper, we will generalize this setup to rational parking functions indexed by a pair (a,b) of coprime positive integers. We show that these parking functions, which are counted by b^(a-1), carry a permutation representation of S_a in which the number of orbits is a rational Catalan number. We compute the Frobenius characteristic of the S_a-module of (a,b)-parking functions. Next we propose a combinatorial formula for a q-analogue of the rational Catalan numbers and relate this formula to a new combinatorial model for q-binomial coefficients. Finally, we discuss q,t-analogues of rational Catalan numbers and parking functions (generalizing the shuffle conjecture for the classical case) and present several conjectures."
"We generalize a well-known algorithm for the generation of all subsets of a set in lexicographic order with respect to the sets as lists of elements (subset-lex order). We obtain algorithms for various combinatorial objects such as the subsets of a multiset, compositions and partitions represented as lists of parts, and for certain restricted growth strings. The algorithms are often loopless and require at most one extra variable for the computation of the next object. The performance of the algorithms is very competitive even when not loopless. A Gray code corresponding to the subset-lex order and a Gray code for compositions that was found during this work are described."
"In this paper, we analyze the mean number $E(n,d)$ of internal equilibria in a general $d$-player $n$-strategy evolutionary game where the agents' payoffs are normally distributed. First, we give a computationally implementable formula for the general case. Next we characterize the asymptotic behavior of $E(2,d)$, estimating its lower and upper bounds as $d$ increases. Two important consequences are obtained from this analysis. On the one hand, we show that in both cases the probability of seeing the maximal possible number of equilibria tends to zero when $d$ or $n$ respectively goes to infinity. On the other hand, we demonstrate that the expected number of stable equilibria is bounded within a certain interval. Finally, for larger $n$ and $d$, numerical results are provided and discussed."
"We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and $n$-best re-ranking in machine translation show that $gen$CNN outperforms the state-of-the-arts with big margins."
"We explore the method of style transfer presented in the article ""A Neural Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge (arXiv:1508.06576).   We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in the original paper, among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used.   To study the style space further we attempt to generate synthetic images by maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures.   Finally, we propose two new style representations built on top of network's features and discuss how one could be used to achieve local and potentially content-aware style transfer."
"Recently, evidence was provided for the existence of an $a$-function for renormalisable quantum field theories in three dimensions. An explicit expression was given at lowest order for general theories involving scalars and fermions, and shown to be related to the beta-functions by a gradient flow equation with positive-definite metric as in four dimensions. Here, we extend this lowest-order calculation to a general abelian Chern-Simons gauge theory coupled to fermions and scalars, and derive a prediction for part of the four-loop Yukawa beta-function. We also compute the complete four-loop Yukawa beta-function for the scalar-fermion theory and show that it is entirely consistent with the gradient flow equations at next-to-leading order."
"In this set of lectures we give a pedagogical introduction to the way in which the nilpotency of a super-de Rham operator can be exploited for the construction of gauge theories in superspace. We begin with a discussion of how the super-geometric closure conditions can be solved by simply computing the cocycles of the super-algebra. The next couple lectures are then devoted to applying this idea to extensions of the standard super-de Rham complex. This eventually results in a geometric ""trivialization"" of the consistency conditions required for non-abelian tensor hierarchies. Although this is a general conclusion, we focus specifically on the hierarchy obtained by compactifying the 3-form gauge field of 11D supergravity to 4D, $N = 1$ superspace. In the final lecture, we use the cohomological arguments developed herein to provide a geometric construction of the non-trivial Chern-Simons-type invariant in that tensor hierarchy and comment on generalizations. These lectures are based on a series of talks given at Texas A&M University from March 21-25."
"The dynamics of compact binary systems at the fourth post-Newtonian (4PN) approximation of general relativity has been recently completed in a self-consistent way. In this paper, we compute the ten Poincar\'e constants of the motion and present the equations of motion in the frame of the center of mass (CM), together with the corresponding CM Lagrangian, conserved energy and conserved angular momentum. Next, we investigate the reduction of the CM dynamics to the case of quasi-circular orbits. The non local (in time) tail effect at the 4PN order is consistently included, as well as the relevant radiation-reaction dissipative contributions to the energy and angular momentum."
"For the N=2 Kazama-Suzuki(KS) model on CP^3, the lowest higher spin current with spins (2, 5/2, 5/2,3) is obtained from the generalized GKO coset construction. By computing the operator product expansion of this current and itself, the next higher spin current with spins (3, 7/2, 7/2, 4) is also derived. This is a realization of the N=2 W_{N+1} algebra with N=3 in the supersymmetric WZW model. By incorporating the self-coupling constant of lowest higher spin current which is known for the general (N,k), we present the complete nonlinear operator product expansion of the lowest higher spin current with spins (2, 5/2, 5/2, 3) in the N=2 KS model on CP^N space. This should coincide with the asymptotic symmetry of the higher spin AdS_3 supergravity at the quantum level. The large (N,k) 't Hooft limit and the corresponding classical nonlinear algebra are also discussed."
"Multi-photon propagation in connected structures - a quantum walk - offers the potential for simulating complex physical systems and provides a route to universal quantum computation. Increasing the complexity of quantum photonic networks where the walk occurs is essential for many applications. Here, we implement a quantum walk of indistinguishable photon pairs in a multimode fiber supporting 380 modes. Using wavefront shaping, we control the propagation of the two-photon state through the fiber in which all modes are coupled. Excitation of arbitrary output modes of the system is realized by controlling classical and quantum interferences. This experiment demonstrates a highly multimode platform for multi-photon interference experiments and provides a powerful method to program a general high-dimensional multiport optical circuit. This work paves the way for the next generation of photonic devices for quantum simulation, computing and communication."
"We show that a digital sphere, constructed by the circular sweep of a digital semicircle (generatrix) around its diameter, consists of some holes (absentee-voxels), which appear on its spherical surface of revolution. This incompleteness calls for a proper characterization of the absentee-voxels whose restoration will yield a complete spherical surface without any holes. In this paper, we present a characterization of such absentee-voxels using certain techniques of digital geometry and show that their count varies quadratically with the radius of the semicircular generatrix. Next, we design an algorithm to fill these absentee-voxels so as to generate a spherical surface of revolution, which is more realistic from the viewpoint of visual perception. We further show that covering a solid sphere by a set of complete spheres also results in an asymptotically larger count of absentees, which is cubic in the radius of the sphere. The characterization and generation of complete solid spheres without any holes can also be accomplished in a similar fashion. We furnish test results to substantiate our theoretical findings."
"Write-Once-Memory (WOM) is a model for many modern non-volatile memories, such as flash memories. Recently, several capacity-achieving WOM coding schemes have been proposed based on polar coding. Due to the fact that practical computer memory systems always contain noises, a nature question to ask next is how may we generalize these coding schemes, such that they may also have the error-control capabilities. In this paper, we discuss a joint WOM and error-control coding scheme, which is a generalization of the capacity-achieving WOM codes based on source polarization. In this paper, we prove a sufficient and necessary condition for the noisy reading channel being less noisy than the test channel in data encoding in the polar WOM coding. Such a sufficient and necessary condition is usually satisfied in reality. As a consequence of the sufficient and necessary condition, the high entropy set related to the noisy channel is usually strictly contained in the high entropy set related to the test channel in data encoding. Therefore the low-complexity polar joint WOM and error-control codes are sufficient for most practical coding scenarios."
"We study the operator product expansions in the chiral algebra $\mathcal{W}_{\infty}$, first using the associativity conditions in the basis of primary generating fields and second using a different basis coming from the free field representation in which the OPE takes a simpler quadratic form. The results in the quadratic basis can be compactly written using certain bilocal combinations of the generating fields and we conjecture a closed-form formula for the complete OPE in this basis. Next we show that the commutation relations as well as correlation functions can be easily computed using properties of these bilocal fields. In the last part of this paper we verify the consistency with results derived previously by studying minimal models of $\mathcal{W}_{\infty}$ and comparing them to known reductions of $\mathcal{W}_{\infty}$ to $\mathcal{W}_N$. The results we obtain illustrate nicely the role of triality symmetry in the representation theory of $\mathcal{W}_{\infty}$."
"This paper treats power-aware throughput maxi-mization in a multi-user file downloading system. Each user can receive a new file only after its previous file is finished. The file state processes for each user act as coupled Markov chains that form a generalized restless bandit system. First, an optimal algorithm is derived for the case of one user. The algorithm maximizes throughput subject to an average power constraint. Next, the one-user algorithm is extended to a low complexity heuristic for the multi-user problem. The heuristic uses a simple online index policy. In a special case with no power-constraint, the multi-user heuristic is shown to be throughput optimal. Simulations are used to demonstrate effectiveness of the heuristic in the general case. For simple cases where the optimal solution can be computed offline, the heuristic is shown to be near-optimal for a wide range of parameters."
"Universal schema jointly embeds knowledge bases and textual patterns to reason about entities and relations for automatic knowledge base construction and information extraction. In the past, entity pairs and relations were represented as learned vectors with compatibility determined by a scoring function, limiting generalization to unseen text patterns and entities. Recently, 'column-less' versions of Universal Schema have used compositional pattern encoders to generalize to all text patterns. In this work we take the next step and propose a 'row-less' model of universal schema, removing explicit entity pair representations. Instead of learning vector representations for each entity pair in our training set, we treat an entity pair as a function of its relation types. In experimental results on the FB15k-237 benchmark we demonstrate that we can match the performance of a comparable model with explicit entity pair representations using a model of attention over relation types. We further demonstrate that the model per- forms with nearly the same accuracy on entity pairs never seen during training."
"In this paper, we develop a novel unified framework called DeepText for text region proposal generation and text detection in natural images via a fully convolutional neural network (CNN). First, we propose the inception region proposal network (Inception-RPN) and design a set of text characteristic prior bounding boxes to achieve high word recall with only hundred level candidate proposals. Next, we present a powerful textdetection network that embeds ambiguous text category (ATC) information and multilevel region-of-interest pooling (MLRP) for text and non-text classification and accurate localization. Finally, we apply an iterative bounding box voting scheme to pursue high recall in a complementary manner and introduce a filtering algorithm to retain the most suitable bounding box, while removing redundant inner and outer boxes for each text instance. Our approach achieves an F-measure of 0.83 and 0.85 on the ICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous state-of-the-art results."
"We propose a deep neural network fusion architecture for fast and robust pedestrian detection. The proposed network fusion architecture allows for parallel processing of multiple networks for speed. A single shot deep convolutional network is trained as a object detector to generate all possible pedestrian candidates of different sizes and occlusions. This network outputs a large variety of pedestrian candidates to cover the majority of ground-truth pedestrians while also introducing a large number of false positives. Next, multiple deep neural networks are used in parallel for further refinement of these pedestrian candidates. We introduce a soft-rejection based network fusion method to fuse the soft metrics from all networks together to generate the final confidence scores. Our method performs better than existing state-of-the-arts, especially when detecting small-size and occluded pedestrians. Furthermore, we propose a method for integrating pixel-wise semantic segmentation network into the network fusion architecture as a reinforcement to the pedestrian detector. The approach outperforms state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with significant boosts on several protocols. It is also faster than all other methods."
"The Wisdom of Crowds (WOC), as a theory in the social science, gets a new paradigm in computer science. The WOC theory explains that the aggregate decision made by a group is often better than those of its individual members if specific conditions are satisfied. This paper presents a novel framework for unsupervised and semi-supervised cluster ensemble by exploiting the WOC theory. We employ four conditions in the WOC theory, i.e., diversity, independency, decentralization and aggregation, to guide both the constructing of individual clustering results and the final combination for clustering ensemble. Firstly, independency criterion, as a novel mapping system on the raw data set, removes the correlation between features on our proposed method. Then, decentralization as a novel mechanism generates high-quality individual clustering results. Next, uniformity as a new diversity metric evaluates the generated clustering results. Further, weighted evidence accumulation clustering method is proposed for the final aggregation without using thresholding procedure. Experimental study on varied data sets demonstrates that the proposed approach achieves superior performance to state-of-the-art methods."
"We introduce a real-time, constrained, nonlinear Model Predictive Control for the motion planning of legged robots. The proposed approach uses a constrained optimal control algorithm known as SLQ. We improve the efficiency of this algorithm by introducing a multi-processing scheme for estimating value function in its backward pass. This pass has been often calculated as a single process. This parallel SLQ algorithm can optimize longer time horizons without proportional increase in its computation time. Thus, our MPC algorithm can generate optimized trajectories for the next few phases of the motion within only a few milliseconds. This outperforms the state of the art by at least one order of magnitude. The performance of the approach is validated on a quadruped robot for generating dynamic gaits such as trotting."
"We present Hindley-Milner-Cousots (HMC), an algorithm that allows any interprocedural analysis for first-order imperative programs to be used to verify safety properties of typed higher-order functional programs. HMC works as follows. First, it uses the type structure of the functional program to generate a set of logical refinement constraints whose satisfaction implies the safety of the source program. Next, it transforms the logical refinement constraints into a simple first-order imperative program that is safe iff the constraints are satisfiable. Thus, in one swoop, HMC makes tools for invariant generation, e.g., based on abstract domains, predicate abstraction, counterexample-guided refinement, and Craig interpolation be directly applicable to verify safety properties of modern functional languages in a fully automatic manner. We have implemented HMC and describe preliminary experimental results using two imperative checkers -- ARMC and InterProc -- to verify OCaml programs. Thus, by composing type-based reasoning grounded in program syntax and state-based reasoning grounded in abstract interpretation, HMC opens the door to automatic verification of programs written in modern programming languages."
"Multi-homogeneous polynomial systems arise in many applications. We provide bit complexity estimates for solving them which, up to a few extra other factors, are quadratic in the number of solutions and linear in the height of the input system under some genericity assumptions. The assumptions essentially imply that the Jacobian matrix of the system under study has maximal rank at the solution set and that this solution set if finite. The algorithm is probabilistic and a probability analysis is provided. Next, we apply these results to the problem of optimizing a linear map on the real trace of an algebraic set. Under some genericity assumptions, we provide bit complexity estimates for solving this polynomial minimization problem."
"Data warehouses are nowadays an important component in every competitive system, it's one of the main components on which business intelligence is based. We can even say that many companies are climbing to the next level and use a set of Data warehouses to provide the complete information or it's generally due to fusion of two or many companies. these Data warehouses can be heterogeneous and geographically separated, this structure is what we call federation, and even if the components are physically separated, they are logically seen as a single component. generally, these items are heterogeneous which make it difficult to create the logical federation schema,and the execution of user queries a complicated mission. In this paper, we will fill this gap by proposing an extension of an existent algorithm in order to treat different schema types (star, snow flack) including the treatment of hierarchies dimension using ontology"
"Triple-Helix arrangements of bi- and trilateral relations can be considered as adaptive eco-systems. During the last decade, we have further developed a Triple-Helix indicator of synergy as reduction of uncertainty in niches that can be shaped among three or more distributions. Reduction of uncertainty can be generated in correlations among distributions of relations, but this (next-order) effect can be counterbalanced by uncertainty generated in the relations. We first explain the indicator, and then review possible results when this indicator is applied to (i) co-author networks of academic, industrial, and governmental authors and (ii) synergies in the distributions of firms over geographical addresses, technological classes, and industrial-size classes for a number of nations. Co-variation is then considered as a measure of relationship. The balance between globalizing and localizing dynamics can be quantified. Too much synergy locally can also be considered as lock-in. Tendencies are different for the globalizing knowledge dynamics versus locally retaining wealth from knowledge in industrial innovations."
"The 5th Generation cellular network may have the key feature of smaller cell size and denser resource employment, resulted from diminishing resource and increasing communication demands. However, small cell may result in high interference between cells. Moreover, the random geographic patterns of small cell networks make them hard to analyze, at least excluding schemes in the well-accepted hexagonal grid model. In this paper, a new model---the matrix graph is proposed which takes advantage of the small cell size and high inter-cell interference to reduce computation complexity. This model can simulate real world networks accurately and offers convenience in frequency allocation problems which are usually NP-complete. An algorithm dealing with this model is also given, which asymptotically achieves the theoretical limit of frequency allocation, and has a complexity which decreases with cell size and grows linearly with the network size. This new model is specifically proposed to characterize the next-generation cellular networks."
"In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation."
"Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data."
"Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights inside the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest ways to harness it to create more robust representations."
"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end deep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words. In this paper we proposed a method which combines the general pre-trained word embedding vectors with those generated on the task-specific training set to address this issue. We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags."
"In this work, lossy distributed compression of pairs of correlated sources is considered. Conventionally, Shannon's random coding arguments -- using randomly generated unstructured codebooks whose blocklength is taken to be asymptotically large -- are used to derive achievability results. However, it was recently observed that in various multi-terminal communications scenarios, using random codes with constant finite blocklength may lead to improved achievable regions compared to the conventional approach. In other words, in some network communication scenarios, there is a finite optimal value in the blocklength of the randomly generated code used for distributed processing of information sources. Motivated by this, a coding scheme is proposed which consists of two codebook layers: i) the primary codebook which has constant finite blocklength, and ii) the secondary codebook whose blocklength is taken to be asymptotically large. The achievable region is analyzed in two steps. In the first step, a characterization of the achievable region is derived using information measures which are functions of multi-letter probability distributions. In the next step, a computable single-letter inner-bound to the achievable region is derived. It is shown through several examples that the resulting rate-distortion region is strictly larger than the Berger Tung achievable region."
"One of the fundamental questions of cultural evolutionary research is how individual-level processes scale up to generate population-level patterns. Previous studies in music have revealed that frequency-based bias (e.g. conformity and novelty) drives large-scale cultural diversity in different ways across domains and levels of analysis. Music sampling is an ideal research model for this process because samples are known to be culturally transmitted between collaborating artists, and sampling events are reliably documented in online databases. The aim of the current study was to determine whether frequency-based bias has played a role in the cultural transmission of music sampling traditions, using a longitudinal dataset of sampling events across three decades. Firstly, we assessed whether turn-over rates of popular samples differ from those expected under neutral evolution. Next, we used agent-based simulations in an approximate Bayesian computation framework to infer what level of frequency-based bias likely generated the observed data. Despite anecdotal evidence of novelty bias, we found that sampling patterns at the population-level are most consistent with conformity bias."
"We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach."
"We prove that universal quantum computation can be realized---using only linear optics and $\chi^{(2)}$ (three-wave mixing) interactions---in any $(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we exhibit a strictly universal gate set for the qubit basis in the one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by proving that $\chi^{(2)}$ Hamiltonians and photon-number operators generate the full $\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing how the qutrit controlled-$Z$ gate can be implemented with only linear optics and $\chi^{(2)}$ interactions. We then use proof by induction to obtain our general qudit result. Our induction proof relies on coherent photon injection/subtraction, a technique enabled by $\chi^{(2)}$ interaction between the encoding modes and ancillary modes. Finally, we show that coherent photon injection is more than a conceptual tool in that it offers a route to preparing high-photon-number Fock states from single-photon Fock states."
"Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks."
"We compute a general expression for the contribution of vector perturbations to the redshift-space distortion of galaxy surveys. We show that they contribute to the same multipoles of the correlation function as scalar perturbations and should thus in principle be taken into account in data analysis. We derive constraints for next-generation surveys on the amplitude of two sources of vector perturbations, namely non-linear clustering and topological defects. While topological defects leave a very small imprint on redshift-space distortions, we show that the multipoles of the correlation function are sensitive to vorticity induced by non-linear clustering. Therefore future redshift surveys such as DESI or the SKA should be capable of measuring such vector modes, especially with the hexadecapole which appears to be the most sensitive to the presence of vorticity."
"The goal of this work is to improve images of traffic scenes that are degraded by natural causes such as fog, rain and limited visibility during the night. For these applications, it is next to impossible to get pixel perfect pairs of the same scene, with and without the degrading conditions. This makes it unsuitable for conventional supervised learning approaches, however, it is easy to collect unpaired images of the scenes in a perfect and in a degraded condition. To enhance the images taken in a poor visibility condition, domain transfer models can be trained to transform an image from the degraded to the clear domain. A well-known concept for unsupervised domain transfer are cycle-consistent generative adversarial models. Unfortunately, the resulting generators often change the structure of the scene. This causes an undesirable change in the semantics. We propose three ways to cope with this problem depending on the type of degradation."
"Learning a goal-oriented dialog policy is generally performed offline with supervised learning algorithms or online with reinforcement learning (RL). Additionally, as companies accumulate massive quantities of dialog transcripts between customers and trained human agents, encoder-decoder methods have gained popularity as agent utterances can be directly treated as supervision without the need for utterance-level annotations. However, one potential drawback of such approaches is that they myopically generate the next agent utterance without regard for dialog-level considerations. To resolve this concern, this paper describes an offline RL method for learning from unannotated corpora that can optimize a goal-oriented policy at both the utterance and dialog level. We introduce a novel reward function and use both on-policy and off-policy policy gradient to learn a policy offline without requiring online user interaction or an explicit state space definition."
"Nearly all standard force fields employ the 'sum-of-spheres' approximation, which models intermolecular interactions purely in terms of interatomic distances. Nonetheless, atoms in molecules can have significantly non-spherical shapes, leading to interatomic interaction energies with strong orientation dependencies. Neglecting this 'atomic-level anisotropy' can lead to significant errors in predicting interaction energies. Herein we propose a simple, transferable, and computationally-efficient model (MASTIFF) whereby atomic-level orientation dependence can be incorporated into ab initio intermolecular force fields. MASTIFF includes anisotropic exchange-repulsion, charge penetration, and dispersion effects, in conjunction with a standard treatment of anisotropic long-range (multipolar) electrostatics. To validate our approach, we benchmark MASTIFF against various sum-of-spheres models over a large library of intermolecular interactions between small organic molecules. MASTIFF achieves quantitative accuracy with respect to both high-level electronic structure theory and experiment, thus showing promise as a basis for 'next-generation' force field development."
"The aim of the article is to describe the problems of personnel training that arise in view of extension of the STEM approach to education, development of innovative technologies, in particular, virtualization, augmented reality, the use of ICT outsourcing in educational systems design. The object of research is the process of formation and development of the educational and scientific envi- ronment of educational institution. The subject of the study is the formation and development of the cloud-based learning and research environment for STEM education. The methods of research are: the analysis of publications on the prob- lem, generalization of domestic and foreign experience, theoretical analysis, sys- tem analysis, systematization and generalization of research facts and laws for the development and design of the model of the cloud-based learning environ- ment, substantiation of the main conclusions. The results of the research are the next: the concepts and the model of the cloud-based environment of STEM edu- cation is substantiated, the problems of personnel training at the present stage are outlined."
"Existing video prediction methods mainly rely on observing multiple historical frames or focus on predicting the next one-frame. In this work, we study the problem of generating consecutive multiple future frames by observing one single still image only. We formulate the multi-frame prediction task as a multiple time step flow (multi-flow) prediction phase followed by a flow-to-frame synthesis phase. The multi-flow prediction is modeled in a variational probabilistic manner with spatial-temporal relationships learned through 3D convolutions. The flow-to-frame synthesis is modeled as a generative process in order to keep the predicted results lying closer to the manifold shape of real video sequence. Such a two-phase design prevents the model from directly looking at the high-dimensional pixel space of the frame sequence and is demonstrated to be more effective in predicting better and diverse results. Extensive experimental results on videos with different types of motion show that the proposed algorithm performs favorably against existing methods in terms of quality, diversity and human perceptual evaluation."
"Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving."
"We compute the production cross section of a top-antitop pair in association with a jet at hadron colliders at next-to-leading order accuracy matched with parton shower algorithms to make predictions at the hadron level. The parton shower allows for including the decay of the top quarks at the leading order accuracy. We use a framework based on three well established numerical codes, the POWHEG-BOX, used for the calculation of the cross section, HELAC, which generates the matrix elements for the Born-level, real emission and the virtual part, and finally a parton shower program, such as PYTHIA or HERWIG, which generate the parton-shower and hadronization."
"We present a decomposition of the hadronic tensor for a general polarized Drell-Yan process AB --> l^+ l^- X in terms of helicity structure functions using the helicity axes of the dilepton rest frame as a basis. Next, we consider a QCD parton model and in the framework of a generalized QCD factorization scheme, which applies when the dilepton invariant mass M is much larger than the transverse component of the photon momentum, q_T, in the hadronic center of mass frame. In this approximation we compute the angular distribution of the unpolarized Drell Yan cross section and of the Sivers effect, in single polarized Drell-Yan scattering, by taking into account the transverse motion of partons inside the initial hadrons, k_\perp. Interesting and simple results are found in the kinematical region of k_\perp similar to q_T << M, to first order in a q_T / M expansion. Finally, explicit analytical expressions, convenient for phenomenological studies, are obtained assuming a factorized Gaussian dependence on intrinsic momenta for the unpolarized and Sivers distribution functions, in analogy to those obtained for semi-inclusive deep inelastic scattering."
"As digital medical imaging becomes more prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in the biomedical literature, which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature space evaluated using images from the ImageCLEF 2017 concept detection task. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches, in contrast with previously popular computer vision methods. Although generative adversarial networks can provide good results, they are harder to succeed in highly varied data sets. The possibility of semi-supervised learning, as well as their use in medical information retrieval problems, are the next steps to be strongly considered."
"We present a new six-parameter family of potentials whose solutions are expressed in terms of the hypergeometric functions 3F2, 2F2 and 1F2. Both the scattering data and the bound states of these potentials are explicitly computed and the peculiar properties of the discrete spectrum are depicted in a suitable phase diagram. Our starting point is a third-order formal eigenvalue equation of the hypergeometric type (with a suitable solution known) which is transformed to the Schr\""odinger equation by applying the reduction of order technique as the crucial first step. The general preconditions allowing for the reduction to Schr\""odinger form of an arbitrary eigenvalue equation of higher order, are discussed at the end of the article, and two universal features of the potentials arising this way are also stated and discussed. In this general scheme the Natanzon potentials are the simplest special case, those presented here the next ones, and so on for potentials arising from equations of fourth or higher order."
"This paper presents a unified treatment of Gaussian process models that extends to data from the exponential dispersion family and to survival data. Our specific interest is in the analysis of data sets with predictors that have an a priori unknown form of possibly nonlinear associations to the response. The modeling approach we describe incorporates Gaussian processes in a generalized linear model framework to obtain a class of nonparametric regression models where the covariance matrix depends on the predictors. We consider, in particular, continuous, categorical and count responses. We also look into models that account for survival outcomes. We explore alternative covariance formulations for the Gaussian process prior and demonstrate the flexibility of the construction. Next, we focus on the important problem of selecting variables from the set of possible predictors and describe a general framework that employs mixture priors. We compare alternative MCMC strategies for posterior inference and achieve a computationally efficient and practical approach. We demonstrate performances on simulated and benchmark data sets."
"We calculate the linear vacuum perturbations of a Kerr black hole surrounded by a slowly-varying external spacetime to third order in the ratio of the black-hole mass to the radius of curvature of the external spacetime. This expansion applies to two relevant physical scenarios: (i) a small Kerr black hole immersed in the gravitational field of a much larger external black hole, and (ii) a Kerr black hole moving slowly around another external black hole of comparable mass. This small-hole/slow-motion approximation allows us to parametrize the perturbation through slowly-varying, time-dependent electric and magnetic tidal tensors, which then enables us to separate the Teukolsky equation and compute the Newman-Penrose scalar analytically to third order in our expansion parameter. We obtain generic expressions for the mass and angular momentum flux through the perturbed black hole horizon, as well as the rate of change of the horizon surface area, in terms of certain invariants constructed from the electric and magnetic tidal tensors. We conclude by applying these results to the two scenarios described above."
"In this letter we carry out the first systematic investigation of the expected gravitational wave (GW) background generated by supermassive black hole (SMBH) binaries in the nHz frequency band accessible to pulsar timing arrays (PTAs). We take from the literature several estimates of the redshift dependent galaxy mass function and of the fraction of close galaxy pairs to derive a wide range of galaxy merger rates. We then exploit empirical black hole-host relations to populate merging galaxies with SMBHs. The result of our procedure is a collection of a large number of phenomenological SMBH binary merger rates consistent with current observational constraints on the galaxy assembly at z<1.5. For each merger rate we compute the associated GW signal, eventually producing a large set of estimates of the nHz GW background that we use to infer confidence intervals of its expected amplitude. When considering the most recent SMBH-host relations, accounting for ultra-massive black holes in brightest cluster galaxies, we find that the nominal $1\sigma$ interval of the expected GW signal is only a factor of 3-to-10 below current PTA limits, implying a non negligible chance of detection in the next few years."
"We introduce the computer code Recola for the recursive generation of tree-level and one-loop amplitudes in the Standard Model. Tree-level amplitudes are constructed using off-shell currents instead of Feynman diagrams as basic building blocks. One-loop amplitudes are represented as linear combinations of tensor integrals whose coefficients are calculated similarly to the tree-level amplitudes by recursive construction of loop off-shell currents. We introduce a novel algorithm for the treatment of colour, assigning a colour structure to each off-shell current which enables us to recursively construct the colour structure of the amplitude efficiently. Recola is interfaced with a tensor-integral library and provides complete one-loop Standard Model amplitudes including rational terms and counterterms. As a first application we consider Z+2jets production at the LHC and calculate with Recola the next-to-leading-order electroweak corrections to the dominant partonic channels."
"Network-assisted device-to-device communication is a promising technology for improving the performance of proximity-based services. This paper demonstrates how the integration of device-to-device communications and dynamic time-division duplex can improve the energy efficiency of future cellular networks, leading to a greener system operation and a prolonged battery lifetime of mobile devices. We jointly optimize the mode selection, transmission period and power allocation to minimize the energy consumption (from both a system and a device perspective) while satisfying a certain rate requirement. The radio resource management problems are formulated as mixed-integer nonlinear programming problems. Although they are known to be NP-hard in general, we exploit the problem structure to design efficient algorithms that optimally solve several problem cases. For the remaining cases, a heuristic algorithm that computes near-optimal solutions while respecting practical constraints on execution times and signaling overhead is also proposed. Simulation results confirm that the combination of device-to-device and flexible time-division-duplex technologies can significantly enhance spectrum and energy-efficiency of next generation cellular systems."
"We present a novel architecture, the ""stacked what-where auto-encoders"" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the ""what"" which are fed to the next layer, and its complementary variable ""where"" that are fed to the corresponding layer in the generative decoder."
"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
"We introduce a new parameter to discuss the behavior of a genetic algorithm. This parameter is the mean number of exact copies of the best fit chromosomes from one generation to the next. We argue that the genetic algorithm should operate efficiently when this parameter is slightly larger than $1$. We consider the case of the simple genetic algorithm with the roulette--wheel selection mechanism. We denote by $\ell$ the length of the chromosomes, by $m$ the population size, by $p_C$ the crossover probability and by $p_M$ the mutation probability. We start the genetic algorithm with an initial population whose maximal fitness is equal to $f_0^*$ and whose mean fitness is equal to ${\overline{f_0}}$. We show that, in the limit of large populations, the dynamics of the genetic algorithm depends in a critical way on the parameter $\pi \,=\,\big({f_0^*}/{\overline{f_0}}\big) (1-p_C)(1-p_M)^\ell\,.$ Our results suggest that the mutation and crossover probabilities should be tuned so that, at each generation, $\text{maximal fitness} \times (1-p_C) (1-p_M)^\ell > \text{mean fitness}$."
"Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications."
"We apply recent advances in machine learning and computer vision to a central problem in materials informatics: The statistical representation of microstructural images. We use activations in a pre-trained convolutional neural network to provide a high-dimensional characterization of a set of synthetic microstructural images. Next, we use manifold learning to obtain a low-dimensional embedding of this statistical characterization. We show that the low-dimensional embedding extracts the parameters used to generate the images. According to a variety of metrics, the convolutional neural network method yields dramatically better embeddings than the analogous method derived from two-point correlations alone."
"Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation."
"We construct the covariant effective field theory of gravity as an expansion in inverse powers of the Planck mass, identifying the leading and next-to-leading quantum corrections. We determine the form of the effective action for the cases of pure gravity with cosmological constant as well as gravity coupled to matter. By means of heat kernel methods we renormalize and compute the leading quantum corrections to quadratic order in a curvature expansion. The final effective action in our covariant formalism is generally non-local and can be readily used to understand the phenomenology on different spacetimes. In particular, we point out that on curved backgrounds the observable leading quantum gravitational effects are less suppressed than on Minkowski spacetime."
"We compute the width of the decay $h^0$ (125 GeV) $\to b \bar{b}$ at next-to-leading order in the general MSSM with quark-flavour violation (QFV). We study the effect of mixing between the second and the third generation of squarks, taking into account the constraints on QFV from B-meson data. We discuss the renormalisation of the process as well as the resummation of the bottom Yukawa coupling at large $\tan \beta$. We show numerical results on the decay width $\Gamma(h^0 \to b \bar{b})$ as a function of the involved QFV parameters and compare them with the corresponding width in the Standard Model."
"Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some tasks, this assumption is undesirable. For example, when performing entity resolution, the size of each cluster is often unrelated to the size of the data set. Consequently, each cluster contains a negligible fraction of the total number of data points. Such tasks therefore require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the \emph{microclustering property} and introducing a new model that exhibits this property. We compare this model to several commonly used clustering models by checking model fit using real and simulated data sets."
"We define in this paper a class of three indices tensor models, endowed with $O(N)^{\otimes 3}$ invariance ($N$ being the size of the tensor). This allows to generate, via the usual QFT perturbative expansion, a class of Feynman tensor graphs which is strictly larger than the class of Feynman graphs of both the multi-orientable model (and hence of the colored model) and the $U(N)$ invariant models. We first exhibit the existence of a large $N$ expansion for such a model with general interactions. We then focus on the quartic model and we identify the leading and next-to-leading order (NLO) graphs of the large $N$ expansion. Finally, we prove the existence of a critical regime and we compute the critical exponents, both at leading order and at NLO. This is achieved through the use of various analytic combinatorics techniques."
"Face detection is challenging as faces in images could be present at arbitrary locations and in different scales. We propose a three-stage cascade structure based on fully convolutional neural networks (FCNs). It first proposes the approximate locations where the faces may be, then aims to find the accurate location by zooming on to the faces. Each level of the FCN cascade is a multi-scale fully-convolutional network, which generates scores at different locations and in different scales. A score map is generated after each FCN stage. Probable regions of face are selected and fed to the next stage. The number of proposals is decreased after each level, and the areas of regions are decreased to more precisely fit the face. Compared to passing proposals directly between stages, passing probable regions can decrease the number of proposals and reduce the cases where first stage doesn't propose good bounding boxes. We show that by using FCN and score map, the FCN cascade face detector can achieve strong performance on public datasets."
"There is need for several software systems within the energy domain and corresponding systems are being developed to satisfy these needs. These systems include energy monitoring, information, wide area monitoring and control systems, and SCADA systems. Energy monitoring systems are one of the most important and common systems among them. In this study, after briefly reviewing several of the software systems within the energy domain, a centralized and generic software architecture for energy monitoring systems is presented. Next, sample projects are described in which energy monitoring systems based on this architecture have been implemented. We envisage that this study will be an important resource for software projects in the energy domain."
"We propose a scalable approach to learn video-based question answering (QA): answer a ""free-form natural language question"" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines."
"In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, its implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values."
"The rates for generating a matter-antimatter asymmetry in extensions of the Standard Model (SM) containing right-handed neutrinos are the most interesting and least trivial coefficients in the rate equations for baryogenesis through thermal leptogenesis. We obtain a relation of these rates to finite-temperature real-time correlation functions, similar to the Kubo formulas for transport coefficients. Then we consider the case of hierarchical masses for the sterile neutrinos. At leading order in their Yukawa couplings we find a simple master formula which relates the rates to a single finite temperature three-point spectral function. It is valid to all orders in $g$,where $g$ denotes a SM gauge or quark Yukawa coupling. We use it to compute the rate for generating a matter-antimatter asymmetry at next-to-leading order in g in the non-relativistic regime. The corrections are of order $g ^ 2$, and they amount to 4% or less."
"Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation."
"This paper is devoted to the computations of some relevant quantities associated with the free unitary Brownian motion. Using the Lagrange inversion formula, we first derive an explicit expression for its alternating star cumulants of even lengths and relate them to those having odd lengths by means of a summation formula for the free cumulants with product as entries. Next, we use again this formula together with a generating series for Laguerre polynomials in order to compute the Taylor coefficients of the reciprocal of the $R$-transform of the free Jacobi process associated with a single projection of rank $1/2$ and those of the $S$-transform as well. This generating series lead also to the Taylor expansions of the Schur function of the spectral distribution of the free unitary Brownian motion and of its first iterate."
"We create synthetic biometric databases to study general, fundamental, biometric principles. First, we check the validity of the synthetic database design by comparing it to real data in terms of biometric performance. The real data used for this validity check was from an eye-movement related biometric database. Next, we employ our database to evaluate the impact of variations of temporal persistence of features on biometric performance. We index temporal persistence with the intraclass correlation coefficient (ICC). We find that variations in temporal persistence are extremely highly correlated with variations in biometric performance. Finally, we use our synthetic database strategy to determine how many features are required to achieve particular levels of performance as the number of subjects in the database increases from 100 to 10,000. An important finding is that the number of features required to achieve various EER values (2%, 0.3%, 0.15%) is essentially constant in the database sizes that we studied. We hypothesize that the insights obtained from our study would be applicable to many biometric modalities where extracted feature properties resemble the properties of the synthetic features we discuss in this work."
"Power systems are increasingly operated in corrective rather than preventive security mode, which means that appropriate control actions must be taken immediately after a contingency has occurred. This paper proposes an online algorithm for automatically alleviating contingencies such as voltage limit violations and line overloads. Unlike previously proposed approaches, the network itself serves as a natural solver of the power flow equations. This makes it possible to start the implementation immediately and avoids problems caused by modeling errors. Every time the controller receives measurements from the grid, it evaluates the presence of contingencies and computes the optimal corrective actions that can be implemented before the next sampling period, subject to ramping constraints of the generators. These corrective actions are implemented through the standard Automatic Generation Control. Finding the optimal incremental corrective actions is fast because this problem is linearized. The effectiveness of this algorithm at correcting both line overloads and voltage violations is demonstrated using the IEEE-118 Bus test system."
"This work presents the evolution of a solution for predictive maintenance to a Big Data environment. The proposed adaptation aims for predicting failures on wind turbines using a data-driven solution deployed in the cloud and which is composed by three main modules. (i) A predictive model generator which generates predictive models for each monitored wind turbine by means of Random Forest algorithm. (ii) A monitoring agent that makes predictions every 10 minutes about failures in wind turbines during the next hour. Finally, (iii) a dashboard where given predictions can be visualized. To implement the solution Apache Spark, Apache Kafka, Apache Mesos and HDFS have been used. Therefore, we have improved the previous work in terms of data process speed, scalability and automation. In addition, we have provided fault-tolerant functionality with a centralized access point from where the status of all the wind turbines of a company localized all over the world can be monitored, reducing O&M costs."
"In this work, we compute analytically the infrared divergences of massless O($N$) self-interacting scalar field theories with Lorentz violation, which are exact in the Lorentz-violating $K_{\mu\nu}$ coefficients, for evaluating the corresponding next-to-leading order critical exponents. For that, we apply three distinct and independent field-theoretic renormalization group methods. We find that the outcomes for the critical exponents are the same in the three methods and, furthermore, are identical to their Lorentz invariant counterparts. We generalize the results for all loop levels by employing a general theorem arising from the exact procedure and give the corresponding physical interpretation."
"Generative adversarial networks (GANs) transform low-dimensional latent vectors into visually plausible images. If the real dataset contains only clean images, then ostensibly, the manifold learned by the GAN should contain only clean images. In this paper, we propose to denoise corrupted images by finding the nearest point on the GAN manifold, recovering latent vectors by minimizing distances in image space. We first demonstrate that given a corrupted version of an image that truly lies on the GAN manifold, we can approximately recover the latent vector and denoise the image, obtaining significantly higher quality, comparing with BM3D. Next, we demonstrate that latent vectors recovered from noisy images exhibit a consistent bias. By subtracting this bias before projecting back to image space, we improve denoising results even further. Finally, even for unseen images, our method performs better at denoising better than BM3D. Notably, the basic version of our method (without bias correction) requires no prior knowledge on the noise variance. To achieve the highest possible denoising quality, the best performing signal processing based methods, such as BM3D, require an estimate of the blur kernel."
"We present a port of the numerical relativity code SpEC which is capable of running on NVIDIA GPUs. Since this code must be maintained in parallel with SpEC itself, a primary design consideration is to perform as few explicit code changes as possible. We therefore rely on a hierarchy of automated porting strategies. At the highest level we use TLoops, a C++ library of our design, to automatically emit CUDA code equivalent to tensorial expressions written into C++ source using a syntax similar to analytic calculation. Next, we trace out and cache explicit matrix representations of the numerous linear transformations in the SpEC code, which allows these to be performed on the GPU using pre-existing matrix-multiplication libraries. We port the few remaining important modules by hand. In this paper we detail the specifics of our port, and present benchmarks of it simulating isolated black hole spacetimes on several generations of NVIDIA GPU."
"Most generative document models act on bag-of-words input in an attempt to focus on the semantic content and thereby partially forego syntactic information. We argue that it is preferable to keep the original word order intact and explicitly account for the syntactic structure instead. We propose an extension to the Neural Variational Document Model (Miao et al., 2016) that does exactly that to separate local (syntactic) context from the global (semantic) representation of the document. Our model builds on the variational autoencoder framework to define a generative document model based on next-word prediction. We name our approach Sequence-Aware Variational Autoencoder since in contrast to its predecessor, it operates on the true input sequence. In a series of experiments we observe stronger topicality of the learned representations as well as increased robustness to syntactic noise in our training data."
"State-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance, particularly in scenarios with fine-grained boundaries between categories. To this end, we propose a multimodal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multimodal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multimodal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose a framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection. We show the results of our proposed discriminative hallucinated method for 1-, 2-, and 5- shot learning on the CUB dataset, where the accuracy is improved by employing multimodal data."
"A large number of objectives have been proposed to train latent variable generative models. We show that many of them are Lagrangian dual functions of the same primal optimization problem. The primal problem optimizes the mutual information between latent and visible variables, subject to the constraints of accurately modeling the data distribution and performing correct amortized inference. By choosing to maximize or minimize mutual information, and choosing different Lagrange multipliers, we obtain different objectives including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB, AS-VAE and InfoVAE. Based on this observation, we provide an exhaustive characterization of the statistical and computational trade-offs made by all the training objectives in this class of Lagrangian duals. Next, we propose a dual optimization method where we optimize model parameters as well as the Lagrange multipliers. This method achieves Pareto optimal solutions in terms of optimizing information and satisfying the constraints."
"Minimal linear codes have significant applications in secret sharing schemes and secure two-party computation. There are several methods to construct linear codes, one of which is based on functions over finite fields. Recently, many construction methods of linear codes based on functions have been proposed in the literature. In this paper, we generalize the recent construction methods given by Tang et al. in [IEEE Transactions on Information Theory, 62(3), 1166-1176, 2016] to weakly regular plateaued functions over finite fields of odd characteristic. We first construct three weight linear codes from weakly regular plateaued functions based on the second generic construction and determine their weight distributions. We next give a subcode with two or three weights of each constructed code as well as its parameter. We finally show that the constructed codes in this paper are minimal, which confirms that the secret sharing schemes based on their dual codes have the nice access structures."
"This paper describes the Direct Fourier Permuation Algorithm, an efficient method of computing Bit Reversal of natural indices [1, 2, 3, ..., 2^k] in a vectorial manner (k iterations) and also proposes the Vectorial Digit Reversal Algorithm, a natural generalization of Direct Fourier Permutation Algorithm that is enabled to compute the r-digit reversal of natural indices [1, 2, 3, ..., r^k] where r is an arbitrary radix. Matlab functions implementing these two algo- rithms and various test and comparative results are presented in this paper to support the idea of inclusion of these two algorithms in the next Matlab Signal Processing Toolbox official distribution package as much faster alternatives to current Matlab functions bitrevorder and digitrevorder."
"Urban transportation of next decade is expected to be disrupted by Autonomous Mobility on Demand (AMoD): AMoD providers will collect ride requests from users and will dispatch a fleet of autonomous vehicles to satisfy requests in the most efficient way. Differently from current ride sharing systems, in which driver behavior has a clear impact on the system, AMoD systems will be exclusively determined by the dispatching logic. As a consequence, a recent interest in the Operations Research and Computer Science communities has focused on this control logic. The new propositions and methodologies are generally evaluated via simulation. Unfortunately, there is no simulation platform that has emerged as reference, with the consequence that each author uses her own custom-made simulator, applicable only in her specific study, with no aim of generalization and without public release. This slows down the progress in the area as researchers cannot build on each other's work and cannot share, reproduce and verify the results. The goal of this paper is to present AMoDSim, an open-source simulation platform aimed to fill this gap and accelerate research in future ride sharing systems."
"Gravitational wave astronomy has placed strong constraints on fundamental physics, and there is every expectation that future observations will continue to do so. In this work we quantify this expectation for future binary merger observations to constrain hidden sectors, such as scalar-tensor gravity or dark matter, which induce a Yukawa-type modification to the gravitational potential. We explicitly compute the gravitational waveform, and perform a Fisher information matrix analysis to estimate the sensitivity of next generation gravitational wave detectors to these modifications. We find an optimal sensitivity to the Yukawa interaction strength of $10^{-5}$ and to the associated dipole emission parameter of $10^{-7}$, with the best constraints arising from the Einstein Telescope. When applied to a minimal model of dark matter, this provides an exquisite probe of dark matter accumulation by neutron stars, and for sub-TeV dark matter gravitational waves are able to detect mass fractions $m_{DM}/m_{NS}$ less then 1 part in $10^{15}$."
"Recommendation systems that automatically generate personalized music playlists for users have attracted tremendous attention in recent years. Nowadays, most music recommendation systems rely on item-based or user-based collaborative filtering or content-based approaches. In this paper, we propose a novel mixture hidden Markov model (HMM) for music play sequence prediction. We compare the mixture model with state-of-the-art methods and evaluate the predictions quantitatively and qualitatively on a large-scale real-world dataset in a Kaggle competition. Results show that our model significantly outperforms traditional methods as well as other competitors. We conclude by envisioning a next-generation music recommendation system that integrates our model with recent advances in deep learning, computer vision, and speech techniques, and has promising potential in both academia and industry."
"We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training."
"The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features of high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the transmitted intermediate layers. We also present the results for evaluation of lossless deep feature compression with four benchmark data compression methods, which provides meaningful investigations and baselines for future research and standardization activities."
"Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry. Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection. In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy."
"Discourse coherence plays an important role in the translation of one text. However, the previous reported models most focus on improving performance over individual sentence while ignoring cross-sentence links and dependencies, which affects the coherence of the text. In this paper, we propose to use discourse context and reward to refine the translation quality from the discourse perspective. In particular, we generate the translation of individual sentences at first. Next, we deliberate the preliminary produced translations, and train the model to learn the policy that produces discourse coherent text by a reward teacher. Practical results on multiple discourse test datasets indicate that our model significantly improves the translation quality over the state-of-the-art baseline system by +1.23 BLEU score. Moreover, our model generates more discourse coherent text and obtains +2.2 BLEU improvements when evaluated by discourse metrics."
"Amorphous materials are coming within reach of realistic computer simulations, but new approaches are needed to fully understand their intricate atomic structures. Here, we show how machine-learning (ML)-based techniques can give new, quantitative chemical insight into the atomic-scale structure of amorphous silicon (a-Si). Based on a similarity function (""kernel""), we define a structural metric that unifies the description of nearest- and next-nearest-neighbor environments in the amorphous state. We apply this to an ensemble of a-Si networks, generated in melt-quench simulations with an ML-based interatomic potential, in which we tailor the degree of ordering by varying the quench rates down to $10^{10}$ K/s (leading to a structural model that is lower in energy than the established WWW network). We then show how ""machine-learned"" atomic energies permit a chemical interpretation, associating coordination defects in a-Si with distinct energetic stability regions. The approach is straightforward and inexpensive to apply to arbitrary structural models, and it is therefore expected to have more general significance for developing a quantitative understanding of the amorphous state."
"We present a novel analysis of semidefinite programs (SDPs) with positive duality gaps, i.e. different optimal values in the primal and dual problems. These SDPs are extremely pathological, often unsolvable, and also serve as models of more general pathological convex programs. However, despite their allure, they are not well understood even when they have just two variables.   We first completely characterize two variable SDPs with positive gaps; in particular, we transform them into a standard form that makes the positive gap trivial to recognize. The transformation is very simple, as it mostly uses elementary row operations coming from Gaussian elimination. We next show that the two variable case sheds light on larger SDPs with positive gaps: we present SDPs in any dimension in which the positive gap is caused by the same structure as in the two variable case. We analyze a fundamental parameter, the {\em singularity degree} of the duals of our SDPs, and show that it is the largest that can result in a positive gap.   We finally generate a library of difficult SDPs with positive gaps (some of these SDPs have only two variables) and present a computational study."
"Observational data from the ESA astrometric mission Gaia determining the positions of celestial objects within an accuracy of few microarcseconds will be soon fully available. Other satellite-based space missions are currently planned to significantly improve such precision in the next years. The data reduction process needs high-precision general relativistic models, allowing one to solve the inverse ray-tracing problem in the gravitational field of the Solar System up to the requested level of accuracy and leading then to the estimate of astrometric parameters. Besides a satisfactory description of the background field due to the planets (which should include their multipolar structure), one should consider also other effects which may induce modifications to the light propagation. For instance, the interaction of the light signal with the superposed gravitational field of a gravitational wave emitted by a distant source would cause a shift in the apparent positions of the stars. We compute here the main astrometric observables needed for data reduction of satellite-based missions in the presence of a passing plane gravitational wave. We also take into account the effect of the mass quadrupole moment of the planets, improving previous results obtained for Gaia."
"We prove a version of the Cohen--Lenstra conjecture over function fields (completing the results of our prior paper). This is deduced from two more general theorems, one topological, one arithmetic: We compute the direct limit of homology, over puncture-stabilization, of spaces of maps from a punctured manifold to a fixed target; and we compute the Galois action on the set of stable components of Hurwitz schemes."
"End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation."
This paper presents a novel framework for predicting shot location and type in tennis. Inspired by recent neuroscience discoveries we incorporate neural memory modules to model the episodic and semantic memory components of a tennis player. We propose a Semi Supervised Generative Adversarial Network architecture that couples these memory models with the automatic feature learning power of deep neural networks and demonstrate methodologies for learning player level behavioural patterns with the proposed framework. We evaluate the effectiveness of the proposed model on tennis tracking data from the 2012 Australian Tennis open and exhibit applications of the proposed method in discovering how players adapt their style depending on the match context.
"We present DREAM, the first dialogue-based multiple-choice reading comprehension dataset. Collected from English-as-a-foreign-language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our dataset contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.   We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM dataset show the effectiveness of dialogue structure and general world knowledge. DREAM will be available at https://dataset.org/dream/."
"With the overwhelming amount of complex and heterogeneous data pouring from any-where, any-time, and any-device, there is undeniably an era of Big Data. The emergence of the Big Data as a disruptive technology for next generation of intelligent systems, has brought many issues of how to extract and make use of the knowledge obtained from the data within short times, limited budget and under high rates of data generation. Companies are recognizing that big data can be used to make more accurate predictions, and can be used to enhance the business with the help of appropriate association rule mining algorithm. To help these organizations, with which software and algorithm is more appropriate for them depending on their dataset, we compared the most famous three MapReduce based software Hadoop, Spark, Flink on two widely used algorithms Apriori and Fp-Growth on different scales of dataset."
"The next generation of aircraft collision avoidance systems frame the problem as a Markov decision process and use dynamic programming to optimize the alerting logic. The resulting system uses a large lookup table to determine advisories given to pilots, but these tables can grow very large. To enable the system to operate on limited hardware, prior work investigated compressing the table using a deep neural network. However, ensuring that the neural network reliably issues safe advisories is important for certification. This work defines linearized regions where each advisory can be safely provided, allowing Reluplex, a neural network verification tool, to check if unsafe advisories are ever issued. A notional collision avoidance policy is generated and used to train a neural network representation. The neural networks are checked for unsafe advisories, resulting in the discovery of thousands of unsafe counterexamples."
"Resource allocation problems are a family of problems in which resources must be selected to satisfy given demands. This paper focuses on the two-stage stochastic generalization of resource allocation problems where future demands are expressed in a finite number of possible scenarios. The goal is to select cost effective resources to be acquired in the present time (first stage), and to implement a complete solution for each scenario (second stage), while minimizing the total expected cost of the choices in both stages.   We propose an evolutionary framework for solving general two-stage stochastic resource allocation problems. In each iteration of our framework, a local search algorithm selects resources to be acquired in the first stage. A genetic metaheuristic then completes the solutions for each scenario and relevant information is passed onto the next iteration, thereby supporting the acquisition of promising resources in the following first stage. Experimentation on numerous instances of the two-stage stochastic Steiner tree problem suggests that our evolutionary framework is powerful enough to address large instances of a wide variety of two-stage stochastic resource allocation problems."
"Temporally locating and classifying action segments in long untrimmed videos is of particular interest to many applications like surveillance and robotics. While traditional approaches follow a two-step pipeline, by generating frame-wise probabilities and then feeding them to high-level temporal models, recent approaches use temporal convolutions to directly classify the video frames. In this paper, we introduce a multi-stage architecture for the temporal action segmentation task. Each stage features a set of dilated temporal convolutions to generate an initial prediction that is refined by the next one. This architecture is trained using a combination of a classification loss and a proposed smoothing loss that penalizes over-segmentation errors. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our model achieves state-of-the-art results on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset."
"Recent advances in haptic hardware and software technology have generated interest in novel, multimodal interfaces based on the sense of touch. Such interfaces have the potential to revolutionize the way we think about human computer interaction and open new possibilities for simulation and training in a variety of fields. In this paper we review several frameworks, APIs and toolkits for haptic user interface development. We explore these software components focusing on minimally invasive surgical simulation systems. In the area of medical diagnosis, there is a strong need to determine mechanical properties of biological tissue for both histological and pathological considerations. Therefore we focus on the development of affordable visuo-haptic simulators to improve practice-based education in this area. We envision such systems, designed for the next generations of learners that enhance their knowledge in connection with real-life situations while they train in mandatory safety conditions."
"We consider an attacker-operator game for monitoring a large-scale network that is comprised on components that differ in their criticality levels. In this zero-sum game, the operator seeks to position a limited number of sensors to monitor the network against an attacker who strategically targets a network component. The operator (resp. attacker) seeks to minimize (resp. maximize) the network loss. To study the properties of mixed-strategy Nash Equilibria of this game, we first study two simple instances: (i) When component sets monitored by individual sensor locations are mutually disjoint; (ii) When only a single sensor is positioned, but with possibly overlapping monitoring component sets. Our analysis reveals new insights on how criticality levels impact the players equilibrium strategies. Next, we extend a previously known approach to obtain an approximate Nash equilibrium for the general case of the game. This approach uses solutions to minimum set cover and maximum set packing problems to construct an approximate Nash equilibrium. Finally, we implement a column generation procedure to improve this solution and numerically evaluate the performance of our approach."
"We introduce a new dynamic analysis technique to discover invariants in separation logic for heap-manipulating programs. First, we use a debugger to obtain rich program execution traces at locations of interest on sample inputs. These traces consist of heap and stack information of variables that point to dynamically allocated data structures. Next, we iteratively analyze separate memory regions related to each pointer variable and search for a formula over predefined heap predicates in separation logic to model these regions. Finally, we combine the computed formulae into an invariant that describes the shape of explored memory regions.   We present SLING, a tool that implements these ideas to automatically generate invariants in separation logic at arbitrary locations in C programs, e.g., program pre and postconditions and loop invariants. Preliminary results on existing benchmarks show that SLING can efficiently generate correct and useful invariants for programs that manipulate a wide variety of complex data structures."
"We compute the quasinormal mode frequencies and Regge poles of the canonical acoustic hole (a black hole analogue), using three methods. First, we show how damped oscillations arise by evolving generic perturbations in the time domain using a simple finite-difference scheme. We use our results to estimate the fundamental QN frequencies of the low multipolar modes $l=1, 2, \ldots$. Next, we apply an asymptotic method to obtain an expansion for the frequency in inverse powers of $l+1/2$ for low overtones. We test the expansion by comparing against our time-domain results, and (existing) WKB results. The expansion method is then extended to locate the Regge poles. Finally, to check the expansion of Regge poles we compute the spectrum numerically by direct integration in the frequency domain. We give a geometric interpretation of our results and comment on experimental verification."
"Quanta image sensor (QIS) is envisioned to be the next generation image sensor after CCD and CMOS. In this paper, we discuss how to design color filter arrays for QIS and other small pixels. Designing color filter arrays for small pixels is challenging because maximizing the light efficiency while suppressing aliasing and crosstalk are conflicting tasks. We present an optimization-based framework which unifies several mainstream color filter array design methodologies. Our method offers greater generality and flexibility. Compared to existing methods, the new framework can simultaneously handle luminance sensitivity, chrominance sensitivity, cross-talk, anti-aliasing, manufacturability and orthogonality. Extensive experimental comparisons demonstrate the effectiveness of the framework."
"We present a self-supervised task on point clouds, in order to learn meaningful point-wise features that encode local structure around each point. Our self-supervised network, named MortonNet, operates directly on unstructured/unordered point clouds. Using a multi-layer RNN, MortonNet predicts the next point in a point sequence created by a popular and fast Space Filling Curve, the Morton-order curve. The final RNN state (coined Morton feature) is versatile and can be used in generic 3D tasks on point clouds. In fact, we show how Morton features can be used to significantly improve performance (+3% for 2 popular semantic segmentation algorithms) in the task of semantic segmentation of point clouds on the challenging and large-scale S3DIS dataset. We also show how MortonNet trained on S3DIS transfers well to another large-scale dataset, vKITTI, leading to an improvement over state-of-the-art of 3.8%. Finally, we use Morton features to train a much simpler and more stable model for part segmentation in ShapeNet. Our results show how our self-supervised task results in features that are useful for 3D segmentation tasks, and generalize well to other datasets."
"We are given a video of a person performing a certain activity, from which we extract a controllable model. The model generates novel image sequences of that person, according to arbitrary user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person.   The method is based on two networks. The first network maps a current pose, and a single-instance control signal to the next pose. The second network maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes."
"The arXiv has collected 1.5 million pre-print articles over 28 years, hosting literature from scientific fields including Physics, Mathematics, and Computer Science. Each pre-print features text, figures, authors, citations, categories, and other metadata. These rich, multi-modal features, combined with the natural graph structure---created by citation, affiliation, and co-authorship---makes the arXiv an exciting candidate for benchmarking next-generation models. Here we take the first necessary steps toward this goal, by providing a pipeline which standardizes and simplifies access to the arXiv's publicly available data. We use this pipeline to extract and analyze a 6.7 million edge citation graph, with an 11 billion word corpus of full-text research articles. We present some baseline classification results, and motivate application of more exciting generative graph models."
"We propose regression networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each class. In high dimensional embedding spaces the direction of data generally contains richer information than magnitude. Next to this, state-of-the-art few-shot metric methods that compare distances with aggregated class representations, have shown superior performance. Combining these two insights, we propose to meta-learn classification of embedded points by regressing the closest approximation in every class subspace while using the regression error as a distance metric. Similarly to recent approaches for few-shot learning, regression networks reflect a simple inductive bias that is beneficial in this limited-data regime and they achieve excellent results, especially when more aggregate class representations can be formed with multiple shots."
"SimRank is a widely studied link-based similarity measure that is known for its simple, yet powerful philosophy that two nodes are similar if they are referenced by similar nodes. While this philosophy has been the basis of several improvements, there is another useful, albeit less frequently discussed interpretation for SimRank known as the Random Surfer-Pair Model. In this work, we show that other well known measures related to SimRank can also be reinterpreted using Random Surfer-Pair Models, and establish a mathematically sound, general and unifying framework for several link-based similarity measures. This also serves to provide new insights into their functioning and allows for using these measures in a Monte Carlo framework, which provides several computational benefits. Next, we describe how the framework can be used as a versatile tool for developing measures according to given design requirements. As an illustration of this utility, we develop a new measure by combining the benefits of two existing measures under this framework, and empirically demonstrate that it results in a better performing measure."
"We formulate the inverse scattering transform for the scalar Maxwell-Bloch system of equations describing the resonant interaction of light and active optical media in the case when the light intensity does not vanish at infinity. We show that pure background states in general do not exist with a nonzero background field. We then use the formalism to compute explicitly the soliton solutions of this system. We discuss the initial population of atoms and show that the pure soliton solutions do not correspond to a pure state initially. We obtain a representation for the soliton solutions in determinant form, and explicitly write down the one-soliton solutions. We next derive periodic solutions and rational solutions from the one-soliton solutions. We then analyze the properties of these solutions, including discussion of the sharp-line and small-amplitude limits, and thereafter show that the two limits do not commute. Finally, we investigate the behavior of general solutions, showing that solutions are stable (i.e., the radiative parts of solutions decay) only when initially atoms in the ground state dominant, i.e., initial population inversion is negative."
"We explore a general framework in Markov chain Monte Carlo (MCMC) sampling where sequential proposals are tried as a candidate for the next state of the Markov chain. This sequential-proposal framework can be applied to various existing MCMC methods, including Metropolis-Hastings algorithms using random proposals and methods that use deterministic proposals such as Hamiltonian Monte Carlo (HMC) or the bouncy particle sampler. Sequential-proposal MCMC methods construct the same Markov chains as those constructed by the delayed rejection method under certain circumstances. In the context of HMC, the sequential-proposal approach has been proposed as extra chance generalized hybrid Monte Carlo (XCGHMC). We develop two novel methods in which the trajectories leading to proposals in HMC are automatically tuned to avoid doubling back, as in the No-U-Turn sampler (NUTS). The numerical efficiency of these new methods compare favorably to the NUTS. We additionally show that the sequential-proposal bouncy particle sampler enables the constructed Markov chain to pass through regions of low target density and thus facilitates better mixing of the chain when the target density is multimodal."
"Pedestrian trajectory prediction is a challenging task because of the complexity of real-world human social behaviors and uncertainty of the future motion. For the first issue, existing methods adopt fully connected topology for modeling the social behaviors, while ignoring non-symmetric pairwise relationships. To effectively capture social behaviors of relevant pedestrians, we utilize a directed social graph which is dynamically constructed on timely location and speed direction. Based on the social graph, we further propose a network to collect social effects and accumulate with individual representation, in order to generate destination-oriented and social-aware representations. For the second issue, instead of modeling the uncertainty of the entire future as a whole, we utilize a temporal stochastic method for sequentially learning a prior model of uncertainty during social interactions. The prediction on the next step is then generated by sampling on the prior model and progressively decoding with a hierarchical LSTMs. Experimental results on two public datasets show the effectiveness of our method, especially when predicting trajectories in very crowded scenes."
"We introduce a new learning strategy for image enhancement by recurrently training the same simple superresolution (SR) network multiple times. After initially training an SR network by using pairs of a corrupted low resolution (LR) image and an original image, the proposed method makes use of the trained SR network to generate new high resolution (HR) images with a doubled resolution from the original uncorrupted images. Then, the new HR images are downscaled to the original resolution, which work as target images for the SR network in the next stage. The newly generated HR images by the repeatedly trained SR network show better image quality and this strategy of training LR to mimic new HR can lead to a more efficient SR network. Up to a certain point, by repeating this process multiple times, better and better images are obtained. This recurrent leaning strategy for SR can be a good solution for downsizing convolution networks and making a more efficient SR network. To measure the enhanced image quality, for the first time in this area of super-resolution and image enhancement, we use VIQET MOS score which reflects human visual quality more accurately than the conventional MSE measure."
"We propose in this paper a new method to compute the characteristic function (CF) of generalized Gaussian (GG) random variable in terms of the Fox H function. The CF of the sum of two independent GG random variables is then deduced. Based on this results, the probability density function (PDF) and the cumulative distribution function (CDF) of the sum distribution are obtained. These functions are expressed in terms of the bivariate Fox H function. Next, the statistics of the distribution of the sum, such as the moments, the cumulant, and the kurtosis, are analyzed and computed. Due to the complexity of bivariate Fox H function, a solution to reduce such complexity is to approximate the sum of two independent GG random variables by one GG random variable with suitable shape factor. The approximation method depends on the utility of the system so three methods of estimate the shape factor are studied and presented."
"Blockchain is an innovative distributed ledger technology which has attracted a wide range of interests for building the next generation of applications to address lack-of-trust issues in business. Blockchain as a service (BaaS) is a promising solution to improve the productivity of blockchain application development. However, existing BaaS deployment solutions are mostly vendor-locked: they are either bound to a cloud provider or a blockchain platform. In addition to deployment, design and implementation of blockchain-based applications is a hard task requiring deep expertise. Therefore, this paper presents a unified blockchain as a service platform (uBaaS) to support both design and deployment of blockchain-based applications. The services in uBaaS include deployment as a service, design pattern as a service and auxiliary services. In uBaaS, deployment as a service is platform agnostic, which can avoid lock-in to specific cloud platforms, while design pattern as a service applies design patterns for data management and smart contract design to address the scalability and security issues of blockchain. The proposed solutions are evaluated using a real-world quality tracing use case in terms of feasibility and scalability."
"Redshift-space distortions (RSD) in galaxy redshift surveys generally break both the isotropy and homogeneity of galaxy distribution. While the former aspect is particularly highlighted as a probe of growth of structure induced by gravity, the latter aspect, often quoted as wide-angle RSD but ignored in most of the cases, will become important and critical to account for as increasing the statistical precision in next-generation surveys. However, the impact of wide-angle RSD has been mostly studied using linear perturbation theory. In this paper, employing the Zel'dovich approximation, i.e., first-order Lagrangian perturbation theory for gravitational evolution of matter fluctuations, we present a quasi-linear treatment of wide-angle RSD, and compute the cross-correlation function. The present formalism consistently reproduces linear theory results, and can be easily extended to incorporate relativistic corrections (e.g., gravitational redshift)."
"The fifth generation (5G) of wireless communication is in its infancy, and its evolving versions will be launched over the coming years. However, according to exposing the inherent constraints of 5G and the emerging applications and services with stringent requirements e.g. latency, energy/bit, traffic capacity, peak data rate, and reliability, telecom researchers are turning their attention to conceptualize the next generation of wireless communications, i.e. 6G. In this paper, we investigate 6G challenges, requirements, and trends. Furthermore, we discuss how artificial intelligence (AI) techniques can contribute to 6G. Based on the requirements and solutions, we identify some new fascinating services and use-cases of 6G, which can not be supported by 5G appropriately. Moreover, we explain some research directions that lead to the successful conceptualization and implementation of 6G."
"Computing a sample mean of time series under dynamic time warping (DTW) is NP-hard. Consequently, there is an ongoing research effort to devise efficient heuristics. The majority of heuristics have been developed for the constrained sample mean problem that assumes a solution of predefined length. In contrast, research on the unconstrained sample mean problem is underdeveloped. In this article, we propose a generic average-compress (AC) algorithm for solving the unconstrained problem. The algorithm alternates between averaging (A-step) and compression (C-step). The A-step takes an initial guess as input and returns an approximation of a sample mean. Then the C-step reduces the length of the approximate solution. The compressed approximation serves as initial guess of the A-step in the next iteration. The purpose of the C-step is to direct the algorithm to more promising solutions of shorter length. The proposed algorithm is generic in the sense that any averaging and any compression method can be used. Experimental results show that the AC algorithm substantially outperforms current state-of-the-art algorithms for time series averaging."
"Geometric folding processes are ubiquitous in natural systems ranging from protein biochemistry to patterns of insect wings and leaves. In a previous study, a folding operation between strings of formal languages was introduced as a model of such processes. The operation was then used to define a folding system (F-system) as a construct consisting of a core language, containing the strings to be folded, and a folding procedure language, which defines how the folding is done. This paper reviews main definitions associated with F-systems and next it determines necessary conditions for a language to belong to classes generated by such systems. The conditions are stated in the form of pumping lemmas and four classes are considered, in which the core and folding procedure languages are both regular, one of them is regular and the other context-free, or both are context-free. Full demonstrations of the lemmas are provided, and the analysis is illustrated with examples."
"The paper describes a deep reinforcement learning framework based on self-supervised learning within the proof assistant HOL4. A close interaction between the machine learning modules and the HOL4 library is achieved by the choice of tree neural networks (TNNs) as machine learning models and the internal use of HOL4 terms to represent tree structures of TNNs. Recursive improvement is possible when a task is expressed as a search problem. In this case, a Monte Carlo Tree Search (MCTS) algorithm guided by a TNN can be used to explore the search space and produce better examples for training the next TNN. As an illustration, term synthesis tasks on combinators and Diophantine equations are specified and learned. We achieve a success rate of 65% on combinator synthesis problems outperforming state-of-the-art ATPs run with their best general set of strategies. We set a precedent for statistically guided synthesis of Diophantine equations by solving 78.5% of the generated test problems."
"We study self-dual SU(N) gauge field configurations on the 4 torus with twisted boundary conditions, known as fractional instantons. Focusing on the minimum non-zero action case, we generalize the constant field strength solutions discovered by `t Hooft and valid for certain geometries. For the general case, we construct the vector potential and field strength in a power series expansion in a deformation parameter of the metric. The next to leading term is explicitly computed. The methodology is an extension of that used by the author for SU(2) fractional instantons and for vortices in two-dimensional Abelian Higgs models. Obviously, these solutions can also be seen as self-dual configurations in $\mathbb{R}^4$ having a crystal structure, where each node of the crystal carries a topological charge of $1/N$."
"In a dislocation problem, a paradoxical discordance is known to occur between an original smooth curve and an infinitesimally discretized curve. To solve this paradox, we have investigated a non-hypersingular expression for the integral kernel (called the stress Green's function) which describes the stress field caused by the displacement discontinuity. We first develop a compact alternative expression of the non-hypersingular stress Green's function for general two- and three-dimensional infinite homogeneous elastic media. We next compute the stress Green's functions on a curved fault and revisit the paradox. We find that previously obtained non-hypersingular stress Green's functions are incorrect for curved faults, and that smooth and infinitesimally segmented faults are equivalent. Their compatibility bridges the gap between analytical methods featuring curved faults and numerical methods using subdivided flat patches."
"A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores."
"Simultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency. We have to determine when we start the translation for observed inputs so far, to achieve good practical performance. In this work, we propose a neural machine translation method to determine this timing in an adaptive manner. The proposed method introduces a special token '<wait>', which is generated when the translation model chooses to read the next input token instead of generating an output token. It also introduces an objective function to handle the ambiguity in wait timings that can be optimized using an algorithm called Connectionist Temporal Classification (CTC). The use of CTC enables the optimization to consider all possible output sequences including '<wait>' that are equivalent to the reference translations and to choose the best one adaptively. We apply the proposed method into simultaneous translation from English to Japanese and investigate its performance and remaining problems."
"We consider the general chiral effective action which parametrizes the nonlinear realization of the spontaneous breaking of the electroweak symmetry with a light Higgs, and compute the one-loop ultraviolet divergences coming from Higgs and electroweak Goldstone fluctuations using the background field method. The renormalization of the divergences is carried out through operators of next-to-leading order in the chiral counting, i.e. of O(p^4). Being of the same order in power counting, the logarithmic corrections linked to these divergences can be as important as the tree-level contributions from the O(p^4) operators, and must be accounted for in the phenomenological analysis of experimental data. Deviations in the O(p^2) (leading-order) couplings with respect to the Standard Model values, e.g., in the h->WW coupling, would generate contributions from the 1-loop chiral logarithms computed in this work to a vast variety of observables, which do not have a counterpart in the conventional electroweak effective theory with a linearly transforming Higgs complex doublet."
"In the near future, Internet-of-Things (IoT) is expected to connect billions of devices (e.g., smartphones and sensors), which generate massive real-time data at the network edge. Intelligence can be distilled from the data to support next-generation AI-powered applications, which is called edge machine learning. One challenge faced by edge learning is the communication bottleneck, which is caused by the transmission of high-dimensional data from many edge devices to edge servers for learning. Traditional wireless techniques focusing only on efficient radio access are ineffective in tackling the challenge. Solutions should be based on a new approach that seamlessly integrates communication and computation. This has led to the emergence of a new cross-disciplinary paradigm called communication efficient edge learning. The main theme in the area is to design new communication techniques and protocols for efficient implementation of different distributed learning frameworks (i.e., federated learning) in wireless networks. This article provides an overview of the emerging area by introducing new design principles, discussing promising research opportunities, and providing design examples based on recent work."
"Van der Waals materials and heterostructures manifesting strongly bound room temperature exciton states exhibit emergent physical phenomena and are of a great promise for optoelectronic applications. Here, we demonstrate that nanostructured multilayer transition metal dichalcogenides by themselves provide an ideal platform for excitation and control of excitonic modes, paving the way to exciton-photonics. Hence, we show that by patterning the TMDCs into nanoresonators, strong dispersion and avoided crossing of excitons and hybrid polaritons with interaction potentials exceeding 410 meV may be controlled with great precision. We further observe that inherently strong TMDC exciton absorption resonances may be completely suppressed due to excitation of hybrid photon states and their interference. Our work paves the way to a next generation of integrated exciton optoelectronic nano-devices and applications in light generation, computing, and sensing."
"Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose TED, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in news articles to pretrain the model on millions of unlabeled corpora. Next, we finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of generated summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN/DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by TED are highly abstractive, and each component in the objective function of TED is highly effective."
"This paper connects a series of papers dealing with taxonomic word embeddings. It begins by noting that there are different types of semantic relatedness and that different lexical representations encode different forms of relatedness. A particularly important distinction within semantic relatedness is that of thematic versus taxonomic relatedness. Next, we present a number of experiments that analyse taxonomic embeddings that have been trained on a synthetic corpus that has been generated via a random walk over a taxonomy. These experiments demonstrate how the properties of the synthetic corpus, such as the percentage of rare words, are affected by the shape of the knowledge graph the corpus is generated from. Finally, we explore the interactions between the relative sizes of natural and synthetic corpora on the performance of embeddings when taxonomic and thematic embeddings are combined."
"The multi-factor model is a widely used model in quantitative investment. The success of a multi-factor model is largely determined by the effectiveness of the alpha factors used in the model. This paper proposes a new evolutionary algorithm called AutoAlpha to automatically generate effective formulaic alphas from massive stock datasets. Specifically, first we discover an inherent pattern of the formulaic alphas and propose a hierarchical structure to quickly locate the promising part of space for search. Then we propose a new Quality Diversity search based on the Principal Component Analysis (PCA-QD) to guide the search away from the well-explored space for more desirable results. Next, we utilize the warm start method and the replacement method to prevent the premature convergence problem. Based on the formulaic alphas we discover, we propose an ensemble learning-to-rank model for generating the portfolio. The backtests in the Chinese stock market and the comparisons with several baselines further demonstrate the effectiveness of AutoAlpha in mining formulaic alphas for quantitative trading."
"When working on intelligent tutor systems designed for mathematics education and its specificities, an interesting objective is to provide relevant help to the students by anticipating their next steps. This can only be done by knowing, beforehand, the possible ways to solve a problem. Hence the need for an automated theorem prover that provide proofs as they would be written by a student. To achieve this objective, logic programming is a natural tool due to the similarity of its reasoning with a mathematical proof by inference. In this paper, we present the core ideas we used to implement such a prover, from its encoding in Prolog to the generation of the complete set of proofs. However, when dealing with educational aspects, there are many challenges to overcome. We also present the main issues we encountered, as well as the chosen solutions."
"In this paper, we address a pursuit-evasion game involving multiple players by utilizing tools and techniques from reinforcement learning and matrix game theory. In particular, we consider the problem of steering an evader to a goal destination while avoiding capture by multiple pursuers, which is a high-dimensional and computationally intractable problem in general. In our proposed approach, we first formulate the multi-agent pursuit-evasion game as a sequence of discrete matrix games. Next, in order to simplify the solution process, we transform the high-dimensional state space into a low-dimensional manifold and the continuous action space into a feature-based space, which is a discrete abstraction of the original space. Based on these transformed state and action spaces, we subsequently employ min-max Q-learning, to generate the entries of the payoff matrix of the game, and subsequently obtain the optimal action for the evader at each stage. Finally, we present extensive numerical simulations to evaluate the performance of the proposed learning-based evading strategy in terms of the evader's ability to reach the desired target location without being captured, as well as computational efficiency."
"Abstractive document summarization is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Unfortunately, training large Seq2Seq based summarization models on limited supervised summarization data is challenging. This paper presents three pre-training objectives which allow us to pre-train a Seq2Seq based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation, and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (more than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness."
"Neural summarization models suffer from the fixed-size input limitation: if text length surpasses the model's maximal number of input tokens, some document content (possibly summary-relevant) gets truncated Independently summarizing windows of maximal input size disallows for information flow between windows and leads to incoherent summaries. We propose windowing models for neural abstractive summarization of (arbitrarily) long texts. We extend the sequence-to-sequence model augmented with pointer generator network by (1) allowing the encoder to slide over different windows of the input document and (2) sharing the decoder and retaining its state across different input windows. We explore two windowing variants: Static Windowing precomputes the number of tokens the decoder should generate from each window (based on training corpus statistics); in Dynamic Windowing the decoder learns to emit a token that signals encoder's shift to the next input window. Empirical results render our models effective in their intended use-case: summarizing long texts with relevant content not bound to the very document beginning."
"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ""reorder'' the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases"
"We introduce a novel dictionary optimization method for high-dimensional vector quantization employed in approximate nearest neighbor (ANN) search. Vector quantization methods first seek a series of dictionaries, then approximate each vector by a sum of elements selected from these dictionaries. An optimal series of dictionaries should be mutually independent, and each dictionary should generate a balanced encoding for the target dataset. Existing methods did not explicitly consider this. To achieve these goals along with minimizing the quantization error (residue), we propose a novel dictionary optimization method called \emph{Dictionary Annealing} that alternatively ""heats up"" a single dictionary by generating an intermediate dataset with residual vectors, ""cools down"" the dictionary by fitting the intermediate dataset, then extracts the new residual vectors for the next iteration. Better codes can be learned by DA for the ANN search tasks. DA is easily implemented on GPU to utilize the latest computing technology, and can easily extended to an online dictionary learning scheme. We show by experiments that our optimized dictionaries substantially reduce the overall quantization error. Jointly used with residual vector quantization, our optimized dictionaries lead to a better approximate nearest neighbor search performance compared to the state-of-the-art methods."
"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.   To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. At each iteration, a base MRC model is trained with golden answers and noisy evidence labels. The trained model will predict pseudo evidence labels as extra supervision in the next iteration. We evaluate STM on seven datasets over three MRC tasks. Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC. The source code can be obtained from https://github.com/SparkJiao/Self-Training-MRC"
"The next generation 21 cm surveys open a new window onto the early stages of cosmic structure formation and provide new insights about the Epoch of Reionization (EoR). However, the non-Gaussian nature of the 21 cm signal along with the huge amount of data generated from these surveys will require more advanced techniques capable to efficiently extract the necessary information to constrain the Reionization History of the Universe. In this paper we present the use of Bayesian Neural Networks (BNNs) to predict the posterior distribution for four astrophysical and cosmological parameters. Besides achieving state-of-the-art prediction performances, the proposed methods provide accurate estimation of parameters uncertainties and infer correlations among them. Additionally, we demonstrate the advantages of Normalizing Flows (NF) combined with BNNs, being able to model more complex output distributions and thus capture key information as non-Gaussianities in the parameter conditional density distribution for astrophysical and cosmological dataset. Finally, we propose novel calibration methods employing Normalizing Flows after training, to produce reliable predictions, and we demonstrate the advantages of this approach both in terms of computational cost and prediction performances."
"Nonlinear and hysteretic electrical devices are needed for applications from circuit protection to next-generation computing. Widely-studied devices for resistive switching are based on mass transport, such as the drift of ions in an electric field, and on collective phenomena, such as insulator-metal transitions. We ask whether the large photoconductive response known in many semiconductors can be stimulated in the dark and harnessed to design electrical devices. We design and test devices based on photoconductive CdS, and our results are consistent with the hypothesis that resistive switching arises from point defects that switch between deep- and shallow-donor configurations: defect level switching (DLS). This new electronic device design principle - photoconductivity without photons - leverages decades of research on photoconductivity and defect spectroscopy. It is easily generalized and will enable the rational design of new nonlinear, hysteretic devices for future electronics."
"We propose the Positive Resampler to solve the problem associated with event samples from state-of-the-art predictions for scattering processes at hadron colliders typically involving a sizeable number of events contributing with negative weight. The proposed method guarantees positive weights for all physical distributions, and a correct description of all observables. A desirable side product of the method is the possibility to reduce the size of event samples produced by General Purpose Event Generators, thus lowering the resource demands for subsequent computing-intensive event processing steps. We demonstrate the viability and efficiency of our approach by considering its application to a next-to-leading order + parton shower merged prediction for the production of a $W$ boson in association with multiple jets."
"We investigate a monotonic multihead attention (MMA) by extending hard monotonic attention to Transformer-based automatic speech recognition (ASR) for online streaming applications. For streaming inference, all monotonic attention (MA) heads should learn proper alignments because the next token is not generated until all heads detect the corresponding token boundaries. However, we found not all MA heads learn alignments with a na\""ive implementation. To encourage every head to learn alignments properly, we propose HeadDrop regularization by masking out a part of heads stochastically during training. Furthermore, we propose to prune redundant heads to improve consensus among heads for boundary detection and prevent delayed token generation caused by such heads. Chunkwise attention on each MA head is extended to the multihead counterpart. Finally, we propose head-synchronous beam search decoding to guarantee stable streaming inference."
"Existing image inpainting methods often produce artifacts when dealing with large holes in real applications. To address this challenge, we propose an iterative inpainting method with a feedback mechanism. Specifically, we introduce a deep generative model which not only outputs an inpainting result but also a corresponding confidence map. Using this map as feedback, it progressively fills the hole by trusting only high-confidence pixels inside the hole at each iteration and focuses on the remaining pixels in the next iteration. As it reuses partial predictions from the previous iterations as known pixels, this process gradually improves the result. In addition, we propose a guided upsampling network to enable generation of high-resolution inpainting results. We achieve this by extending the Contextual Attention module to borrow high-resolution feature patches in the input image. Furthermore, to mimic real object removal scenarios, we collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs. Experiments show that our method significantly outperforms existing methods in both quantitative and qualitative evaluations. More results and Web APP are available at https://zengxianyu.github.io/iic."
"The properties of flat minima in the empirical risk landscape of neural networks have been debated for some time. Increasing evidence suggests they possess better generalization capabilities with respect to sharp ones. First, we discuss Gaussian mixture classification models and show analytically that there exist Bayes optimal pointwise estimators which correspond to minimizers belonging to wide flat regions. These estimators can be found by applying maximum flatness algorithms either directly on the classifier (which is norm independent) or on the differentiable loss function used in learning. Next, we extend the analysis to the deep learning scenario by extensive numerical validations. Using two algorithms, Entropy-SGD and Replicated-SGD, that explicitly include in the optimization objective a non-local flatness measure known as local entropy, we consistently improve the generalization error for common architectures (e.g. ResNet, EfficientNet). An easy to compute flatness measure shows a clear correlation with test accuracy."
"The growth rate of large-scale structure provides a powerful consistency test of the standard cosmological model and a probe of possible deviations from general relativity. We use a Fisher analysis to forecast constraints on the growth rate from a combination of next-generation spectroscopic surveys. In the overlap survey volumes, we use a multi-tracer analysis to significantly reduce the effect of cosmic variance. The non-overlap individual survey volumes are included in the Fisher analysis in order to utilise the entire volume. We use the observed angular power spectrum, which naturally includes all wide-angle and lensing effects and circumvents the need for an Alcock-Paczynski correction. Cross correlations between redshift bins are included by using a novel technique to avoid computation of the sub-dominant contributions. Marginalising over the standard cosmological parameters, as well as the clustering bias in each redshift bin, we find that the precision on $\gamma$ improves on the best single-tracer precision by up to $\sim$50\%."
"Video interpolation increases the temporal resolution of a video sequence by synthesizing intermediate frames between two consecutive frames. We propose a novel deep-learning-based video interpolation algorithm based on bilateral motion estimation. First, we develop the bilateral motion network with the bilateral cost volume to estimate bilateral motions accurately. Then, we approximate bi-directional motions to predict a different kind of bilateral motions. We then warp the two input frames using the estimated bilateral motions. Next, we develop the dynamic filter generation network to yield dynamic blending filters. Finally, we combine the warped frames using the dynamic blending filters to generate intermediate frames. Experimental results show that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms on several benchmark datasets."
"In this paper, we study the quantum corrected thermodynamics of a class of black holes generated by self-gravitating Skyrmion models. One such black hole solution is Einstein-Skrymion black hole. We first compute the ADM mass of Einstein-Skyrmion black hole using on-shell Hamiltonian formalism present already in the literature. We then consider non-extended phase space thermodynamics and derive expressions for various thermodynamic quantities like the Hawking temperature, entropy, pressure, Gibbs free energy, and heat capacity. Next, we study the effect of quantum corrections on the thermodynamics of the Einstein-Skyrmion black hole. We observe that apart from leading to stability, the quantum correction induce an AdS (anti-de sitter) to dS (de Sitter) phase transition in Einstein-Skrymion black hole. Treating cosmological constant as the pressure, we determine the $P-V$ criticality of the Einstein-Skrymion black hole and observe that the $P-V$ criticality depends on model parameters $\lambda$ and $K$. The study of $P-V$ criticality done here could help to estimate the experimental bound on the values of parameters $\lambda$ and $K$."
"We consider decidability problems associated with Engel's identity ($[\cdots[[x,y],y],\dots,y]=1$ for a long enough commutator sequence) in groups generated by an automaton. We give a partial algorithm that decides, given $x,y$, whether an Engel identity is satisfied. It succeeds, importantly, in proving that Grigorchuk's $2$-group is not Engel. We consider next the problem of recognizing Engel elements, namely elements $y$ such that the map $x\mapsto[x,y]$ attracts to $\{1\}$. Although this problem seems intractable in general, we prove that it is decidable for Grigorchuk's group: Engel elements are precisely those of order at most $2$. Our computations were implemented using the package FR within the computer algebra system GAP."
"We present a novel algorithm for generating high quality holograms for Computer Generated Holography - Holographic Predictive Search. This approach is presented as an alternative to traditional Holographic Search Algorithms such as Direct Search (DS) and Simulated Annealing (SA). We first introduce the current search based methods and then introduce an analytical model of the underlying Fourier elements. This is used to make prescient judgements regarding the next iteration of the algorithm. This new approach is developed for the case of phase modulating devices with phase sensitive reconstructions. When compared to conventional iterative approaches such as DS and SA on a multi-phase device, Holographic Predictive Search offered improvements in quality of 5x as well up to 10x improvements in convergence time. This is at the cost of an increased iteration overhead."
"We present a simple, effective, and general activation function we term ACON which learns to activate the neurons or not. Interestingly, we find Swish, the recent popular NAS-searched activation, can be interpreted as a smooth approximation to ReLU. Intuitively, in the same way, we approximate the more general Maxout family to our novel ACON family, which remarkably improves the performance and makes Swish a special case of ACON. Next, we present meta-ACON, which explicitly learns to optimize the parameter switching between non-linear (activate) and linear (inactivate) and provides a new design space. By simply changing the activation function, we show its effectiveness on both small models and highly optimized large models (e.g. it improves the ImageNet top-1 accuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively). Moreover, our novel ACON can be naturally transferred to object detection and semantic segmentation, showing that ACON is an effective alternative in a variety of tasks. Code is available at https://github.com/nmaac/acon."
"Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits and datasets. Based on the discussion of the recent advances, we present several promising future directions toward real-world and next-generation automated log analysis."
"This paper proposes three novel resource and user scheduling algorithms with contiguous frequency-domain resource allocation (FDRA) for wireless communications systems. The first proposed algorithm jointly schedules users and resources selected adaptively from both ends of the bandwidth part (BWP), while the second and third ones apply disjoint user and resource selection with either single-end or dual-end BWP strategies. Distinct from existing contiguous FDRA approaches, the proposed ones comply with standards specifications for fifth-generation (5G) and beyond 5G communications, and have lower computational complexity hence are more practical. Simulation results show that all of the proposed algorithms can achieve near-optimal performance in terms of throughput and packet loss rate for low to moderate traffic load, and the first one can still perform relatively well even with a large number of users."
"We study the group of transformations of 4F3 hypergeometric functions evaluated at unity with one unit shift in parameters. We reveal the general form of this family of transformations and its group property. Next, we use explicitly known transformations to generate a subgroup whose structure is then thoroughly studied. Using some known results for 3F2 transformation groups, we show that this subgroup is isomorphic to the direct product of the symmetric group of degree 5 and 5-dimensional integer lattice. We investigate the relation between two-term 4F3 transformations from our group and three-term 3F2 transformations and present a method for computing the coefficients of the contiguous relations for 3F2 functions evaluated at unity. We further furnish a class of summation formulas associated with the elements of our group. In the appendix to this paper, we give a collection of Wolfram Mathematica routines facilitating the group calculations."
"The next generation of gravitational-wave observatories will reach low frequency limits on the orders of a few Hz, thus enabling the detection of gravitational wave signals of very long duration. The run time of standard parameter estimation techniques with these long waveforms can be months or even years, making it impractical with existing Bayesian inference pipelines. Reduced order modeling and reduced order quadrature integration rule have recently been exploited as promising techniques that can greatly reduce parameter estimation computational costs. We describe a Python-based reduced order quadrature building code, PyROQ, which builds the reduced order quadrature data needed to accelerate parameter estimation of gravitational waves. We present the first bases for the IMRPhenomXPHM waveform model of binary-black-hole coalescences, including subdominant harmonic modes and precessing spins effects. Furthermore, the code infrastructure makes it directly applicable to the gravitational wave inference for space-borne detectors such as the Laser Interferometer Space Antenna (LISA)."
"This paper suggests leveraging reactive power potential (RPP) embedded in wind farms to improve power system operational safety and optimality. First, three typical RPP provision approaches are analyzed and a two-stage robust linear optimization based RPP evaluation method is proposed. This approach yields an RPP range that ensures the security of wind farm operations under any realization of uncertainty regarding the wind farm. Simplified DistFlow equations are employed here for a compromise between computational accuracy and cost. Next, an uncertain RPP-involved reactive power optimization problem is introduced, through which system operators ensure system-wide security and optimality regarding the base case and against any possible deviation caused by uncertain lumped loads and renewable generation. Steady-state models of automatic generation control and local voltage control are also captured in this uncertain reactive power optimization, which is then transformed through Soyster's method into a deterministic optimization problem that is readily solvable. Case studies have conceptually validated that even with notable uncertainty, wind farms are still a competent reactive power resource providing considerable RPP. Also, simulation confirms positive and notable improvement of leveraging wind-farm RPP on system-wide operational security and optimality, especially for power systems with high wind penetration."
"Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state-action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable. We derive an upper bound of the negative log-likelihood of the environmental transition and use such an upper bound as the intrinsic reward for exploration, which allows the agent to learn skills by self-supervised exploration without observing extrinsic rewards. We evaluate the proposed method on several image-based simulation tasks and a real robotic manipulating task. Our method outperforms several state-of-the-art environment model-based exploration approaches."
"Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. Existing methods typically employ intuitive, carefully hand-designed mechanisms for controlling such rotations. In this paper, we take a novel approach to address this issue by casting min-max optimization as a physical system. We leverage tools from physics to introduce LEAD (Least-Action Dynamics), a second-order optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete-time settings for bilinear games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements over baseline methods."
"Ultrathin meta-optics offer unmatched, multifunctional control of light. Next-generation optical technologies, however, demand unprecedented performance. This will likely require design algorithms surpassing the capability of human intuition. For the adjoint method, this requires explicitly deriving gradients, which is sometimes challenging for certain photonics problems. Existing techniques also comprise a patchwork of application-specific algorithms, each focused in scope and scatterer type. Here, we leverage algorithmic differentiation as used in artificial neural networks, treating photonic design parameters as trainable weights, optical sources as inputs, and encapsulating device performance in the loss function. By solving a complex, degenerate eigenproblem and formulating rigorous coupled-wave analysis as a computational graph, we support both arbitrary, parameterized scatterers and topology optimization. With iteration times below the cost of two forward simulations typical of adjoint methods, we generate multilayer, multifunctional, and aperiodic meta-optics. As an open-source platform adaptable to other algorithms and problems, we enable fast and flexible meta-optical design."
"Networking data analytics is increasingly used for enhanced network visibility and controllability. We draw the similarities between the Software Defined Networking (SDN) architecture and the MapReduce programming model. Inspired by the similarity, we suggest the necessary data plane innovations to make network data plane devices function as distributed mappers and optionally, reducers. A streaming network data MapReduce architecture can therefore conveniently solve a series of network monitoring and management problems. Unlike the traditional networking data analytical system, our proposed system embeds the data analytics engine directly in the network infrastructure. The affinity leads to a concise system architecture and better cost performance ratio. On top of this architecture, we propose a general MapReduce-like programming model for real-time and one-pass networking data analytics, which involves joint in-network and out-of-network computing. We show this model can address a wide range of interactive queries from various network applications. This position paper strives to make a point that the white-box trend does not necessarily lead to simple and dumb networking devices. Rather, the defining characteristics of the next generation white-box are open and programmable, so that the network devices can be made smart and versatile to support new services and applications."
"We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces."
"Meta-learning becomes a practical approach towards few-shot image classification, where a visual recognition system is constructed with limited annotated data. Inductive bias such as embedding is learned from a base class set with ample labeled examples and then generalizes to few-shot tasks with novel classes. Surprisingly, we find that the base class set labels are not necessary, and discriminative embeddings could be meta-learned in an unsupervised manner. Comprehensive analyses indicate two modifications -- the semi-normalized distance metric and the sufficient sampling -- improves unsupervised meta-learning (UML) significantly. Based on the modified baseline, we further amplify or compensate for the characteristic of tasks when training a UML model. First, mixed embeddings are incorporated to increase the difficulty of few-shot tasks. Next, we utilize a task-specific embedding transformation to deal with the specific properties among tasks, maintaining the generalization ability into the vanilla embeddings. Experiments on few-shot learning benchmarks verify that our approaches outperform previous UML methods by a 4-10% performance gap, and embeddings learned with our UML achieve comparable or even better performance than its supervised variants."
"Intrinsic galaxy alignments yield an important contribution to the observed statistics of galaxy shapes. The general bias expansion for galaxy sizes and shapes in three dimensions has been recently described by Vlah, Chisari \& Schmidt using the general perturbative effective field theory (EFT) framework, in analogy to the clustering of galaxies. In this work, we present a formalism that uses the properties of spherical tensors to project galaxy shapes onto the observed sky in the flat-sky approximation and compute the two-point functions at next-to-leading order as well as the leading-order three-point functions of galaxy shapes and number counts. The resulting expressions are given in forms that are convenient for efficient numerical implementation. For a source redshift distribution typical of Stage IV surveys, we find that that nonlinear intrinsic alignment contributions to galaxy shape correlations become relevant at angular wavenumbers $l \gtrsim 100$."
"We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework's guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation."
"We introduce a unified framework for generic video annotation with bounding boxes. Video annotation is a longstanding problem, as it is a tedious and time-consuming process. We tackle two important challenges of video annotation: (1) automatic temporal interpolation and extrapolation of bounding boxes provided by a human annotator on a subset of all frames, and (2) automatic selection of frames to annotate manually. Our contribution is two-fold: first, we propose a model that has both interpolating and extrapolating capabilities; second, we propose a guiding mechanism that sequentially generates suggestions for what frame to annotate next, based on the annotations made previously. We extensively evaluate our approach on several challenging datasets in simulation and demonstrate a reduction in terms of the number of manual bounding boxes drawn by 60% over linear interpolation and by 35% over an off-the-shelf tracker. Moreover, we also show 10% annotation time improvement over a state-of-the-art method for video annotation with bounding boxes [25]. Finally, we run human annotation experiments and provide extensive analysis of the results, showing that our approach reduces actual measured annotation time by 50% compared to commonly used linear interpolation."
"Knowledge is a formal way of understanding the world, providing a human-level cognition and intelligence for the next-generation artificial intelligence (AI). One of the representations of knowledge is semantic relations between entities. An effective way to automatically acquire this important knowledge, called Relation Extraction (RE), a sub-task of information extraction, plays a vital role in Natural Language Processing (NLP). Its purpose is to identify semantic relations between entities from natural language text. To date, there are several studies for RE in previous works, which have documented these techniques based on Deep Neural Networks (DNNs) become a prevailing technique in this research. Especially, the supervised and distant supervision methods based on DNNs are the most popular and reliable solutions for RE. This article 1) introduces some general concepts, and further 2) gives a comprehensive overview of DNNs in RE from two points of view: supervised RE, which attempts to improve the standard RE systems, and distant supervision RE, which adopts DNNs to design sentence encoder and de-noise method. We further 3) cover some novel methods and recent trends as well as discuss possible future research directions for this task."
"In this Thesis we design radiation patterns capable of creating effective light-matter interactions suited to applications in quantum computing, quantum simulation and quantum sensing. On the one hand, we have used dynamical decoupling techniques to design quantum operations that are robust against errors in environmental and control fields, achieving high-fidelity quantum logic in trapped ions and energy-efficient nuclear magnetic resonance at the nanoscale with nitrogen-vacancy centers in diamond. On the other hand, we have studied generalised models of light-matter interaction, leading to the discovery of selective multi-photon interactions in the Rabi-Stark model and a proposal for preparing non-classical quantum states using the nonlinear quantum Rabi model. Finally, we have shown how the appropriate tailoring of interactions among ultracold atoms in optical lattices could lead to solve the boson sampling problem faster than the best supercomputers, thus demonstrating quantum supremacy. In this manner, we believe the results presented here significantly expand our knowledge on the control of light-matter interactions, and provide optimal scenarios for current quantum devices to generate the next-generation of quantum applications."
"Dipole-dipole interactions lead to frequency shifts that are expected to limit the performance of next-generation atomic clocks. In this work, we compute dipolar frequency shifts accounting for the intrinsic atomic multilevel structure in standard Ramsey spectroscopy. When interrogating the transitions featuring the smallest Clebsch-Gordan coefficients, we find that a simplified two-level treatment becomes inappropriate, even in the presence of large Zeeman shifts. For these cases, we show a net suppression of dipolar frequency shifts and the emergence of dominant non-classical effects for experimentally relevant parameters. Our findings are pertinent to current generations of optical lattice and optical tweezer clocks, opening a way to further increase their current accuracy, and thus their potential to probe fundamental and many-body physics."
"VPIC is a general purpose Particle-in-Cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC's capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this paper we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC's modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors."
"Topologically non-trivial spin textures, such as skyrmions and dislocations, display emergent electrodynamics and can be moved by spin currents over macroscopic distances. These unique properties and their nanoscale size make them excellent candidates for the development of next-generation logic gates, race-track memory, and artificial synapses for neuromorphic computing. A major challenge for these applications - and the investigation of nanoscale magnetic structures in general - is the realization of detection schemes that provide high resolution and sensitivity. We study the local magnetic properties of disclinations, dislocations, and domain walls in FeGe, and reveal a pronounced response that distinguishes the individual spin textures from the helimagnetic background. Combination of magnetic force microscopy and micromagnetic simulations links the non-linear response to the local magnetic susceptibility. Based on the findings, we propose a read-out scheme using superconducting micro-coils, representing an innovative approach for detecting topologically non-trivial spin textures and domain walls in device-relevant geometries."
"To solve the limitation of Hadoop on scalability, resource sharing, and application support, the open-source community proposes the next generation of Hadoop's compute platform called Yet Another Resource Negotiator (YARN) by separating resource management functions from the programming model. This separation enables various application types to run on YARN in parallel. To achieve fair resource sharing and high resource utilization, YARN provides the capacity scheduler and the fair scheduler. However, the performance impacts of the two schedulers are not clear when mixed applications run on a YARN cluster. Therefore, in this paper, we study four scheduling-policy combinations (SPCs for short) derived from the two schedulers and then evaluate the four SPCs in extensive scenarios, which consider not only four application types, but also three different queue structures for organizing applications. The experimental results enable YARN managers to comprehend the influences of different SPCs and different queue structures on mixed applications. The results also help them to select a proper SPC and an appropriate queue structure to achieve better application execution performance."
"We compute minimal sets of generators for the S_n-modules (n <= 4) of multilinear polynomial identities of arity n satisfied by the Jordan product and the Jordan diproduct (resp. pre-Jordan product) in every triassociative (resp. tridendriform) algebra. These identities define Jordan trialgebras and post-Jordan algebras: Jordan analogues of the Lie trialgebras and post-Lie algebras introduced by Dotsenko et al., Pei et al., Vallette & Loday. We include an extensive review of analogous structures existing in the literature, and their interrelations, in order to identify the gaps filled by our two new varieties of algebras. We use computer algebra (linear algebra over finite fields, representation theory of symmetric groups), to verify in both cases that every polynomial identity of arity <= 6 is a consequence of those of arity <= 4. We conjecture that in both cases the next independent identities have arity 8, imitating the Glennie identities for Jordan algebras. We formulate our results as a commutative square of operad morphisms, which leads to the conjecture that the squares in a much more general class are also commutative."
"We present a general procedure to introduce electronic polarization into classical Molecular Dynamics (MD) force-fields using a Neural Network (NN) model. We apply this framework to the simulation of a solid-liquid interface where the polarization of the surface is essential to correctly capture the main features of the system. By introducing a multi-input, multi-output NN and treating the surface polarization as a discrete classification problem, for which NNs are known to excel, we are able to obtain very good accuracy in terms of quality of predictions. Through the definition of a custom loss function we are able to impose a physically motivated constraint within the NN itself making this model extremely versatile, especially in the modelling of different surface charge states. The NN is validated considering the redistribution of electronic charge density within a graphene based electrode in contact with aqueous electrolyte solution, a system highly relevant to the development of next generation low-cost supercapacitors. We compare the performances of our NN/MD model against Quantum Mechanics/Molecular dynamics simulations where we obtain a most satisfactorily agreement."
"Although deep learning models for chest X-ray interpretation are commonly trained on labels generated by automatic radiology report labelers, the impact of improvements in report labeling on the performance of chest X-ray classification models has not been systematically investigated. We first compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of extracting accurate chest X-ray image labels from radiology reports, reporting that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers. Next, after training image classification models using labels generated from the different radiology report labelers on one of the largest datasets of chest X-rays, we show that an image classification model trained on labels from the VisualCheXbert labeler outperforms image classification models trained on labels from the CheXpert and CheXbert labelers. Our work suggests that recent improvements in radiology report labeling can translate to the development of higher performing chest X-ray classification models."
"Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information-seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model's trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE."
"In model-based reinforcement learning (MBRL), Wan et al. (2019) showed conditions under which the environment model could produce the expectation of the next feature vector rather than the full distribution, or a sample thereof, with no loss in planning performance. Such expectation models are of interest when the environment is stochastic and non-stationary, and the model is approximate, such as when it is learned using function approximation. In these cases a full distribution model may be impractical and a sample model may be either more expensive computationally or of high variance. Wan et al. considered only planning for prediction to evaluate a fixed policy. In this paper, we treat the control case - planning to improve and find a good approximate policy. We prove that planning with an expectation model must update a state-value function, not an action-value function as previously suggested (e.g., Sorg & Singh, 2010). This opens the question of how planning influences action selections. We consider three strategies for this and present general MBRL algorithms for each. We identify the strengths and weaknesses of these algorithms in computational experiments. Our algorithms and experiments are the first to treat MBRL with expectation models in a general setting."
"Biological materials are self-assembled with near-atomic precision in living cells, whereas synthetic 3D structures generally lack such precision and controllability. Recently, DNA nanotechnology, especially DNA origami technology, has been useful in the bottom-up fabrication of well-defined nanostructures ranging from tens of nanometres to sub-micrometres. In this Primer, we summarize the methodologies of DNA origami technology, including origami design, synthesis, functionalization and characterization. We highlight applications of origami structures in nanofabrication, nanophotonics and nanoelectronics, catalysis, computation, molecular machines, bioimaging, drug delivery and biophysics. We identify challenges for the field, including size limits, stability issues and the scale of production, and discuss their possible solutions. We further provide an outlook on next-generation DNA origami techniques that will allow in vivo synthesis and multiscale manufacturing."
"To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DialoGraph, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DialoGraph explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues."
"Inspired by natural evolution, evolutionary search algorithms have proven remarkably capable due to their dual abilities to radiantly explore through diverse populations and to converge to adaptive pressures. A large part of this behavior comes from the selection function of an evolutionary algorithm, which is a metric for deciding which individuals survive to the next generation. In deceptive or hard-to-search fitness landscapes, greedy selection often fails, thus it is critical that selection functions strike the correct balance between gradient-exploiting adaptation and exploratory diversification. This paper introduces Sel4Sel, or Selecting for Selection, an algorithm that searches for high-performing neural-network-based selection functions through a meta-evolutionary loop. Results on three distinct bitstring domains indicate that Sel4Sel networks consistently match or exceed the performance of both fitness-based selection and benchmarks explicitly designed to encourage diversity. Analysis of the strongest Sel4Sel networks reveals a general tendency to favor highly novel individuals early on, with a gradual shift towards fitness-based selection as deceptive local optima are bypassed."
"We study how higher-order gravity affects Higgs inflation in the Palatini formulation. We first review the metric and Palatini formulations in comparative manner and discuss their differences. Next cosmic inflation driven by a scalar field and inflationary observables are discussed. After this we review the Higgs inflation and compute the inflationary observables both in the metric and Palatini formulations. We then consider adding higher-order terms of the curvature to the action. We derive the equations of motion for the most general action quadratic in the curvature that does not violate parity in both the metric and Palatini formulations. Finally we present a new result. We analyse Higgs inflation in the Palatini formulation with higher-order curvature terms. We consider a simplified scenario where only terms constructed from the symmetric part of the Ricci tensor are added to the action. This implies that there are no new gravitational degrees of freedom, which makes the analysis easier. As a new result we found out that the scalar perturbation spectrum is unchanged, but the tensor perturbation spectrum is suppressed by the higher-order curvature couplings."
"When unexpected incidents occur, new innovative and flexible solutions are required. If this event is something such radical and dramatic like the COVID-19 pandemic, these solutions must aim to guarantee as much normality as possible while protecting lives. After a moment of shock our university decided that the students have to be able to pursue their studies for guaranteeing a degree in the expected time since most of them faced immediate financial problems due to the loss of their student jobs. This implied, for us as teachers, that we had to reorganise not only the teaching methods from nearly one day to the next, but we also had to come up with an adjusted way of examinations which had to take place in person with pen and paper under strict hygiene rules. On the other hand the correction should avoid personal contacts. We developed a framework which allowed us to correct the digitalised exams safely at home while providing the high standards given by the general data protection regulation of our country. Moreover, the time spent in the offices could be reduced to a minimum thanks to automatically generated exam sheets, automatically re-digitalised and sorted worked-on exams."
"Test Case Selection (TCS) aims to select a subset of the test suite to run for regression testing. The selection is typically based on past coverage and execution cost data. Researchers have successfully used multi-objective evolutionary algorithms (MOEAs), such as NSGA-II and its variants, to solve this problem. These MOEAs use traditional crossover operators to create new candidate solutions through genetic recombination. Recent studies in numerical optimization have shown that better recombinations can be made using machine learning, in particular link-age learning. Inspired by these recent advances in this field, we propose a new variant of NSGA-II, called L2-NSGA, that uses linkage learning to optimize test case selection. In particular, we use an unsupervised clustering algorithm to infer promising patterns among the solutions (subset of test suites). Then, these patterns are used in the next iterations of L2-NSGA to create solutions that preserve these inferred patterns. Our results show that our customizations make NSGA-II more effective for test case selection. The test suite sub-sets generated by L2-NSGA are less expensive and detect more faults than those generated by MOEAs used in the literature for regression testing."
"In this paper we present a novel two-scale framework to optimize the structure and the material distribution of an object given its functional specifications. Our approach utilizes multi-material microstructures as low-level building blocks of the object. We start by precomputing the material property gamut -- the set of bulk material properties that can be achieved with all material microstructures of a given size. We represent the boundary of this material property gamut using a level set field. Next, we propose an efficient and general topology optimization algorithm that simultaneously computes an optimal object topology and spatially-varying material properties constrained by the precomputed gamut. Finally, we map the optimal spatially-varying material properties onto the microstructures with the corresponding properties in order to generate a high-resolution printable structure. We demonstrate the efficacy of our framework by designing, optimizing, and fabricating objects in different material property spaces on the level of a trillion voxels, i.e several orders of magnitude higher than what can be achieved with current systems."
"Current datasets to train social behaviors are usually borrowed from surveillance applications that capture visual data from a bird's-eye perspective. This leaves aside precious relationships and visual cues that could be captured through a first-person view of a scene. In this work, we propose a strategy to exploit the power of current game engines, such as Unity, to transform pre-existing bird's-eye view datasets into a first-person view, in particular, a depth view. Using this strategy, we are able to generate large volumes of synthetic data that can be used to pre-train a social navigation model. To test our ideas, we present DeepSocNav, a deep learning based model that takes advantage of the proposed approach to generate synthetic data. Furthermore, DeepSocNav includes a self-supervised strategy that is included as an auxiliary task. This consists of predicting the next depth frame that the agent will face. Our experiments show the benefits of the proposed model that is able to outperform relevant baselines in terms of social navigation scores."
"This paper provides an efficient recursive approach of the spectral Tau method to approximate the solution of system of generalized Abel-Volterra integral equations. In this regards, we first investigate the existence, uniqueness as well as smoothness of the solutions under assumption on the given data. Next, from a numerical perspective, we express approximated solution as a linear combination of suitable canonical polynomials which are constructed by an easy to use recursive formula. Mostly, the unknown parameters are calculated by solving a low dimensional algebraic systems independent of degree of approximation which prevent from high computational costs. Obviously, due to singular behavior of the exact solutions, using classical polynomials to construct canonical polynomials, leads to low accuracy results. In this regards, we develop a new fractional order canonical polynomials using M\""untz-Legendre polynomials which have a same asymptotic behavior with the solution of underlying problem. The convergence analysis is discussed, and the familiar spectral accuracy is achieved in $L^{\infty}$ norm. Finally, the reliability of the method is evaluated using various problems."
"We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by `state-of-the-art' research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting."
"We consider the Sinai model describing a particle diffusing in a 1D random force field. As shown by Golosov, this model exhibits a strong localization phenomenon for the thermal packet: the disorder average of the thermal distribution of the relative distance y=x-m(t), with respect to the (disorder-dependent) most probable position m(t), converges in the limit of infinite time towards a distribution P(y). In this paper, we revisit this question of the localization of the thermal packet. We first generalize the result of Golosov by computing explicitly the joint asymptotic distribution of relative position y=x(t)-m(t) and relative energy u=U(x(t))-U(m(t)) for the thermal packet. Next, we compute in the infinite-time limit the localization parameters Y_k, representing the disorder-averaged probabilities that k particles of the thermal packet are at the same place, and the correlation function C(l) representing the disorder-averaged probability that two particles of the thermal packet are at a distance l from each other. We moreover prove that our results for Y_k and C(l) exactly coincide with the thermodynamic limit of the analog quantities computed for independent particles at equilibrium in a finite sample of length L. Finally, we discuss the properties of the finite-time metastable states that are responsible for the localization phenomenon and compare with the general theory of metastable states in glassy systems, in particular as a test of the Edwards conjecture."
"We compute the entropy of entanglement in the ground states of a general class of quantum spin-chain Hamiltonians - those that are related to quadratic forms of Fermi operators - between the first N spins and the rest of the system in the limit of infinite total chain length. We show that the entropy can be expressed in terms of averages over the classical compact groups and establish an explicit correspondence between the symmetries of a given Hamiltonian and those characterizing the Haar measure of the associated group. These averages are either Toeplitz determinants or determinants of combinations of Toeplitz and Hankel matrices. Recent generalizations of the Fisher-Hartwig conjecture are used to compute the leading order asymptotics of the entropy as N --> infinity . This is shown to grow logarithmically with N. The constant of proportionality is determined explicitly, as is the next (constant) term in the asymptotic expansion. The logarithmic growth of the entropy was previously predicted on the basis of numerical computations and conformal-field-theoretic calculations. In these calculations the constant of proportionality was determined in terms of the central charge of the Virasoro algebra. Our results therefore lead to an explicit formula for this charge. We also show that the entropy is related to solutions of ordinary differential equations of Painlev\'e type. In some cases these solutions can be evaluated to all orders using recurrence relations."
"In this thesis, we explore two approaches to string phenomenology. In the first half of the work, we investigate M-theory compactifications on spaces with co-dimension four, orbifold singularities. We construct M-theory on C^2/Z_N by coupling 11-dimensional supergravity to a seven-dimensional Yang-Mills theory located on the orbifold fixed-plane. The resulting action is supersymmetric to leading non-trivial order in the 11-dim Newton constant. We thereby reduce M-theory on a G2 orbifold with C^2/Z_N singularities, explicitly incorporating the additional gauge fields at the singularities. We derive the Kahler potential, gauge-kinetic function and superpotential for the resulting N=1 four-dimensional theory. Blowing-up of the orbifold is described by a Higgs effect and the results are consistent with the corresponding ones obtained for smooth G2 spaces. Further, we consider flux and Wilson lines on singular loci of the G2 space, and discuss the relation to N=4 SYM theory.   In the second half, we develop an algorithmic framework for E8 x E8 heterotic compactifications with monad bundles. We begin by considering cyclic Calabi-Yau manifolds where we classify positive monad bundles, prove stability, and compute the complete particle spectrum for all bundles. Next, we generalize the construction to bundles on complete intersection Calabi-Yau manifolds. We show that the class of positive monad bundles, subject to the heterotic anomaly condition, is finite (~7000 models). We compute the particle spectrum for these models and develop new techniques for computing the cohomology of line bundles. There are no anti-generations of particles and the spectrum is manifestly moduli-dependent. We further study the slope-stability of positive monad bundles and develop a new method for proving stability of SU(n) vector bundles."
"In this paper we study the parameterized complexity of two well-known permutation group problems which are NP-complete. 1. Given a permutation group G=<S>, subgroup of $S_n$, and a parameter $k$, find a permutation $\pi$ in G such that $|{i\in [n]\mid \pi(i)\ne i}|$ is at least $k$. This generalizes the well-known NP-complete problem of finding a fixed-point free permutation in G. (this is the case when $k=n$). We show that this problem with parameter $k$ is fixed parameter tractable. In the process, we give a simple deterministic polynomial-time algorithm for finding a fixed point free element in a transitive permutation group, answering an open question of Cameron. 2. Next we consider the problem of computing a base for a permutation group G=<S>. A base for G is a subset B of $[n]$ such that the subgroup of G that fixes B pointwise is trivial. This problem is known to be NP-complete. We show that it is fixed parameter tractable for the case of cyclic permutation groups and for permutation groups of constant orbit size. For more general classes of permutation groups we do not know whether the problem is in FPT or is W[1]-hard."
"The winning condition of a parity game with costs requires an arbitrary, but fixed bound on the cost incurred between occurrences of odd colors and the next occurrence of a larger even one. Such games quantitatively extend parity games while retaining most of their attractive properties, i.e, determining the winner is in NP and co-NP and one player has positional winning strategies.   We show that the characteristics of parity games with costs are vastly different when asking for strategies realizing the minimal such bound: The solution problem becomes PSPACE-complete and exponential memory is both necessary in general and always sufficient. Thus, solving and playing parity games with costs optimally is harder than just winning them. Moreover, we show that the tradeoff between the memory size and the realized bound is gradual in general. All these results hold true for both a unary and binary encoding of costs.   Moreover, we investigate Streett games with costs. Here, playing optimally is as hard as winning, both in terms of complexity and memory."
"Enterprises are investing heavily in cloud data centers to meet the ever surging business demand. Data Center is a facility, which houses computer systems and associated components, such as telecommunications and storage systems. It generally includes power supply equipment, communication connections and cooling equipment. A large data center can use as much electricity as a small town. Due to the emergence of data center based computing services, it has become necessary to examine how the costs associated with data centers evolve over time, mainly in view of efficiency issues. We have presented a quasi form of Cobb Douglas model, which addresses revenue and profit issues in running large data centers. The stochastic form has been introduced and explored along with the quasi Cobb Douglas model to understand the behavior of the model in depth. Harrod neutrality and Solow neutrality are incorporated in the model to identify the technological progress in cloud data centers. This allows us to shed light on the stochastic uncertainty of cloud data center operations. A general approach to optimizing the revenue cost of data centers using Cobb Douglas Stochastic Frontier Analysis,CDSFA is presented. Next, we develop the optimization model for large data centers. The mathematical basis of CDSFA has been utilized for cost optimization and profit maximization in data centers. The results are found to be quite useful in view of production reorganization in large data centers around the world."
"Approximation problems involving a single convex body in $d$-dimensional space have received a great deal of attention in the computational geometry community. In contrast, works involving multiple convex bodies are generally limited to dimensions $d \leq 3$ and/or do not consider approximation. In this paper, we consider approximations to two natural problems involving multiple convex bodies: detecting whether two polytopes intersect and computing their Minkowski sum. Given an approximation parameter $\varepsilon > 0$, we show how to independently preprocess two polytopes $A,B$ into data structures of size $O(1/\varepsilon^{(d-1)/2})$ such that we can answer in polylogarithmic time whether $A$ and $B$ intersect approximately. More generally, we can answer this for the images of $A$ and $B$ under affine transformations. Next, we show how to $\varepsilon$-approximate the Minkowski sum of two given polytopes defined as the intersection of $n$ halfspaces in $O(n \log(1/\varepsilon) + 1/\varepsilon^{(d-1)/2 + \alpha})$ time, for any constant $\alpha > 0$. Finally, we present a surprising impact of these results to a well studied problem that considers a single convex body. We show how to $\varepsilon$-approximate the width of a set of $n$ points in $O(n \log(1/\varepsilon) + 1/\varepsilon^{(d-1)/2 + \alpha})$ time, for any constant $\alpha > 0$, a major improvement over the previous bound of roughly $O(n + 1/\varepsilon^{d-1})$ time."
"We introduce a graphical framework for fair division in cake cutting, where comparisons between agents are limited by an underlying network structure. We generalize the classical fairness notions of envy-freeness and proportionality to this graphical setting. Given a simple undirected graph G, an allocation is envy-free on G if no agent envies any of her neighbor's share, and is proportional on G if every agent values her own share no less than the average among her neighbors, with respect to her own measure. These generalizations open new research directions in developing simple and efficient algorithms that can produce fair allocations under specific graph structures.   On the algorithmic frontier, we first propose a moving-knife algorithm that outputs an envy-free allocation on trees. The algorithm is significantly simpler than the discrete and bounded envy-free algorithm recently designed by Aziz and Mackenzie for complete graphs. Next, we give a discrete and bounded algorithm for computing a proportional allocation on descendant graphs, a class of graphs by taking a rooted tree and connecting all its ancestor-descendant pairs."
"We relate the computational complexity of finite strings to universal representations of their underlying symmetries. First, Boolean functions are classified using the universal covering topologies of the circuits which enumerate them. A binary string is classified as a fixed point of its automorphism group; the irreducible representation of this group is the string's universal covering group. Such a measure may be used to test the quasi-randomness of binary sequences with regard to first-order set membership. Next, strings over general alphabets are considered. The complexity of a general string is given by a universal representation which recursively factors the codeword number associated with a string. This is the complexity of the representation recursively decoding a Godel number having the value of the string; the result is a tree of prime numbers which forms a universal representation of the string's group symmetries."
"The self-force acting on a (scalar or electric) charge held in place outside a massive body contains information about the body's composition, and can therefore be used as a probe of internal structure. We explore this theme by computing the (scalar or electromagnetic) self-force when the body is a spherical ball of perfect fluid in hydrostatic equilibrium, under the assumption that its rest-mass density and pressure are related by a polytropic equation of state. The body is strongly self-gravitating, and all computations are performed in exact general relativity. The dependence on internal structure is best revealed by expanding the self-force in powers of 1/r, with r denoting the radial position of the charge outside the body. To the leading order, the self-force scales as 1/r^3 and depends only on the square of the charge and the body's mass; the leading self-force is universal. The dependence on internal structure is seen at the next order, 1/r^5, through a structure factor that depends on the equation of state. We compute this structure factor for relativistic polytropes, and show that for a fixed mass, it increases linearly with the body's radius in the case of the scalar self-force, and quadratically with the body's radius in the case of the electromagnetic self-force. In both cases we find that for a fixed mass and radius, the self-force is smaller if the body is more centrally dense, and larger if the mass density is more uniformly distributed."
"The first output-sensitive algorithm for the Maximal Clique Listing problem was given by Tsukiyama et.al. in 1977. As any algorithm falling within the Reverse Search paradigm, it performs a DFS visit of a directed tree (the RS-tree) having the objects to be listed (i.e. maximal cliques) as its nodes. In a recursive implementation, the RS-tree corresponds to the recursion tree of the algorithm. The time delay is given by the cost of generating the next child of a node, and Tsukiyama showed it is $O(mn)$. In 2004, Makino and Uno sharpened the time delay to $O(n^{\omega})$ by generating all the children of a node in one single shot performed by computing a \emph{square} fast matrix multiplication. In this paper, we further improve the asymptotics for the exploration of the same RS-tree by grouping the offsprings' computation even further. Our idea is to rely on rectangular fast matrix multiplication in order to compute all children of $n^2$ nodes in one shot. According to the current upper bounds on fast matrix multiplication, with this the time delay improves from $O(n^{2.3728639})$ to $O(n^{2.093362})$."
"Biosciences have been revolutionized by next generation sequencing (NGS) technologies in last years, leading to new perspectives in medical, industrial and environmental applications. And although our motivation comes from biosciences, the following is true for many areas of science: published results are usually hard to reproduce either because data is not available or tools are not readily available, which delays the adoption of new methodologies and hinders innovation. Our focus is on tool readiness and pipelines availability. Even though most tools are freely available, pipelines for data analysis are in general barely described and their configuration is far from trivial, with many parameters to be tuned.   In this paper we discuss how to effectively build and use pipelines, relying on state of the art computing technologies to execute them without users need to configure, install and manage tools, servers and complex workflow management systems. We perform an in depth comparative analysis of state of the art frameworks and systems. The NGSPipes framework is proposed showing that we can have public pipelines ready to process and analyse experimental data, produced for instance by high-throughput technologies, but without relying on centralized servers or Web services.   The NGSPipes framework and underlying architecture provides a major step towards open science and true collaboration in what concerns tools and pipelines among computational biology researchers and practitioners. We show that it is possible to execute data analysis pipelines in a decentralized and platform independent way. Approaches like the one proposed are crucial for archiving and reusing data analysis pipelines at medium/long-term. NGSPipes framework is freely available at http://ngspipes.github.io/."
"Recently we have proposed an effective Hartree-Fock (EHF) theory for the electrons of the muonic molecules that is formally equivalent to the HF theory within the context of the Nuclear-Electronic Orbital theory [Phys. Chem. Chem. Phys. 20, 4466 (2018)]. In the present report we extend the muon-specific effective electronic structure theory beyond the EHF level by introducing the effective second order Muller-Plesset perturbation theory (EMP2) and the effective coupled-cluster theory at single and double excitation levels (ECCSD) as well as an improved version including perturbative triple excitations (ECCSD(T)). These theories incorporate electron-electron correlation into the effective paradigm and through their computational implementation, a diverse set of small muonic species is considered as a benchmark at these post-EHF levels. A comparative computational study on this set demonstrates that the muonic bond length is in general non-negligibly longer than corresponding hydrogenic analogs. Next, the developed post-EHF theories are applied for the muoniated N-Heterocyclic carbene/silylene/germylene and the muoniated triazolium cation revealing the relative stability of the sticking sites of the muon in each species. The computational results, in line with previously reported experimental data demonstrate that the muon generally prefers to attach to the divalent atom with carbeneic nature. A detailed comparison of these muonic adducts with the corresponding hydrogenic adducts reveals subtle differences that have already been overlooked."
"We present a novel and effective method for detecting 3D primitives in cluttered, unorganized point clouds, without axillary segmentation or type specification. We consider the quadric surfaces for encapsulating the basic building blocks of our environments - planes, spheres, ellipsoids, cones or cylinders, in a unified fashion. Moreover, quadrics allow us to model higher degree of freedom shapes, such as hyperboloids or paraboloids that could be used in non-rigid settings.   We begin by contributing two novel quadric fits targeting 3D point sets that are endowed with tangent space information. Based upon the idea of aligning the quadric gradients with the surface normals, our first formulation is exact and requires as low as four oriented points. The second fit approximates the first, and reduces the computational effort. We theoretically analyze these fits with rigor, and give algebraic and geometric arguments. Next, by re-parameterizing the solution, we devise a new local Hough voting scheme on the null-space coefficients that is combined with RANSAC, reducing the complexity from $O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes without segmentation. Our extensive qualitative and quantitative results show that our method is efficient and flexible, as well as being accurate."
"We propose the first framework for defining relational program logics for arbitrary monadic effects. The framework is embedded within a relational dependent type theory and is highly expressive. At the semantic level, we provide an algebraic presentation of relational specifications as a class of relative monads, and link computations and specifications by introducing relational effect observations, which map pairs of monadic computations to relational specifications in a way that respects the algebraic structure. For an arbitrary relational effect observation, we generically define the core of a sound relational program logic, and explain how to complete it to a full-fledged logic for the monadic effect at hand. We show that this generic framework can be used to define relational program logics for effects as diverse as state, input-output, nondeterminism, and discrete probabilities. We, moreover, show that by instantiating our framework with state and unbounded iteration we can embed a variant of Benton's Relational Hoare Logic, and also sketch how to reconstruct Relational Hoare Type Theory. Finally, we identify and overcome conceptual challenges that prevented previous relational program logics from properly dealing with control effects, and are the first to provide a relational program logic for exceptions."
"Neural networks are complex algorithms that loosely model the behaviour of the human brain. They play a significant role in computational neuroscience and artificial intelligence. The next generation of neural network models is based on the spike timing activity of neurons: spiking neural networks (SNNs). However, model parameters in SNNs are difficult to search and optimise. Previous studies using genetic algorithm (GA) optimisation of SNNs were focused mainly on simple, feedforward, or oscillatory networks, but not much work has been done on optimising cortex-like recurrent SNNs. In this work, we investigated the use of GAs to search for optimal parameters in recurrent SNNs to reach targeted neuronal population firing rates, e.g. as in experimental observations. We considered a cortical column based SNN comprising 1000 Izhikevich spiking neurons for computational efficiency and biologically realism. The model parameters explored were the neuronal biased input currents. First, we found for this particular SNN, the optimal parameter values for targeted population averaged firing activities, and the convergence of algorithm by ~100 generations. We then showed that the GA optimal population size was within ~16-20 while the crossover rate that returned the best fitness value was ~0.95. Overall, we have successfully demonstrated the feasibility of implementing GA to optimise model parameters in a recurrent cortical based SNN."
"The DREAM project was funded more than 3 years ago to design and implement a next-generation ESGF (Earth System Grid Federation [1]) architecture which would be suitable for managing and accessing data and services resources on a distributed and scalable environment. In particular, the project intended to focus on the computing and visualization capabilities of the stack, which at the time were rather primitive. At the beginning, the team had the general notion that a better ESGF architecture could be built by modularizing each component, and redefining its interaction with other components by defining and exposing a well defined API. Although this was still the high level principle that guided the work, the DREAM project was able to accomplish its goals by leveraging new practices in IT that started just about 3 or 4 years ago: the advent of containerization technologies (specifically, Docker), the development of frameworks to manage containers at scale (Docker Swarm and Kubernetes), and their application to the commercial Cloud. Thanks to these new technologies, DREAM was able to improve the ESGF architecture (including its computing and visualization services) to a level of deployability and scalability beyond the original expectations."
"The advent of a new generation of large-scale galaxy surveys is pushing cosmological numerical simulations in an uncharted territory. The simultaneous requirements of high resolution and very large volume pose serious technical challenges, due to their computational and data storage demand. In this paper, we present a novel approach dubbed Dynamic Zoom Simulations -- or DZS -- developed to tackle these issues. Our method is tailored to the production of lightcone outputs from N-body numerical simulations, which allow for a more efficient storage and post-processing compared to standard comoving snapshots, and more directly mimic the format of survey data. In DZS, the resolution of the simulation is dynamically decreased outside the lightcone surface, reducing the computational work load, while simultaneously preserving the accuracy inside the lightcone and the large-scale gravitational field. We show that our approach can achieve virtually identical results to traditional simulations at half of the computational cost for our largest box. We also forecast this speedup to increase up to a factor of 5 for larger and/or higher-resolution simulations. We assess the accuracy of the numerical integration by comparing pairs of identical simulations run with and without DZS. Deviations in the lightcone halo mass function, in the sky-projected lightcone, and in the 3D matter lightcone always remain below 0.1%. In summary, our results indicate that the DZS technique may provide a highly-valuable tool to address the technical challenges that will characterise the next generation of large-scale cosmological simulations."
"Verification is a task to check whether a given quantum state is close to an ideal state or not. In this paper, we show that a variety of many-qubit quantum states can be verified with only sequential single-qubit measurements of Pauli operators. First, we introduce a protocol for verifying ground states of Hamiltonians. We next explain how to verify quantum states generated by a certain class of quantum circuits. We finally propose an adaptive test of stabilizers that enables the verification of all polynomial-time-generated hypergraph states, which include output states of the Bremner-Montanaro-Shepherd-type instantaneous quantum polynomial time (IQP) circuits. Importantly, we do not make any assumption that the identically and independently distributed copies of the same states are given: Our protocols work even if some highly complicated entanglement is created among copies in any artificial way. As applications, we consider the verification of the quantum computational supremacy demonstration with IQP models, and verifiable blind quantum computing."
"The design of a stellarator with acceptable confinement properties requires optimization of the magnetic field in the non-convex, high-dimensional spaces describing their geometry. Another major challenge facing the stellarator program is the sensitive dependence of confinement properties on electro-magnetic coil shapes, necessitating the construction of the coils under tight tolerances. In this Thesis, we address these challenges with the application of adjoint methods and shape sensitivity analysis. Adjoint methods enable the efficient computation of the gradient of a function that depends on the solution to a system of equations, such as linear or nonlinear PDEs. This enables gradient-based optimization in high-dimensional spaces and efficient sensitivity analysis. We present the first applications of adjoint methods for stellarator shape optimization. The first example we discuss is the optimization of coil shapes based on the generalization of a continuous current potential model. Understanding the sensitivity of coil metrics to perturbations of the winding surface allows us to understand features of configurations that enable simpler coils. We next consider solutions of the drift-kinetic equation. An adjoint drift-kinetic equation is derived based on the self-adjointness property of the Fokker-Planck collision operator, allowing us to compute the sensitivity of neoclassical quantities to perturbations of the magnetic field strength. Finally, we consider functions that depend on solutions of the MHD equilibrium equations. We generalize the self-adjointness property of the MHD force operator to include perturbations of the rotational transform and the currents outside the confinement region. This self-adjointness property is applied to develop an adjoint method for computing the derivatives of such functions with respect to perturbations of coil shapes or the plasma boundary."
"In this paper, we analyze the performance and energy consumption of an Arm-based high-performance computing (HPC) system developed within the European project Mont-Blanc 3. This system, called Dibona, has been integrated by ATOS/Bull, and it is powered by the latest Marvell's CPU, ThunderX2. This CPU is the same one that powers the Astra supercomputer, the first Arm-based supercomputer entering the Top500 in November 2018. We study from micro-benchmarks up to large production codes. We include an interdisciplinary evaluation of three scientific applications (a finite-element fluid dynamics code, a smoothed particle hydrodynamics code, and a lattice Boltzmann code) and the Graph 500 benchmark, focusing on parallel and energy efficiency as well as studying their scalability up to thousands of Armv8 cores. For comparison, we run the same tests on state-of-the-art x86 nodes included in Dibona and the Tier-0 supercomputer MareNostrum4. Our experiments show that the ThunderX2 has a 25% lower performance on average, mainly due to its small vector unit yet somewhat compensated by its 30% wider links between the CPU and the main memory. We found that the software ecosystem of the Armv8 architecture is comparable to the one available for Intel. Our results also show that ThunderX2 delivers similar or better energy-to-solution and scalability, proving that Arm-based chips are legitimate contenders in the market of next-generation HPC systems."
"Cryo-electron microscopy (cryo-EM) has become a major experimental technique to determine the structures of large protein complexes and molecular assemblies, as evidenced by the 2017 Nobel Prize. Although cryo-EM has been drastically improved to generate high-resolution three-dimensional (3D) maps that contain detailed structural information about macromolecules, the computational methods for using the data to automatically build structure models are lagging far behind. The traditional cryo-EM model building approach is template-based homology modeling. Manual de novo modeling is very time-consuming when no template model is found in the database. In recent years, de novo cryo-EM modeling using machine learning (ML) and deep learning (DL) has ranked among the top-performing methods in macromolecular structure modeling. Deep-learning-based de novo cryo-EM modeling is an important application of artificial intelligence, with impressive results and great potential for the next generation of molecular biomedicine. Accordingly, we systematically review the representative ML/DL-based de novo cryo-EM modeling methods. And their significances are discussed from both practical and methodological viewpoints. We also briefly describe the background of cryo-EM data processing workflow. Overall, this review provides an introductory guide to modern research on artificial intelligence (AI) for de novo molecular structure modeling and future directions in this emerging field."
"Federated edge learning (FEEL) is a promising distributed learning technique for next-generation wireless networks. FEEL preserves the user's privacy, reduces the communication costs, and exploits the unprecedented capabilities of edge devices to train a shared global model by leveraging a massive amount of data generated at the network edge. However, FEEL might significantly shorten energy-constrained participating devices' lifetime due to the power consumed during the model training round. This paper proposes a novel approach that endeavors to minimize computation and communication energy consumption during FEEL rounds to address this issue. First, we introduce a modified local training algorithm that intelligently selects only the samples that enhance the model's quality based on a predetermined threshold probability. Then, the problem is formulated as joint energy minimization and resource allocation optimization problem to obtain the optimal local computation time and the optimal transmission time that minimize the total energy consumption considering the worker's energy budget, available bandwidth, channel states, beamforming, and local CPU speed. After that, we introduce a tractable solution to the formulated problem that ensures the robustness of FEEL. Our simulation results show that our solution substantially outperforms the baseline FEEL algorithm as it reduces the local consumed energy by up to 79%."
"Faster-than-Nyquist (FTN) signaling is a promising non-orthogonal pulse modulation technique that can improve the spectral efficiency (SE) of next generation communication systems at the expense of higher detection complexity to remove the introduced inter-symbol interference (ISI). In this paper, we investigate the detection problem of ultra high-order quadrature-amplitude modulation (QAM) FTN signaling where we exploit a mathematical programming technique based on the alternating directions multiplier method (ADMM). The proposed ADMM sequence estimation (ADMMSE) FTN signaling detector demonstrates an excellent trade-off between performance and computational effort enabling, for the first time in the FTN signaling literature, successful detection and SE gains for QAM modulation orders as high as 64K (65,536). The complexity of the proposed ADMMSE detector is polynomial in the length of the transmit symbols sequence and its sensitivity to the modulation order increases only logarithmically. Simulation results show that for 16-QAM, the proposed ADMMSE FTN signaling detector achieves comparable SE gains to the generalized approach semidefinite relaxation-based sequence estimation (GASDRSE) FTN signaling detector, but at an experimentally evaluated much lower computational time. Simulation results additionally show SE gains for modulation orders starting from 4-QAM, or quadrature phase shift keying (QPSK), up to and including 64K-QAM when compared to conventional Nyquist signaling. The very low computational effort required makes the proposed ADMMSE detector a practically promising FTN signaling detector for both low order and ultra high-order QAM FTN signaling systems."
"A method for incorporating information from next-to-leading order QCD matrix elements for hadronic diboson production into showering event generators is presented. In the hard central region (high jet transverse momentum) where perturbative QCD is reliable, events are sampled according to the first order tree level matrix element. In the soft and collinear regions next-to-leading order corrections are approximated by calculating the differential cross section across the phase space accessible to the parton shower using the first order (virtual graphs included) matrix element. The parton shower then provides an all-orders exclusive description of parton emissions. Events generated in this way provide a physical result across the entire jet transverse momentum spectrum, have next-to-leading order normalization everywhere, and have positive definite event weights. The method is generalizable without modification to any color singlet production process."
"We derive a general resummation formula for transverse-momentum distributions of hard processes at the leading logarithmic level in the high-energy limit, to all orders in the strong coupling. Our result is based on a suitable generalization of high-energy factorization theorems, whereby all-order resummation is reduced to the determination of the Born-level process but with incoming off-shell gluons. We validate our formula by applying it to Higgs production in gluon fusion in the infinite top mass limit. We check our result up to next-to-leading order by comparison to the high energy limit of the exact expression and to next-to-next-to leading by comparison to NNLL order trasverse momentum (Sudakov) resummation, and we predict the high-energy behaviour at next$^3$-to-leading order. We also show that the structure of the result in the small transverse momentum limit agrees to all orders with general constraints from Sudakov resummation."
"Pulsar timing arrays (PTAs) can be used to detect and study gravitational waves in the nanohertz band (i.e., wavelengths of order light-years). This requires high-precision, decades-long data sets from sensitive, instrumentally stable telescopes. NANOGrav and its collaborators in the International Pulsar Timing Array consortium are on the verge of the first detection of the stochastic background produced by supermassive binary black holes, which form via the mergers of massive galaxies. By providing Northern hemisphere sky coverage with exquisite sensitivity and higher frequency coverage compared to the SKA, a Next-Generation Very Large Array (ngVLA) will be a fundamental component in the next phase of nanohertz GW astrophysics, enabling detailed characterization of the stochastic background and the detection of individual sources contributing to the background, as well as detections of (or stringent constraints on) cosmic strings and other exotica. Here we summarize the scientific goals of PTAs and the technical requirements for the ngVLA to play a significant role in the characterization of the nanohertz gravitational wave universe."
"Free space optics (FSO) refers to optical wireless communications in outdoor environments. The aim of this paper is to analyze the role that FSO is envisaged to play in the creation of next-generation satellite networks. To begin with, the reader is introduced to the types of FSO links and functionalities of a basic FSO system. Next, a comparison of FSO and radio frequency (RF) technologies for inter-satellite links (ISLs) is provided, including a comparison between FSO and RF links when employed between low Earth orbit satellites. After that, the types of FSO or laser ISLs are considered, and the challenges in establishing laser ISLs, the properties of laser ISLs, and the capabilities of terminals for laser ISLs are discussed. Then, the parameters of a satellite constellation are highlighted, and different aspects of SpaceX's upcoming mega-constellation Starlink are explored. In addition, the optical wireless satellite network that is created by utilizing laser ISLs is examined. Finally, a use case is investigated for next-generation optical wireless satellite networks that are envisioned by the mid to late 2020s."
"We consider the challenging problem of obtaining an analytic understanding of realistic astrophysical dynamics in the presence of a Vainshtein screened fifth force arising from infrared modifications of General Relativity. In particular, we attempt to solve -- within the most general flat spacetime galileon model -- the scalar force law between well separated bodies located well within the Vainshtein radius of the Sun. To this end, we derive the exact static Green's function of the galileon wave equation linearized about the background field generated by the Sun, for the minimal cubic and maximally quartic galileon theories, and then introduce a method to compute the general leading order force law perturbatively away from these limits. We also show that the same nonlinearities which produce the Vainshtein screening effect present obstacles to an analytic calculation of the galileon forces between closely bound systems within the solar system, such as that of the Earth and Moon. Within the test mass approximation, we deduce that a large enough quartic galileon interaction would suppress the effect on planetary perihelion precession below the level detectable by even the next-generation experiments."
"We introduce a new regularization scheme for Quantum Cosmology in Loop Quantum Gravity (LQG) using the tools of Quantum Reduced Loop Gravity (QRLG). It is obtained considering density matrices for superposition of graphs based on statistical countings of microstates compatible with macroscopic configurations. We call this procedure statistical regularization scheme. In particular, we show how the $\mu_0$ and $\bar{\mu}$ schemes introduced in Loop Quantum Cosmology (LQC) emerge with specific choices of density matrices. Within this new scheme we compute effective Hamiltonians suitable to describe quantum corrected Friedmann and Bianchi I universes and their leading orders coincide with the corresponding effective LQC Hamiltonians in the $\bar{\mu}$ scheme. We compute the next to the leading orders corrections and numerical investigation of the resulting dynamics shows evidence for the emergent-bouncing universe scenario to be a general property of the isotropic sector of QRLG."
"Next-generation sequencing technologies generate millions of short sequence reads, which are usually aligned to a reference genome. In many applications, the key information required for downstream analysis is the number of reads mapping to each genomic feature, for example to each exon or each gene. The process of counting reads is called read summarization. Read summarization is required for a great variety of genomic analyses but has so far received relatively little attention in the literature. We present featureCounts, a read summarization program suitable for counting reads generated from either RNA or genomic DNA sequencing experiments. featureCounts implements highly efficient chromosome hashing and feature blocking techniques. It is considerably faster than existing methods (by an order of magnitude for gene-level summarization) and requires far less computer memory. It works with either single or paired-end reads and provides a wide range of options appropriate for different sequencing applications. featureCounts is available under GNU General Public License as part of the Subread (http://subread.sourceforge.net) or Rsubread (http://www.bioconductor.org) software packages."
"Poincare-Birkhoff-Witt (PBW) Theorems have attracted significant attention since the work of Drinfeld (1986), Lusztig (1989), and Etingof-Ginzburg (2002) on deformations of skew group algebras $H \ltimes {\rm Sym}(V)$, as well as for other cocommutative Hopf algebras $H$. In this paper we show that such PBW theorems do not require the full Hopf algebra structure, by working in the more general setting of a ""cocommutative algebra"", which involves a coproduct but not a counit or antipode. Special cases include infinitesimal Hecke algebras, as well as symplectic reflection algebras, rational Cherednik algebras, and more generally, Drinfeld orbifold algebras. In this generality we identify precise conditions that are equivalent to the PBW property, including a Yetter-Drinfeld type compatibility condition and a Jacobi identity. We also characterize the graded deformations that possess the PBW property. In turn, the PBW property helps identify an analogue of symplectic reflections in general cocommutative bialgebras.   Next, we introduce a family of cocommutative algebras outside the traditionally studied settings: generalized nil-Coxeter algebras. These are necessarily not Hopf algebras, in fact, not even (weak) bialgebras. For the corresponding family of deformed smash product algebras, we compute the center as well as abelianization, and classify all simple modules."
"Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model's performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo-WikiHop, another multi-hop reasoning dataset."
"In an earlier work [S. Kastha et al., PRD {\bf 98}, 124033 (2018)], we developed the {\it parametrized multipolar gravitational wave phasing formula} to test general relativity, for the non-spinning compact binaries in quasi-circular orbit. In this paper, we extend the method and include the important effect of spins in the inspiral dynamics. Furthermore, we consider parametric scaling of PN coefficients of the conserved energy for the compact binary, resulting in the parametrized phasing formula for non-precessing spinning compact binaries in quasi-circular orbit. We also compute the projected accuracies with which the second and third generation ground-based gravitational wave detector networks as well as the planned space-based detector LISA will be able to measure the multipole deformation parameters and the binding energy parameters. Based on different source configurations, we find that a network of third-generation detectors would have comparable ability to that of LISA in constraining the conservative and dissipative dynamics of the compact binary systems. This parametrized multipolar waveform would be extremely useful not only in deriving the first upper limits on any deviations of the multipole and the binding energy coefficients from general relativity using the gravitational wave detections, but also for science case studies of next generation gravitational wave detectors."
"Owing to the lack of defect samples in industrial product quality inspection, trained segmentation model tends to overfit when applied online. To address this problem, we propose a defect sample simulation algorithm based on neural style transfer. The simulation algorithm requires only a small number of defect samples for training, and can efficiently generate simulation samples for next-step segmentation task. In our work, we introduce a masked histogram matching module to maintain color consistency of the generated area and the true defect. To preserve the texture consistency with the surrounding pixels, we take the fast style transfer algorithm to blend the generated area into the background. At the same time, we also use the histogram loss to further improve the quality of the generated image. Besides, we propose a novel structure of segment net to make it more suitable for defect segmentation task. We train the segment net with the real defect samples and the generated simulation samples separately on the button datasets. The results show that the F1 score of the model trained with only the generated simulation samples reaches 0.80, which is better than the real sample result."
"A yuru-chara is a mascot character created by local governments and companies for publicizing information on areas and products. Because it takes various costs to create a yuruchara, the utilization of machine learning techniques such as generative adversarial networks (GANs) can be expected. In recent years, it has been reported that the use of class conditions in a dataset for GANs training stabilizes learning and improves the quality of the generated images. However, it is difficult to apply class conditional GANs when the amount of original data is small and when a clear class is not given, such as a yuruchara image. In this paper, we propose a class conditional GAN based on clustering and data augmentation. Specifically, first, we performed clustering based on K-means++ on the yuru-chara image dataset and converted it into a class conditional dataset. Next, data augmentation was performed on the class conditional dataset so that the amount of data was increased five times. In addition, we built a model that incorporates ResBlock and self-attention into a network based on class conditional GAN and trained the class conditional yuru-chara dataset. As a result of evaluating the generated images, the effect on the generated images by the difference of the clustering method was confirmed."
"We address the problem of layout generation for diverse domains such as images, documents, and mobile applications. A layout is a set of graphical elements, belonging to one or more categories, placed together in a meaningful way. Generating a new layout or extending an existing layout requires understanding the relationships between these graphical elements. To do this, we propose a novel framework, LayoutTransformer, that leverages a self-attention based approach to learn contextual relationships between layout elements and generate layouts in a given domain. The proposed model improves upon the state-of-the-art approaches in layout generation in four ways. First, our model can generate a new layout either from an empty set or add more elements to a partial layout starting from an initial set of elements. Second, as the approach is attention-based, we can visualize which previous elements the model is attending to predict the next element, thereby providing an interpretable sequence of layout elements. Third, our model can easily scale to support both a large number of element categories and a large number of elements per layout. Finally, the model also produces an embedding for various element categories, which can be used to explore the relationships between the categories. We demonstrate with experiments that our model can produce meaningful layouts in diverse settings such as object bounding boxes in scenes (COCO bounding boxes), documents (PubLayNet), and mobile applications (RICO dataset)."
"We aim to solve the highly challenging task of generating continuous sign language videos solely from speech segments for the first time. Recent efforts in this space have focused on generating such videos from human-annotated text transcripts without considering other modalities. However, replacing speech with sign language proves to be a practical solution while communicating with people suffering from hearing loss. Therefore, we eliminate the need of using text as input and design techniques that work for more natural, continuous, freely uttered speech covering an extensive vocabulary. Since the current datasets are inadequate for generating sign language directly from speech, we collect and release the first Indian sign language dataset comprising speech-level annotations, text transcripts, and the corresponding sign-language videos. Next, we propose a multi-tasking transformer network trained to generate signer's poses from speech segments. With speech-to-text as an auxiliary task and an additional cross-modal discriminator, our model learns to generate continuous sign pose sequences in an end-to-end manner. Extensive experiments and comparisons with other baselines demonstrate the effectiveness of our approach. We also conduct additional ablation studies to analyze the effect of different modules of our network. A demo video containing several results is attached to the supplementary material."
"We present a finite difference code intended for computing linear, adiabatic, nonradial pulsations of spherical stars. This code is based on a general Newton - Raphson technique in order to handle the relaxation of the eigenvalue (square of the eigenfrequency) of the modes and their corresponding eigenfunctions. This code has been tested computing the pulsation spectra of polytropic spheres finding a good agreement with previous work. Then, we have coupled this code to our evolutionary code and applied it to the computation of the pulsation spectrum of a low mass, pure - helium white dwarf of 0.3 M_{sun} for a wide range of effective temperatures. In making this calculation we have taken an evolutionary time step short enough such that eigenmodes corresponding to a given model are used as initial approximation to those of the next one. Specifically, we have computed periods, period spacing, eigenfunctions, weight functions, kinetic energies and variational periods for a wide range of modes. To our notice this is the first effort in studying the pulsation properties of helium white dwarfs. The solution we have found working with these realistic white dwarf models are in good accord with the predictions of the asymptotic theory of Tassoul (1980) for high order modes. This indicates that the code presented here is able to work adequately also with realistic stellar models."
"After compilers and operating systems, TSIAs are the third advance in application support. A compiler supports a high level application definition in a programming language. An operating system supports a high level interface to the resources used by an application execution. A Task System and Item Architecture (TSIA) provides an application with a transparent reliable, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, parallel, secure or other execution. In addition to supporting the application execution, a TSIA also supports the application definition. This run-time support for the definition is complementary to the compile-time support of a compiler. For example, this allows a language similar to Fortran or C to deliver features promised by functional computing. While many TSIAs exist, they previously have not been recognized as such and have served only a particular type of application. Existing TSIAs and other projects demonstrate that TSIAs are feasible for most applications. As the next paradigm for application support, the TSIA simplifies and unifies existing computing practice and research. By solving many outstanding problems, the TSIA opens many, many new opportunities for computing."
"We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet."
"Grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (VO). Most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. As a consequence, Grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. Not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific Grid configurations, platforms and infra-structures. Having neither guidelines nor rules in the design of a Grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. It is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows Grid develop-ments requirements. Because Grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. This paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of Grids."
"I show that physical devices that perform observation, prediction, or recollection share an underlying mathematical structure. I call devices with that structure ""inference devices"". I present a set of existence and impossibility results concerning inference devices. These results hold independent of the precise physical laws governing our universe. In a limited sense, the impossibility results establish that Laplace was wrong to claim that even in a classical, non-chaotic universe the future can be unerringly predicted, given sufficient knowledge of the present. Alternatively, these impossibility results can be viewed as a non-quantum mechanical ""uncertainty principle"". Next I explore the close connections between the mathematics of inference devices and of Turing Machines. In particular, the impossibility results for inference devices are similar to the Halting theorem for TM's. Furthermore, one can define an analog of Universal TM's (UTM's) for inference devices. I call those analogs ""strong inference devices"". I use strong inference devices to define the ""inference complexity"" of an inference task, which is the analog of the Kolmogorov complexity of computing a string. However no universe can contain more than one strong inference device. So whereas the Kolmogorov complexity of a string is arbitrary up to specification of the UTM, there is no such arbitrariness in the inference complexity of an inference task. I end by discussing the philosophical implications of these results, e.g., for whether the universe ""is"" a computer."
"Computational astrophysics has undergone unprecedented development over the last decade, becoming a field of its own. The challenge ahead of us will involve increasingly complex multi-scale simulations. These will bridge the gap between areas of astrophysics such as star and planet formation, or star formation and galaxy formation, that have evolved separately until today. A global knowledge of the physics and modeling techniques of astrophysical simulations is thus an important asset for the next generation of modelers. With the aim at fostering such a global approach, we present the Special Issue on Computational Astrophysics for the Advanced Science Letters (http://www.aspbs.com/science.htm). The Advanced Science Letters (ASL) is a new multi-disciplinary scientific journal which will cover extensively computational astrophysics and cosmology, and will act as a forum for the presentation and discussion of novel work attempting to connect different research areas. This Special Issue collects 9 reviews on 9 key topics of modern astrophysics and cosmology in which simulations have had a pivotal contribution. From asteroid collisions to the formation of stars, from the physics and origin of galaxy clusters to large-scale phenomena such as the reionization of the Universe, it is a showcase of state-of-the-art numerical work in a diverse range of topics. The 9 reviews are available on today's listing of the astro-ph archive (arXiv id numbers are provided in this foreword)."
"Relational descriptions have been used in formalizing diverse computational notions, including, for example, operational semantics, typing, and acceptance by non-deterministic machines. We therefore propose a (restricted) logical theory over relations as a language for specifying such notions. Our specification logic is further characterized by an ability to explicitly treat binding in object languages. Once such a logic is fixed, a natural next question is how we might prove theorems about specifications written in it. We propose to use a second logic, called a reasoning logic, for this purpose. A satisfactory reasoning logic should be able to completely encode the specification logic. Associated with the specification logic are various notions of binding: for quantifiers within formulas, for eigenvariables within sequents, and for abstractions within terms. To provide a natural treatment of these aspects, the reasoning logic must encode binding structures as well as their associated notions of scope, free and bound variables, and capture-avoiding substitution. Further, to support arguments about provability, the reasoning logic should possess strong mechanisms for constructing proofs by induction and co-induction. We provide these capabilities here by using a logic called G which represents relations over lambda-terms via definitions of atomic judgments, contains inference rules for induction and co-induction, and includes a special generic quantifier. We show how provability in the specification logic can be transparently encoded in G. We also describe an interactive theorem prover called Abella that implements G and this two-level logic approach and we present several examples that demonstrate the efficacy of Abella in reasoning about computations."
"A binary matrix has the Consecutive Ones Property (C1P) if its columns can be ordered in such a way that all 1's on each row are consecutive. A Minimal Conflicting Set is a set of rows that does not have the C1P, but every proper subset has the C1P. Such submatrices have been considered in comparative genomics applications, but very little is known about their combinatorial structure and efficient algorithms to compute them. We first describe an algorithm that detects rows that belong to Minimal Conflicting Sets. This algorithm has a polynomial time complexity when the number of 1's in each row of the considered matrix is bounded by a constant. Next, we show that the problem of computing all Minimal Conflicting Sets can be reduced to the joint generation of all minimal true clauses and maximal false clauses for some monotone boolean function. We use these methods on simulated data related to ancestral genome reconstruction to show that computing Minimal Conflicting Set is useful in discriminating between true positive and false positive ancestral syntenies. We also study a dataset of yeast genomes and address the reliability of an ancestral genome proposal of the Saccahromycetaceae yeasts."
"We show how to extend a recently proposed multi-level Monte Carlo approach to the continuous time Markov chain setting, thereby greatly lowering the computational complexity needed to compute expected values of functions of the state of the system to a specified accuracy. The extension is non-trivial, exploiting a coupling of the requisite processes that is easy to simulate while providing a small variance for the estimator. Further, and in a stark departure from other implementations of multi-level Monte Carlo, we show how to produce an unbiased estimator that is significantly less computationally expensive than the usual unbiased estimator arising from exact algorithms in conjunction with crude Monte Carlo. We thereby dramatically improve, in a quantifiable manner, the basic computational complexity of current approaches that have many names and variants across the scientific literature, including the Bortz-Kalos-Lebowitz algorithm, discrete event simulation, dynamic Monte Carlo, kinetic Monte Carlo, the n-fold way, the next reaction method,the residence-time algorithm, the stochastic simulation algorithm, Gillespie's algorithm, and tau-leaping. The new algorithm applies generically, but we also give an example where the coupling idea alone, even without a multi-level discretization, can be used to improve efficiency by exploiting system structure. Stochastically modeled chemical reaction networks provide a very important application for this work. Hence, we use this context for our notation, terminology, natural scalings, and computational examples."
"Large scale structure surveys are likely the next leading probe of cosmological information. It is therefore crucial to reliably predict their observables. The Effective Field Theory of Large Scale Structures (EFTofLSS) provides a manifestly convergent perturbation theory for the weakly non-linear regime, where dark matter correlation functions are computed in an expansion of the wavenumber k over the wavenumber associated to the non-linear scale knl. To push the predictions to higher wavenumbers, it is necessary to compute the 2-loop matter power spectrum. For equal-time correlators, exactly as with standard perturturbation theory, there are IR divergences present in each diagram that cancel completely in the final result. We develop a method by which all 2-loop diagrams are computed as one integral, with an integrand that is manifestly free of any IR divergences. This allows us to compute the 2-loop power spectra in a reliable way that is much less numerically challenging than standard techniques. We apply our method to scaling universes where the linear power spectrum is a single power law of k, and where IR divergences can particularly easily interfere with accurate evaluation of loop corrections if not handled carefully. We show that our results are independent of IR cutoff and, after renormalization, of the UV cutoff, and comment how the method presented here naturally generalizes to higher loops."
"The longest common prefix array is a very advantageous data structure that, combined with the suffix array and the Burrows-Wheeler transform, allows to efficiently compute some combinatorial properties of a string useful in several applications, especially in biological contexts. Nowadays, the input data for many problems are big collections of strings, for instance the data coming from ""next-generation"" DNA sequencing (NGS) technologies. In this paper we present the first lightweight algorithm (called extLCP) for the simultaneous computation of the longest common prefix array and the Burrows-Wheeler transform of a very large collection of strings having any length. The computation is realized by performing disk data accesses only via sequential scans, and the total disk space usage never needs more than twice the output size, excluding the disk space required for the input. Moreover, extLCP allows to compute also the suffix array of the strings of the collection, without any other further data structure is needed. Finally, we test our algorithm on real data and compare our results with another tool capable to work in external memory on large collections of strings."
"Two models were recently proposed to explore the robust hardness of Gr\""obner basis computation. Given a polynomial system, both models allow an algorithm to selectively ignore some of the polynomials: the algorithm is only responsible for returning a Gr\""obner basis for the ideal generated by the remaining polynomials. For the $q$-Fractional Gr\""obner Basis Problem the algorithm is allowed to ignore a constant $(1-q)$-fraction of the polynomials (subject to one natural structural constraint). Here we prove a new strongest-parameter result: even if the algorithm is allowed to choose a $(3/10-\epsilon)$-fraction of the polynomials to ignore, and need only compute a Gr\""obner basis with respect to some lexicographic order for the remaining polynomials, this cannot be accomplished in polynomial time (unless $P=NP$). This statement holds even if every polynomial has maximum degree 3. Next, we prove the first robust hardness result for polynomial systems of maximum degree 2: for the $q$-Fractional model a $(1/5-\epsilon)$ fraction of the polynomials may be ignored without losing provable NP-Hardness. Both theorems hold even if every polynomial contains at most three distinct variables. Finally, for the Strong $c$-partial Gr\""obner Basis Problem of De Loera et al. we give conditional results that depend on famous (unresolved) conjectures of Khot and Dinur, et al."
"For effective use of edge computing in an IoT application, we need to partition the application into tasks and map them into the cloud, fog (edge server), device levels such that the resources at the different levels are optimally used to meet the overall quality of service requirements. In this paper, we consider four concerns about application-to-fog mapping: task placement at different levels, data filtering to limit network loading, fog fail-over, and data consistency, and reacting to hotspots at the edge. We describe a programming language and middleware we created for edge computing that addresses the above four concerns. The language has a distributed-node programming model that allows programs to be written for a collection of nodes organized into a cloud, fog, device hierarchy. The paper describes the major design elements of the language and explains the prototype implementation. The unique distributed-node programming model embodied in the language enables new edge-oriented programming patterns that are highly suitable for cognitive or data-intensive edge computing workloads. The paper presents result from an initial evaluation of the language prototype and also a distributed shell and a smart parking app that were developed using the programming language."
"Over the past decade, the fourth paradigm of data-intensive science rapidly became a major driving concept of multiple application domains encompassing and generating large-scale devices such as light sources and cutting edge telescopes. The success of data-intensive projects subsequently triggered the next generation of machine learning approaches. These new artificial intelligent systems clearly represent a paradigm shift from data processing pipelines towards the fifth paradigm of composite cognitive applications requiring the integration of Big Data processing platforms and HPC technologies. The paper addresses the existing impedance mismatch between data-intensive and compute-intensive ecosystems by presenting the Spark-MPI approach based on the MPI Exascale Process Management Interface (PMIx). The approach is demonstrated within the context of hybrid MPI/GPU ptychographic image reconstruction pipelines and distributed deep learning applications."
"Unlike the matrix case, computing low-rank approximations of tensors is NP-hard and numerically ill-posed in general. Even the best rank-1 approximation of a tensor is NP-hard. In this paper, we use convex optimization to develop polynomial-time algorithms for low-rank approximation and completion of positive tensors. Our approach is to use algebraic topology to define a new (numerically well-posed) decomposition for positive tensors, which we show is equivalent to the standard tensor decomposition in important cases. Though computing this decomposition is a nonconvex optimization problem, we prove it can be exactly reformulated as a convex optimization problem. This allows us to construct polynomial-time randomized algorithms for computing this decomposition and for solving low-rank tensor approximation problems. Among the consequences is that best rank-1 approximations of positive tensors can be computed in polynomial time. Our framework is next extended to the tensor completion problem, where noisy entries of a tensor are observed and then used to estimate missing entries. We provide a polynomial-time algorithm that for specific cases requires a polynomial (in tensor order) number of measurements, in contrast to existing approaches that require an exponential number of measurements. These algorithms are extended to exploit sparsity in the tensor to reduce the number of measurements needed. We conclude by providing a novel interpretation of statistical regression problems with categorical variables as tensor completion problems, and numerical examples with synthetic data and data from a bioengineered metabolic network show the improved performance of our approach on this problem."
"The paper investigates efficient distributed computation in dynamic networks in which the network topology changes (arbitrarily) from round to round.   Our first contribution is a rigorous framework for design and analysis of distributed random walk algorithms in dynamic networks. We then develop a fast distributed random walk based algorithm that runs in $\tilde{O}(\sqrt{\tau \Phi})$ rounds (with high probability), where $\tau$ is the dynamic mixing time and $\Phi$ is the dynamic diameter of the network respectively, and returns a sample close to a suitably defined stationary distribution of the dynamic network. We also apply our fast random walk algorithm to devise fast distributed algorithms for two key problems, namely, information dissemination and decentralized computation of spectral properties in a dynamic network.   Our next contribution is a fast distributed algorithm for the fundamental problem of information dissemination (also called as gossip) in a dynamic network. In gossip, or more generally, $k$-gossip, there are $k$ pieces of information (or tokens) that are initially present in some nodes and the problem is to disseminate the $k$ tokens to all nodes. We present a random-walk based algorithm that runs in $\tilde{O}(\min\{n^{1/3}k^{2/3}(\tau \Phi)^{1/3}, nk\})$ rounds (with high probability). To the best of our knowledge, this is the first $o(nk)$-time fully-distributed token forwarding algorithm that improves over the previous-best $O(nk)$ round distributed algorithm [Kuhn et al., STOC 2010], although in an oblivious adversary model.   Our final contribution is a simple and fast distributed algorithm for estimating the dynamic mixing time and related spectral properties of the underlying dynamic network."
"The next generation (5G) cellular network faces the challenges of efficiency, flexibility, and sustainability to support data traffic in the mobile Internet era. To tackle these challenges, cloud-based cellular architectures have been proposed where virtual base stations (VBSs) play a key role. VBSs bring further energy savings but also demands a new energy consumption model as well as the optimization of computational resources. This paper studies the energy-delay tradeoffs of VBSs with delay tolerant traffic. We propose a computational-resource-aware energy consumption model to capture the total energy consumption of a VBS and reflect the dynamic allocation of computational resources including the number of CPU cores and the CPU speed. Based on the model, we analyze the energy-delay tradeoffs of a VBS considering BS sleeping and state switching cost to minimize the weighted sum of power consumption and average delay. We derive the explicit form of the optimal data transmission rate and find the condition under which the energy optimal rate exists and is unique. Opportunities to reduce the average delay and achieve energy savings simultaneously are observed. We further propose an efficient algorithm to jointly optimize the data rate and the number of CPU cores. Numerical results validate our theoretical analyses and under a typical simulation setting we find more than 60% energy savings can be achieved by VBSs compared with conventional base stations under the EARTH model, which demonstrates the great potential of VBSs in 5G cellular systems."
"A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing."
"Automated facial identification and facial expression recognition have been topics of active research over the past few decades. Facial and expression recognition find applications in human-computer interfaces, subject tracking, real-time security surveillance systems and social networking. Several holistic and geometric methods have been developed to identify faces and expressions using public and local facial image databases. In this work we present the evolution in facial image data sets and the methodologies for facial identification and recognition of expressions such as anger, sadness, happiness, disgust, fear and surprise. We observe that most of the earlier methods for facial and expression recognition aimed at improving the recognition rates for facial feature-based methods using static images. However, the recent methodologies have shifted focus towards robust implementation of facial/expression recognition from large image databases that vary with space (gathered from the internet) and time (video recordings). The evolution trends in databases and methodologies for facial and expression recognition can be useful for assessing the next-generation topics that may have applications in security systems or personal identification systems that involve ""Quantitative face"" assessments."
"In future Internet-of-Things networks, sensors or even access points can be mounted on ground/aerial vehicles for smart-city surveillance or environment monitoring. To support the high-mobility sensing with low network latency, a technique called over-the-air-computation (AirComp) was recently developed which enables an access-point to receive a desired function of sensing-data from concurrent-transmissions by exploiting the superposition property of a multi-access-channel. This work aims at further developing AirComp for next-generation multi-antenna multi-modal sensor networks. Specifically, we design beamforming and channel-feedback techniques for multi-function AirComp. Given the objective of minimizing sum-mean-squared-error of computed functions, the optimization of receive-beamforming for multi-function AirComp is a NP-hard problem. The approximate problem based on tightening transmission-power constraints, however, is shown to be solvable using differential-geometry. The solution is proved to be the weighted-centroid of points on a Grassmann-manifold, where each point represents the subspace spanned by the channel matrix of a sensor. As a by-product, the beamforming problem is found to have the same form as the classic problem of multicast-beamforming, establishing the AirComp-multicasting-duality. Its significance lies in making the said Grassmannian-centroid solution transferable to the latter problem which otherwise is solved using the computation-intensive semidefinite-relaxation-technique. Last, building on the AirComp-beamforming solution, an efficient channel-feedback technique is designed for an access-point to receive the beamformer from distributed sensor transmissions of designed signals that are functions of local channel-state-information."
"We propose novel neural temporal models for predicting and synthesizing human motion, achieving state-of-the-art in modeling long-term motion trajectories while being competitive with prior work in short-term prediction and requiring significantly less computation. Key aspects of our proposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate derivative information, and 3) a novel multi-objective loss function that helps the model to slowly progress from simple next-step prediction to the harder task of multi-step, closed-loop prediction. Our results demonstrate that these innovations improve the modeling of long-term motion trajectories. Finally, we propose a novel metric, called Normalized Power Spectrum Similarity (NPSS), to evaluate the long-term predictive ability of motion synthesis models, complementing the popular mean-squared error (MSE) measure of Euler joint angles over time. We conduct a user study to determine if the proposed NPSS correlates with human evaluation of long-term motion more strongly than MSE and find that it indeed does. We release code and additional results (visualizations) for this paper at: https://github.com/cr7anand/neural_temporal_models"
"Fault-tolerant quantum computation promises to solve outstanding problems in quantum chemistry within the next decade. Realizing this promise requires scalable tools that allow users to translate descriptions of electronic structure problems to optimized quantum gate sequences executed on physical hardware, without requiring specialized quantum computing knowledge. To this end, we present a quantum chemistry library, under the open-source MIT license, that implements and enables straightforward use of state-of-art quantum simulation algorithms. The library is implemented in Q#, a language designed to express quantum algorithms at scale, and interfaces with NWChem, a leading electronic structure package. We define a standardized schema for this interface, Broombridge, that describes second-quantized Hamiltonians, along with metadata required for effective quantum simulation, such as trial wavefunction ansatzes. This schema is generated for arbitrary molecules by NWChem, conveniently accessible, for instance, through Docker containers and a recently developed web interface EMSL Arrows. We illustrate use of the library with various examples, including ground- and excited-state calculations for LiH, H$_{10}$, and C$_{20}$ with an active-space simplification, and automatically obtain resource estimates for classically intractable examples."
"A framework for online robust adaptive radiation therapy (ART) is presented. This framework is designed to (i) handle interfractional geometric variations following a probability distribution different from the a priori hypothesis, (ii) address adaptation cost and (iii) address computational tractability. The novelty of this framework is the use of Bayesian inference and scenario-reduction, which is evaluated in a series of treatment on a one-dimensional phantom geometry. The initial robust plan is generated from a robust optimization problem based on either expected-value- or worst-case-optimization approach using the a priori hypothesis of the probability distribution governing the interfractional geometric variations. The actual interfractional variations are evaluated in terms of their likelihood with respect to the a priori hypothesis and violation of user-specified tolerance limits. During an adaptation the a posteriori distribution is computed from the actual variations using Bayesian inference. The adapted plan is optimized to better suit the actual interfractional variations of the individual case, which is used until the next adaptation is. To address adaptation cost, the proposed framework provides an option for increased adaptation frequency. Computational tractability is addressed by scenario-reduction algorithms to reduce the size of the optimization problem. According to the simulations, the proposed framework may improve target coverage compared to the corresponding non-adaptive robust approach. Combining the worst-case-optimization approach with Bayesian inference may perform best in terms of improving CTV coverage and organ-at-risk~(OAR) protection. Bayesian inference may have a greater impact on handling adaptation cost than increased adaptation frequency. The concept of scenario-reduction may be useful to address computational tractability in ART and robust planning."
"Noise is the central obstacle to building large-scale quantum computers. Quantum systems with sufficiently uncorrelated and weak noise could be used to solve computational problems that are intractable with current digital computers. There has been substantial progress towards engineering such systems. However, continued progress depends on the ability to characterize quantum noise reliably and efficiently with high precision. Here we describe such a protocol and report its experimental implementation on a 14-qubit superconducting quantum architecture. The method returns an estimate of the effective noise and can detect correlations within arbitrary sets of qubits. We show how to construct a quantum noise correlation matrix allowing the easy visualization of correlations between all pairs of qubits, enabling the discovery of long-range two-qubit correlations in the 14 qubit device that had not previously been detected. Our results are the first implementation of a provably rigorous and comprehensive diagnostic protocol capable of being run on state of the art devices and beyond. These results pave the way for noise metrology in next-generation quantum devices, calibration in the presence of crosstalk, bespoke quantum error-correcting codes, and customized fault-tolerance protocols that can greatly reduce the overhead in a quantum computation."
"The large dimension ($D$) limit of general relativity has been used in problems involving black holes as an analytical approximation tool. Further it has been proposed that both linear and nonlinear problems involving black holes can be systematically studied in a $1/D$ expansion. Certain quasinormal modes of higher-dimensional Schwarzschild black holes with $\omega \sim \mathcal{O}(1)$ were studied in the large $D$ limit using a $1/D$ expansion for the mode function.   In this paper, we revisit this linear perturbation problem and obtain an analytical equation for the vector quasinormal modes $\omega \sim \mathcal{O}(1)$ in the large $D$ limit, without using a $1/D$ expansion for the mode function. This can be used to compute quasinormal modes to next to leading order in $1/D$. We also compute vector and scalar quasinormal modes with $\omega \sim \mathcal{O}(D)$ in the Laplace transform approach in the large $D$ limit. We discuss the useful features of this approach specifically in the large $D$ limit."
"In Online Social Networks (OSNs), interpersonal communication and information sharing are happening all the time, and it is real-time. When a user initiates an activity in OSNs, immediately, he/she will have a certain influence in his/her friendship circle naturally, some users in the initiator's friendship circle will be attracted to participate in this activity. Based on such a fact, we design a k-hop Collaborate Game Model, which means that an activity initiated by a user can only influence those users whose distance are within k-hop from this initiator in OSNs. Besides, we introduce the problem of Revenue Maximization under k-hop Collaborate Game (RMKCG), which identifies a limited number of initiators in order to obtain revenue as much as possible. Collaborate Game Model describes in detail how to quantify revenue and the logic behind it. We do not know how many followers would be generated from an activity in advance, thus, we need to adopt an adaptive strategy, where the decision who is the next potential initiator depends on the results of past decisions. Adaptive RMKCG problem can be considered as a new stochastic optimization problem, and we prove it is NP-hard, adaptive monotone, but not adaptive submodular. But in some special cases, it is adaptive submodular and an adaptive greedy strategy can obtain a $(1-1/e)$-approximation by adaptive submodularity theory. Due to the complexity of our model, it is hard to compute the marginal gain for each candidate user, then, we propose a convenient and efficient computational method. The effectiveness and correctness of our algorithms are validated by heavy simulation on real-world social networks eventually."
"Internet of Things (IoT) has emerged as one of the key features of the next generation wireless networks, where timely delivery of status update packets is essential for many real-time IoT applications. To provide users with context-aware services and lighten the transmission burden, the raw data usually needs to be preprocessed before being transmitted to the destination. However, the effect of computing on the overall information freshness is not well understood. In this work, we first develop an analytical framework to investigate the information freshness, in terms of peak age of information (PAoI), of a computing enabled IoT system with multiple sensors. Specifically, we model the procedure of computing and transmission as a tandem queue, and derive the analytical expressions of the average PAoI for different sensors. Based on the theoretical results, we formulate a min-max optimization problem to minimize the maximum average PAoI of different sensors. We further design a derivative-free algorithm to find the optimal updating frequency, with which the complexity for checking the convexity of the formulated problem or obtaining the derivatives of the object function can be largely reduced. The accuracy of our analysis and effectiveness of the proposed algorithm are verified with extensive simulation results."
"The next-generation wireless networks are envisioned to support large-scale sensing and distributed machine learning, thereby enabling new intelligent mobile applications. One common network operation will be the aggregation of distributed data (such as sensor observations or AI-model updates) for functional computation (e.g., averaging) so as to support large-scale sensing and distributed machine learning. An efficient solution for data aggregation, called ""over-the-air computation"" (AirComp), embeds functional computation into simultaneous access by many edge devices. Such schemes exploit the waveform superposition of a multi-access channel to allow an access point to receive a desired function of simultaneous signals. In this work, we aim at realizing AirComp in a two-cell multi-antenna system. To this end, a novel scheme of simultaneous signal-and-interference alignment (SIA) is proposed that builds on classic IA to manage interference for multi-cell AirComp. The principle of SIA is to divide the spatial channel space into two subspaces with equal dimensions: one for signal alignment required by AirComp and the other for inter-cell IA. As a result, the number of interference-free spatially multiplexed functional streams received by each AP is maximized (equal to half of the available spatial degrees-of-freedom). Furthermore, the number is independent of the population of devices in each cell. In addition, the extension to SIA for more than two cells is discussed."
"The ever-growing availability of computing power and the sustained development of advanced computational methods have contributed much to recent scientific progress. These developments present new challenges driven by the sheer amount of calculations and data to manage. Next-generation exascale supercomputers will harden these challenges, such that automated and scalable solutions become crucial. In recent years, we have been developing AiiDA (http://www.aiida.net), a robust open-source high-throughput infrastructure addressing the challenges arising from the needs of automated workflow management and data provenance recording. Here, we introduce developments and capabilities required to reach sustained performance, with AiiDA supporting throughputs of tens of thousands processes/hour, while automatically preserving and storing the full data provenance in a relational database making it queryable and traversable, thus enabling high-performance data analytics. AiiDA's workflow language provides advanced automation, error handling features and a flexible plugin model to allow interfacing with any simulation software. The associated plugin registry enables seamless sharing of extensions, empowering a vibrant user community dedicated to making simulations more robust, user-friendly and reproducible."
"Automatic recognition of surgical activities in the operating room (OR) is a key technology for creating next generation intelligent surgical devices and workflow monitoring/support systems. Such systems can potentially enhance efficiency in the OR, resulting in lower costs and improved care delivery to the patients. In this paper, we investigate automatic surgical activity recognition in robot-assisted operations. We collect the first large-scale dataset including 400 full-length multi-perspective videos from a variety of robotic surgery cases captured using Time-of-Flight cameras. We densely annotate the videos with 10 most recognized and clinically relevant classes of activities. Furthermore, we investigate state-of-the-art computer vision action recognition techniques and adapt them for the OR environment and the dataset. First, we fine-tune the Inflated 3D ConvNet (I3D) for clip-level activity recognition on our dataset and use it to extract features from the videos. These features are then fed to a stack of 3 Temporal Gaussian Mixture layers which extracts context from neighboring clips, and eventually go through a Long Short Term Memory network to learn the order of activities in full-length videos. We extensively assess the model and reach a peak performance of 88% mean Average Precision."
"This is a status report on our endeavor to reveal the mechanism of core-collapse supernovae (CCSNe) by large-scale numerical simulations. Multi-dimensionality of the supernova engine, general relativistic magnetohydrodynamics, energy and lepton number transport by neutrinos emitted from the forming neutron star as well as nuclear interactions there, are all believed to play crucial roles in repelling infalling matter and producing energetic explosions. These ingredients are nonlinearly coupled with one another in the dynamics of core-collapse, bounce, and shock expansion. Serious quantitative studies of CCSNe hence make extensive numerical computations mandatory. Since neutrinos are neither in thermal nor in chemical equilibrium in general, their distributions in the phase space should be computed. This is a six dimensional (6D) neutrino transport problem and quite a challenge even for those with an access to the most advanced numerical resources such as the ""K computer"". To tackle this problem, we have embarked on multi-front efforts. In particular we report in this paper our recent progresses in the treatments of multi-dimensional (multi-D) radiation-hydrodynamics. We are currently proceeding on two different paths to the ultimate goal; in one approach we employ an approximate but highly efficient scheme for neutrino transport and treat 3D hydrodynamics and/or general relativity rigorously; some neutrino-driven explosions will be presented and comparisons will be made between 2D and 3D models quantitatively; in the second approach, on the other hand, exact but so far Newtonian Boltzmann equations are solved in two and three spatial dimensions; we will show some demonstrative test simulations. We will also address the perspectives of exa-scale computations on the next generation supercomputers."
"The elucidation upon fly's neuronal patterns as a link to computer graphics and memory cards I/O's, is investigated for the phenomenon by propounding a unified theory of Einstein's two known relativities. It is conclusive that flies could contribute a certain amount of neuromatrices indicating an imagery function of a visual-computational system into computer graphics and storage systems. The visual system involves the time aspect, whereas flies possess faster pulses compared to humans' visual ability due to the E-field state on an active fly's eye surface. This behaviour can be tested on a dissected fly specimen at its ommatidia. Electro-optical contacts and electrodes are wired through the flesh forming organic emitter layer to stimulate light emission, thereby to a computer circuit. The next step is applying a threshold voltage with secondary voltages to the circuit denoting an array of essential electrodes for bit switch. As a result, circuit's dormant pulses versus active pulses at the specimen's area are recorded. The outcome matrix possesses a construction of RGB and time radicals expressing the time problem in consumption, allocating time into computational algorithms, enhancing the technology far beyond. The obtained formulation generates consumed distance cons(x), denoting circuital travel between data source/sink for pixel data and bendable wavelengths. Once 'image logic' is in place, incorporating this point of graphical acceleration permits one to enhance graphics and optimize immensely central processing, data transmissions between memory and computer visual system. The phenomenon can be mainly used in 360-deg. display/viewing, 3D scanning techniques, military and medicine, a robust and cheap substitution for e.g. pre-motion pattern analysis, real-time rendering and LCDs."
"The AI problem has no solution in the environment of existing hardware stack and OS architecture. CPU-centric model of computation has a huge number of drawbacks that originate from memory hierarchy and obsolete architecture of the computing core. The concept of mixing memory and logic has been around since 1960s. However, the concept of Processor-In-Memory (PIM) is unable to resolve the critical issues of the CPU-centric computing model because of inevitable replication of von Neumann architecture's drawbacks. The next generation of NVM/SCM memory is able to give the second birth to the data-centric computing paradigm. This paper presents a concept of Processor in Non-Volatile Memory (PiNVSM) architecture. The basis of PiNVSM architecture is the concept of DPU that contains the NVM memory and dedicated PU. All necessary PU's registers can be implemented in the space of NVM memory. NVM memory of DPU is the single space for storing and transformation of data. In the basis of PiNVSM architecture lies the DPU array is able to overcome the limitations as Turing machine model as von Neumann architecture. The DPU array hasn't a centralized computing core. Every data portion has dedicated computing core that excludes the necessity to transfer data to the place of data processing. Every DPU contains data portion that is associated with the set of keywords. Any complex data structure can be split on elementary items that can be stored into independent DPU with dedicated computing core(s). One DPU is able to apply the elementary transformation on one item. But the DPU array is able to make the transformation of complex structure by means of concurrent execution of elementary transformations in different DPUs. The PiNVSM architecture suggests a principally new architecture of the computing core that creates a new opportunity for data self-organization, data and code synthesis."
"We develop a forward branching phase-space generator for use in next-to-leading order parton level event generators. By performing 2 -> 3 branchings from a fixed jet phase-space point, all bremsstrahlung events contributing to the given jet configuration are generated. The resulting phase-space integration is three-dimensional irrespective of the considered jet multiplicity. In this first study, we use the forward branching phase-space generator to calculate in the leading-color approximation next-to-leading order corrections to fully differential gluonic jet configurations."
"With the proliferation of ultra-high-speed mobile networks and internet-connected devices, along with the rise of artificial intelligence, the world is generating exponentially increasing amounts of data - data that needs to be processed in a fast, efficient and smart way. These developments are pushing the limits of existing computing paradigms, and highly parallelized, fast and scalable hardware concepts are becoming progressively more important. Here, we demonstrate a computational specific integrated photonic tensor core - the optical analog of an ASIC-capable of operating at Tera-Multiply-Accumulate per second (TMAC/s) speeds. The photonic core achieves parallelized photonic in-memory computing using phase-change memory arrays and photonic chip-based optical frequency combs (soliton microcombs). The computation is reduced to measuring the optical transmission of reconfigurable and non-resonant passive components and can operate at a bandwidth exceeding 14 GHz, limited only by the speed of the modulators and photodetectors. Given recent advances in hybrid integration of soliton microcombs at microwave line rates, ultra-low loss silicon nitride waveguides, and high speed on-chip detectors and modulators, our approach provides a path towards full CMOS wafer-scale integration of the photonic tensor core. While we focus on convolution processing, more generally our results indicate the major potential of integrated photonics for parallel, fast, and efficient computational hardware in demanding AI applications such as autonomous driving, live video processing, and next generation cloud computing services."
"With the expansion of high-speed railway network and growth of passenger transportation demands, the fleet size of electric multiple unit (EMU) in China needs to be adjusted accordingly. Generally, an EMU train costs tens of millions of dollars which constitutes a significant portion of capital investment. Thus, the prediction of EMU fleet size has attracted increasing attention from associated railway departments. First, this paper introduces a typical architecture of convolutional neural network (CNN) and its basic theory. Then, some data of nine indices, such as passenger traffic volume and length of high-speed railways in operation, is collected and preprocessed. Next, a CNN and a backpropagation neural network (BPNN) are constructed and trained aiming to predict EMU fleet size in the following years. The differences and performances of these two networks in computation experiments are analyzed in-depth. The results indicate that the CNN is superior to the BPNN both in generalization ability and fitting accuracy, and CNN can serve as an aid in EMU fleet size prediction."
"We present a complete calculation of the photon +~1 jet rate in $e^+e^-$ annihilation up to ${\cal O}(\alpha \alpha_{s})$. Although formally of next-to-leading order in perturbation theory, this calculation contains several ingredients appropriate to a next-to-next-to-leading order calculation of jet observables. In particular, we describe a generalization of the commonly used phase space slicing method to isolate the singularities present when more than one particle is unresolved. Within this approach, we analytically evaluate the singularities associated with the following double unresolved regions; triple collinear, soft/collinear and double single collinear configurations as well as those from the collinear limit of virtual graphs. By comparing the results of our calculation with the existing data on the photon +~1 jet rate from the ALEPH Collaboration at CERN, we make a next-to-leading order determination of the process-independent non-perturbative quark-to-photon fragmentation function $D_{q \to \gamma}(z,\mu_{F})$ at ${\cal O}(\alpha \alpha_{s})$. As a first application of this measurement allied with our improved perturbative calculation, we determine the dependence of the isolated photon +~1 jet cross section in a democratic clustering approach on the jet resolution parameter $\ycut$ at next-to-leading order. The next-to-leading order corrections to this observable are moderate but improve the agreement between theoretical prediction and experimental data."
"In this paper we describe an innovative type of Time Projection Chamber (TPC), which uses high-pressure xenon gas (HPXe) and electroluminescence amplification of the ionization charge as the basis of an apparatus capable of fully reconstructing the energy and topological signature of rare events. We will discuss a specific design of such HPXe TPC, the NEXT-100 detector, that will search for neutrinoless double beta decay events using 100-150 kg of xenon enriched in the isotope Xe-136. NEXT-100 is currently under construction, after completion of an accelerated and very successful R&D period. It will be installed at the Laboratorio Subterr\'aneo de Canfranc (LSC), in Spain. The commissioning run is expected for late 2013 or early 2014. We will also present physics arguments that suggest that the HPXe technology can be extrapolated to the next-to-next generation (e.g, a fiducial mass of 1 ton of target), which will fully explore the Majorana nature of the neutrino if the mass hierarchy is inverse."
"I describe the performance of the CRBLASTER computational framework on a 350-MHz 49-core Maestro Development Board (MDB). The 49-core Interim Test Chip (ITC) was developed by the U.S. Government and is based on the intellectual property of the 64-core TILE64 processor of the Tilera Corporation. The Maestro processor is intended for use in the high radiation environments found in space; the ITC was fabricated using IBM 90-nm CMOS 9SF technology and Radiation-Hardening-by-Design (RHDB) rules. CRBLASTER is a parallel-processing cosmic-ray rejection application based on a simple computational framework that uses the high-performance computing industry standard Message Passing Interface (MPI) library. CRBLASTER was designed to be used by research scientists to easily port image-analysis programs based on embarrassingly-parallel algorithms to a parallel-processing environment such as a multi-node Beowulf cluster or multi-core processors using MPI. I describe my experience of porting CRBLASTER to the 64-core TILE64 processor, the Maestro simulator, and finally the 49-core Maestro processor itself. Performance comparisons using the ITC are presented between emulating all floating-point operations in software and doing all floating point operations with hardware assist from an IEEE-754 compliant Aurora FPU (floating point unit) that is attached to each of the 49 cores. Benchmarking of the CRBLASTER computational framework using the memory-intensive L.A.COSMIC cosmic ray rejection algorithm and a computational-intensive Poisson noise generator reveal subtleties of the Maestro hardware design. Lastly, I describe the importance of using real scientific applications during the testing phase of next-generation computer hardware; complex real-world scientific applications can stress hardware in novel ways that may not necessarily be revealed while executing simple applications or unit tests."
"We consider the problem of information aggregation in sensor networks, where one is interested in computing a function of the sensor measurements. We allow for block processing and study in-network function computation in directed graphs and undirected graphs. We study how the structure of the function affects the encoding strategies, and the effect of interactive information exchange.   We begin by considering a directed graph G = (V, E) on the sensor nodes, where the goal is to determine the optimal encoders on each edge which achieve function computation at the collector node. Our goal is to characterize the rate region in R^{|E|}, i.e., the set of points for which there exist feasible encoders with given rates which achieve zero-error computation for asymptotically large block length. We determine the solution for directed trees, specifying the optimal encoder and decoder for each edge. For general directed acyclic graphs, we provide an outer bound on the rate region by finding the disambiguation requirements for each cut, and describe examples where this outer bound is tight.   Next, we address the scenario where nodes are connected in an undirected tree network, and every node wishes to compute a given symmetric Boolean function of the sensor data. Undirected edges permit interactive computation, and we therefore study the effect of interaction on the aggregation and communication strategies. We focus on sum-threshold functions, and determine the minimum worst-case total number of bits to be exchanged on each edge. The optimal strategy involves recursive in-network aggregation which is reminiscent of message passing. In the case of general graphs, we present a cutset lower bound, and an achievable scheme based on aggregation along trees. For complete graphs, we prove that the complexity of this scheme is no more than twice that of the optimal scheme."
"High-energy physics data analysis relies heavily on the comparison between experimental and simulated data as stressed lately by the Higgs search at LHC and the recent identification of a Higgs-like new boson. The first link in the full simulation chain is the event generation both for background and for expected signals. Nowadays event generators are based on the automatic computation of matrix element or amplitude for each process of interest.   Moreover, recent analysis techniques based on the matrix element likelihood method assign probabilities for every event to belong to any of a given set of possible processes. This method originally used for the top mass measurement, although computing intensive, has shown its power at LHC to extract the new boson signal from the background.   Serving both needs, the automatic calculation of matrix element is therefore more than ever of prime importance for particle physics. Initiated in the eighties, the techniques have matured for the lowest order calculations (tree-level), but become complex and CPU time consuming when higher order calculations involving loop diagrams are necessary like for QCD processes at LHC. New calculation techniques for next-to-leading order (NLO) have surfaced making possible the generation of processes with many final state particles (up to 6). If NLO calculations are in many cases under control, although not yet fully automatic, even higher precision calculations involving processes at 2-loops or more remain a big challenge.   After a short introduction to particle physics and to the related theoretical framework, we will review some of the computing techniques that have been developed to make these calculations automatic. The main available packages and some of the most important applications for simulation and data analysis, in particular at LHC will also be summarized."
"In many current speech recognizers, a statistical language model is used to indicate how likely it is that a certain word will be spoken next, given the words recognized so far. How can statistical language models be improved so that more complex speech recognition tasks can be tackled? Since the knowledge of the weaknesses of any theory often makes improving the theory easier, the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them. To that end, we formally define a weakness of a statistical language model in terms of the logarithm of the total probability, LTP, a term closely related to the standard perplexity measure used to evaluate statistical language models. We apply our definition of a weakness to a frequently used statistical language model, called a bi-pos model. This results, for example, in a new modeling of unknown words which improves the performance of the model by 14% to 21%. Moreover, one of the identified weaknesses has prompted the development of our generalized N-pos language model, which is also outlined in this thesis. It can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional N-pos model. This leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge. These results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general."
"In the present paper we construct differential invariants for generic rank 2 vector distributions on n-dimensional manifold. In the case n=5 (the first case containing functional parameters) E. Cartan found in 1910 the covariant fourth-order tensor invariant for such distributions, using his ""reduction-prolongation"" procedure. After Cartan's work the following questions remained open: first the geometric reason for existence of Cartan's tensor was not clear; secondly it was not clear how to generalize this tensor to other classes of distributions; finally there were no explicit formulas for computation of Cartan's tensor. Our paper is the first in the series of papers, where we develop an alternative approach, which gives the answers to the questions mentioned above. It is based on the investigation of dynamics of the field of so-called abnormal extremals (singular curves) of rank 2 distribution and on the general theory of unparametrized curves in the Lagrange Grassmannian, developed in our previous works with A. Agrachev . In this way we construct the fundamental form and the projective Ricci curvature of rank 2 vector distributions for arbitrary n greater than 4.   For n=5 we give an explicit method for computation of these invariants and demonstrate it on several examples. In our next paper we show that in the case n=5 our fundamental form coincides with Cartan's tensor."
"We describe SuperIso v3.0 which is a public program for evaluation of flavor physics observables in the Standard Model (SM), general two-Higgs-doublet model (2HDM), minimal supersymmetric Standard Model (MSSM) and next to minimal supersymmetric Standard Model (NMSSM). SuperIso v3.0, in addition to the branching ratio of B -> X_s gamma and the isospin asymmetry of B -> K* gamma, incorporates other flavor observables such as the branching ratio of B_s -> mu+ mu-, the branching ratio of B -> tau nu_tau, the branching ratio of B -> D tau nu_tau, the branching ratio of K -> mu nu_mu and the branching ratios of D_s -> tau nu_tau and D_s -> mu nu_mu. The calculation of the branching ratio of B -> X_s gamma is also improved, as it includes NNLO Standard Model contributions. The program also computes the muon anomalous magnetic moment (g-2). Nine sample models are included in the package, namely SM, 2HDM, and mSUGRA, NUHM, AMSB and GMSB for the MSSM, and CNMSSM, NGMSB and NNUHM for the NMSSM. SuperIso uses a SUSY Les Houches Accord file (SLHA1 or SLHA2) as input, which can be either generated automatically by the program via a call to external spectrum calculators, or provided by the user. The calculation of the observables is detailed in the Appendices, where a suggestion for the allowed intervals for each observable is also provided."
"We compute the flux of linear momentum carried by gravitational waves emitted from spinning binary black holes at 2PN order for generic orbits. In particular we provide explicit expressions of three new types of terms, namely next-to-leading order spin-orbit terms at 1.5 PN order, spin-orbit tail terms at 2PN order, and spin-spin terms at 2PN order. Restricting ourselves to quasi-circular orbits, we integrate the linear momentum flux over time to obtain the recoil velocity as function of orbital frequency. We find that in the so-called superkick configuration the higher-order spin corrections can increase the recoil velocity up to about a factor 3 with respect to the leading-order PN prediction. Furthermore, we provide expressions valid for generic orbits, and accurate at 2PN order, for the energy and angular momentum carried by gravitational waves emitted from spinning binary black holes. Specializing to quasi-circular orbits we compute the spin-spin terms at 2PN order in the expression for the evolution of the orbital frequency and found agreement with Mik\'oczi, Vas\'uth and Gergely. We also verified that in the limit of extreme mass ratio our expressions for the energy and angular momentum fluxes match the ones of Tagoshi, Shibata, Tanaka and Sasaki obtained in the context of black hole perturbation theory."
"The scale of Baryon Acoustic Oscillations (BAO) imprinted in the matter power spectrum provides an almost-perfect standard ruler: it only suffers sub-percent deviations from fixed comoving length due to non-linear effects. We study the BAO shift in the large Horndeski class of gravitational theories and compute its magnitude in momentum space using second-order perturbation theory and a peak-background split. The standard prediction is affected by the modified linear growth, as well as by non-linear gravitational effects that alter the mode-coupling kernel. For covariant Galileon models, we find a $14-45\%$ enhancement of the BAO shift with respect to standard gravity and a distinct time evolution depending on the parameters. Despite the larger values, the shift remains well below the forecasted precision of next-generation galaxy surveys. Models that produce significant BAO shift would cause large redshift-space distortions or affect the bispectrum considerably. Our computation therefore validates the use of the BAO scale as a comoving standard ruler for tests of general dark energy models."
"Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing. The task is to generate a textual description of the content of an image. The typical model used for image captioning is an encoder-decoder deep network, where the encoder captures the essence of an image while the decoder is responsible for generating a sentence describing the image. Attention mechanisms can be used to automatically focus the decoder on parts of the image which are relevant to predict the next word. In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. The image captioning module is available as part of SOCKEYE at https://github.com/awslabs/sockeye which tutorial can be found at https://awslabs.github.io/sockeye/image_captioning.html ."
"We consider here a single-compartment model of these neurons which is capable of describing many of the known features of spike generation, particularly the slow rhythmic pacemaking activity often observed in these cells in a variety of species. Included in the model are ten kinds of voltage dependent ion channels as well as calcium-dependent potassium current. Calcium dynamics includes buffering and pumping. In sections 3-9, each component is considered in detail and parameters estimated from voltage clamp data where possible. In the next two sections simplified versions of some components are employed to explore the effects of various parameters on spiking, using a systematic approach, ending up with the following eleven components: a fast sodium current $I_{Na}$, a delayed rectifier potassium current $I_{KDR}$, a transient potassium current $I_A$, a low-threshold calcium current $I_T$, two high threshold calcium currents $I_L$ and $I_N$, small and large conductance potassium currents $I_{SK}$ and $I_{BK}$, a hyperpolarization-activated cation current $I_H$, a leak current $I_{Leak}$ and intracellular calcium ion concentration $Ca_i$. Attention is focused on the properties usually associated with these neurons, particularly long duration of action potential, pacemaker-like spiking and the ramp-like return to threshold after a spike. In some cases the membrane potential trajectories display doublets or have kinks or notches as have been reported in some experimental studies. The computed time courses of $I_A$ and $I_T$ during the interspike interval support the generally held view of a competition between them in influencing the frequency of spiking. Spontaneous spiking could be obtained with small changes in a few parameters from their values with driven spiking."
"Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively."
"With the emergence of high-resolution fingerprint sensors, there has been a lot of focus on level-3 fingerprint features, especially the pores, for the next generation automated fingerprint recognition systems (AFRS). Following the success of deep learning in various computer vision tasks, researchers have developed learning-based approaches for detection of pores in high-resolution fingerprint images. Generally, learning-based approaches provide better performance than handcrafted feature-based approaches. However, domain adaptability of the existing learning-based pore detection methods has never been studied. In this paper, we study this aspect and propose an approach for pore detection in cross-sensor scenarios. For this purpose, we have generated an in-house 1000 dpi fingerprint dataset with ground truth pore coordinates (referred to as IITI-HRFP-GT), and evaluated the performance of the existing learning-based pore detection approaches. The core of the proposed approach for detection of pores in cross-sensor scenarios is DeepDomainPore, which is a residual learning-based convolutional neural network(CNN) trained for pore detection. The domain adaptability in DeepDomainPore is achieved by embedding a gradient reversal layer between the CNN and a domain classifier network. The proposed approach achieves state-of-the-art performance in a cross-sensor scenario involving public high-resolution fingerprint datasets with 88.12% true detection rate and 83.82% F-score."
"Cosmological datasets have great potential to elucidate the nature of dark energy and test gravity on the largest scales available to observation. Theoretical predictions can be computed with hi_class (www.hiclass-code.net), an accurate, fast and flexible code for linear cosmology, incorporating a wide range of dark energy theories and modifications to general relativity. We introduce three new functionalities into hi_class: (1) Support for models based on covariant Lagrangians, including a constraint-preserving integration scheme for the background evolution and a series of worked-out examples: Galileon, nKGB, quintessence (monomial, tracker) and Brans-Dicke. (2) Consistent initial conditions for the scalar-field perturbations in the deep radiation era, identifying the conditions under which modified-gravity isocurvature perturbations may grow faster than adiabatic modes leading to a loss of predictivity. (3) An automated quasistatic approximation scheme allowing order-of-magnitude improvement in computing performance without sacrificing accuracy for wide classes of models. These enhancements bring the treatment of dark energy and modified gravity models to the level of detail comparable to software tools restricted to standard $\Lambda$CDM cosmologies. The hi_class code is publicly available (https://github.com/miguelzuma/hi_class_public), ready to explore current data and prepare for next-generation experiments."
"We propose a new computationally efficient sampling scheme for Bayesian inference involving high dimensional probability distributions. Our method maps the original parameter space into a low-dimensional latent space, explores the latent space to generate samples, and maps these samples back to the original space for inference. While our method can be used in conjunction with any dimension reduction technique to obtain the latent space, and any standard sampling algorithm to explore the low-dimensional space, here we specifically use a combination of auto-encoders (for dimensionality reduction) and Hamiltonian Monte Carlo (HMC, for sampling). To this end, we first run an HMC to generate some initial samples from the original parameter space, and then use these samples to train an auto-encoder. Next, starting with an initial state, we use the encoding part of the autoencoder to map the initial state to a point in the low-dimensional latent space. Using another HMC, this point is then treated as an initial state in the latent space to generate a new state, which is then mapped to the original space using the decoding part of the auto-encoder. The resulting point can be treated as a Metropolis-Hasting (MH) proposal, which is either accepted or rejected. While the induced dynamics in the parameter space is no longer Hamiltonian, it remains time reversible, and the Markov chain could still converge to the canonical distribution using a volume correction term. Dropping the volume correction step results in convergence to an approximate but reasonably accurate distribution. The empirical results based on several high-dimensional problems show that our method could substantially reduce the computational cost of Bayesian inference."
"In order to successfully perform tasks specified by natural language instructions, an artificial agent operating in a visual world needs to map words, concepts, and actions from the instruction to visual elements in its environment. This association is termed as Task-Oriented Grounding. In this work, we propose a novel Dynamic Attention Network architecture for the efficient multi-modal fusion of text and visual representations which can generate a robust definition of state for the policy learner. Our model assumes no prior knowledge from visual and textual domains and is an end to end trainable. For a 3D visual world where the observation changes continuously, the attention on the visual elements tends to be highly co-related from a one-time step to the next. We term this as ""Dynamic Attention"". In this work, we show that Dynamic Attention helps in achieving grounding and also aids in the policy learning objective. Since most practical robotic applications take place in the real world where the observation space is continuous, our framework can be used as a generalized multi-modal fusion unit for robotic control through natural language. We show the effectiveness of using 1D convolution over Gated Attention Hadamard product on the rate of convergence of the network. We demonstrate that the cell-state of a Long Short Term Memory (LSTM) is a natural choice for modeling Dynamic Attention and shows through visualization that the generated attention is very close to how humans tend to focus on the environment."
"Time Series Classification (TSC) has been an important and challenging task in data mining, especially on multivariate time series and multi-view time series data sets. Meanwhile, transfer learning has been widely applied in computer vision and natural language processing applications to improve deep neural network's generalization capabilities. However, very few previous works applied transfer learning framework to time series mining problems. Particularly, the technique of measuring similarities between source domain and target domain based on dynamic representation such as density estimation with importance sampling has never been combined with transfer learning framework. In this paper, we first proposed a general adaptive transfer learning framework for multi-view time series data, which shows strong ability in storing inter-view importance value in the process of knowledge transfer. Next, we represented inter-view importance through some time series similarity measurements and approximated the posterior distribution in latent space for the importance sampling via density estimation techniques. We then computed the matrix norm of sampled importance value, which controls the degree of knowledge transfer in pre-training process. We further evaluated our work, applied it to many other time series classification tasks, and observed that our architecture maintained desirable generalization ability. Finally, we concluded that our framework could be adapted with deep learning techniques to receive significant model performance improvements."
"When solving, modelling or reasoning about complex problems, it is usually convenient to use the knowledge of a parallel physical system for representing it. This is the case of lumped-circuit abstraction, which can be used for representing mechanical and acoustic systems, thermal and heat-diffusion problems and in general partial differential equations. Integrated photonic platforms hold the prospect to perform signal processing and analog computing inherently, by mapping into hardware specific operations which relies on the wave-nature of their signals, without trusting on logic gates and digital states like electronics. Although, the distributed nature of photonic platforms leads to the absence of an equivalent approximation to Kirchhoffs law, the main principle used for representing physical systems using circuits. Here we argue that in absence of a straightforward parallelism and homomorphism can be induced. We introduce a photonic platform capable of mimicking Kirchhoffs law in photonics and used as node of a finite difference mesh for solving partial differential equation using monochromatic light in the telecommunication wavelength. We experimentally demonstrate generating in one-shot discrete solutions of a Laplace partial differential equation, with an accuracy above 95% relative to commercial solvers, for an arbitrary set of boundary conditions. Our photonic engine can provide a route to achieve chip-scale, fast (10s of ps), and integrable reprogrammable accelerators for the next generation hybrid high performance computing."
"Research has focused on automated methods to effectively detect sexism online. Although overt sexism seems easy to spot, its subtle forms and manifold expressions are not. In this paper, we outline the different dimensions of sexism by grounding them in their implementation in psychological scales. From the scales, we derive a codebook for sexism in social media, which we use to annotate existing and novel datasets, surfacing their limitations in breadth and validity with respect to the construct of sexism. Next, we leverage the annotated datasets to generate adversarial examples, and test the reliability of sexism detection methods. Results indicate that current machine learning models pick up on a very narrow set of linguistic markers of sexism and do not generalize well to out-of-domain examples. Yet, including diverse data and adversarial examples at training time results in models that generalize better and that are more robust to artifacts of data collection. By providing a scale-based codebook and insights regarding the shortcomings of the state-of-the-art, we hope to contribute to the development of better and broader models for sexism detection, including reflections on theory-driven approaches to data collection."
"With the advent of the Internet of Things (IoT), the ever growing number of connected devices observed in recent years and foreseen for the next decade suggests that more and more data will have to be transmitted over a network, before being processed and stored in data centers. Generalized deduplication (GD) is a novel technique to effectively reduce the data storage cost by identifying similar data chunks, and able to gradually reduce the pressure from the network infrastructure by limiting the data that needs to be transmitted.   This paper presents Hermes, an application-level protocol for the data-plane that can operate over generalized deduplication, as well as over classic deduplication. Hermes significantly reduces the data transmission traffic while effectively decreasing the energy footprint, a relevant matter to consider in the context of IoT deployments. We fully implemented Hermes and evaluated its performance using consumer-grade IoT devices (e.g., Raspberry Pi 4B models). Our results highlight several trade-offs that must be taken into account when considering real-world workloads."
"The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring focus areas from passive observations of the user's actions, thereby allowing accurate predictions of future events. We evaluate this technique with a crime map and demonstrate that users' clicks appear in our prediction set 95% - 97% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events, laying the foundation for next-generation tools."
"Simulation is one of the key components in high energy physics. Historically it relies on the Monte Carlo methods which require a tremendous amount of computation resources. These methods may have difficulties with the expected High Luminosity Large Hadron Collider (HL LHC) need, so the experiment is in urgent need of new fast simulation techniques. We introduce a new Deep Learning framework based on Generative Adversarial Networks which can be faster than traditional simulation methods by 5 order of magnitude with reasonable simulation accuracy. This approach will allow physicists to produce a big enough amount of simulated data needed by the next HL LHC experiments using limited computing resources."
"To detect bias in face recognition networks, it can be useful to probe a network under test using samples in which only specific attributes vary in some controlled way. However, capturing a sufficiently large dataset with specific control over the attributes of interest is difficult. In this work, we describe a simulator that applies specific head pose and facial expression adjustments to images of previously unseen people. The simulator first fits a 3D morphable model to a provided image, applies the desired head pose and facial expression controls, then renders the model into an image. Next, a conditional Generative Adversarial Network (GAN) conditioned on the original image and the rendered morphable model is used to produce the image of the original person with the new facial expression and head pose. We call this conditional GAN -- MorphGAN. Images generated using MorphGAN conserve the identity of the person in the original image, and the provided control over head pose and facial expression allows test sets to be created to identify robustness issues of a facial recognition deep network with respect to pose and expression. Images generated by MorphGAN can also serve as data augmentation when training data are scarce. We show that by augmenting small datasets of faces with new poses and expressions improves the recognition performance by up to 9% depending on the augmentation and data scarcity."
"Symbiotic Autonomous Systems (SAS) are advanced intelligent and cognitive systems exhibiting autonomous collective intelligence enabled by coherent symbiosis of human-machine interactions in hybrid societies. Basic research in the emerging field of SAS has triggered advanced general AI technologies functioning without human intervention or hybrid symbiotic systems synergizing humans and intelligent machines into coherent cognitive systems. This work presents a theoretical framework of SAS underpinned by the latest advances in intelligence, cognition, computer, and system sciences. SAS are characterized by the composition of autonomous and symbiotic systems that adopt bio-brain-social-inspired and heterogeneously synergized structures and autonomous behaviors. This paper explores their cognitive and mathematical foundations. The challenge to seamless human-machine interactions in a hybrid environment is addressed. SAS-based collective intelligence is explored in order to augment human capability by autonomous machine intelligence towards the next generation of general AI, autonomous computers, and trustworthy mission-critical intelligent systems. Emerging paradigms and engineering applications of SAS are elaborated via an autonomous knowledge learning system that symbiotically works between humans and cognitive robots."
"A recent study has found that malicious bots generated nearly a quarter of overall website traffic in 2019 [100]. These malicious bots perform activities such as price and content scraping, account creation and takeover, credit card fraud, denial of service, etc. Thus, they represent a serious threat to all businesses in general, but are especially troublesome for e-commerce, travel and financial services. One of the most common defense mechanisms against bots abusing online services is the introduction of Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA), so it is extremely important to understand which CAPTCHA schemes have been designed and their actual effectiveness against the ever-evolving bots. To this end, this work provides an overview of the current state-of-the-art in the field of CAPTCHA schemes and defines a new classification that includes all the emerging schemes. In addition, for each identified CAPTCHA category, the most successful attack methods are summarized by also describing how CAPTCHA schemes evolved to resist bot attacks, and discussing the limitations of different CAPTCHA schemes from the security, usability and compatibility point of view. Finally, an assessment of the open issues, challenges, and opportunities for further study is provided, paving the road toward the design of the next-generation secure and user-friendly CAPTCHA schemes."
"Invariant object recognition is one of the most fundamental cognitive tasks performed by the brain. In the neural state space, different objects with stimulus variabilities are represented as different manifolds. In this geometrical perspective, object recognition becomes the problem of linearly separating different object manifolds. In feedforward visual hierarchy, it has been suggested that the object manifold representations are reformatted across the layers, to become more linearly separable. Thus, a complete theory of perception requires characterizing the ability of linear readout networks to classify object manifolds from variable neural responses.   A theory of the perceptron of isolated points was pioneered by E. Gardner who formulated it as a statistical mechanics problem and analyzed it using replica theory. In this thesis, we generalize Gardner's analysis and establish a theory of linear classification of manifolds synthesizing statistical and geometric properties of high dimensional signals. [..] Next, we generalize our theory further to linear classification of general perceptual manifolds, such as point clouds. We identify that the capacity of a manifold is determined that effective radius, R_M, and effective dimension, D_M. Finally, we show extensions relevant for applications to real data, incorporating correlated manifolds, heterogenous manifold geometries, sparse labels and nonlinear classifications. Then, we demonstrate how object-based manifolds transform in standard deep networks.   This thesis lays the groundwork for a computational theory of neuronal processing of objects, providing quantitative measures for linear separability of object manifolds. We hope this theory will provide new insights into the computational principles underlying processing of sensory representations in biological and artificial neural networks."
"Motivation: The Stochastic Simulation Algorithm (SSA) has largely diffused in the field of systems biology. This approach needs many realizations for establishing statistical results on the system under study. It is very computationnally demanding, and with the advent of large models this burden is increasing. Hence parallel implementation of SSA are needed to address these needs.   At the very heart of the SSA is the selection of the next reaction to occur at each time step, and to the best of our knowledge all implementations are based on an inverse transformation method. However, this method involves a random number of steps to select this next reaction and is poorly amenable to a parallel implementation.   Results: Here, we introduce a parallel acceptance-rejection algorithm to select the K next reactions to occur. This algorithm uses a deterministic number of steps, a property well suited to a parallel implementation. It is simple and small, accurate and scalable. We propose a Graphics Processing Unit (GPU) implementation and validate our algorithm with simulated propensity distributions and the propensity distribution of a large model of yeast iron metabolism. We show that our algorithm can handle thousands of selections of next reaction to occur in parallel on the GPU, paving the way to massive SSA.   Availability: We present our GPU-AR algorithm that focuses on the very heart of the SSA. We do not embed our algorithm within a full implementation in order to stay pedagogical and allows its rapid implementation in existing software. We hope that it will enable stochastic modelers to implement our algorithm with the benefits of their own optimizations."
"ably successful in building a large market and adapting to the changes of the last three decades, its impact on the broader market of information management is surprisingly limited. If we were to design an information management system from scratch, based upon today's requirements and hardware capabilities, would it look anything like today's database systems?"" In this paper, we introduce Impliance, a next-generation information management system consisting of hardware and software components integrated to form an easy-to-administer appliance that can store, retrieve, and analyze all types of structured, semi-structured, and unstructured information. We first summarize the trends that will shape information management for the foreseeable future. Those trends imply three major requirements for Impliance: (1) to be able to store, manage, and uniformly query all data, not just structured records; (2) to be able to scale out as the volume of this data grows; and (3) to be simple and robust in operation. We then describe four key ideas that are uniquely combined in Impliance to address these requirements, namely the ideas of: (a) integrating software and off-the-shelf hardware into a generic information appliance; (b) automatically discovering, organizing, and managing all data - unstructured as well as structured - in a uniform way; (c) achieving scale-out by exploiting simple, massive parallel processing, and (d) virtualizing compute and storage resources to unify, simplify, and streamline the management of Impliance. Impliance is an ambitious, long-term effort to define simpler, more robust, and more scalable information systems for tomorrow's enterprises."
"Context. With the arrival of the next generation of ground-based imaging interferometers combining from 4 to possibly 6 telescopes simultaneously, there is also a strong need for a new generation of fringe trackers able to cophase such arrays. These instruments have to be very sensitive and to provide robust operations in quickly varying observational conditions.   Aims. We aim at defining the optimal characteristics of fringe sensor concepts operating with 4 or 6 telescopes. The current detector limitations impose us to consider solutions based on co-axial pairwise combination schemes.   Methods. We independently study several aspects of the fringe sensing process: 1) how to measure the phase and the group delay, and 2) how to combine the telescopes in order to ensure a precise and robust fringe tracking in real conditions. Thanks to analytical developments and numerical simulations, we define the optimal fringe-sensor concepts and compute the expected performance of the 4-telescope one with our dedicated end-to-end simulation tool sim2GFT.   Results. We first show that measuring the phase and the group delay by obtaining the data in several steps (i.e. by temporally modulating the optical path difference) is extremely sensitive to atmospheric turbulence and therefore conclude that it is better to obtain the fringe position with a set of data obtained simultaneously. Subsequently, we show that among all co-axial pairwise schemes, moderately redundant concepts increase the sensitivity as well as the robustness in various atmospheric or observing conditions. Merging all these results, end-to-end simulations show that our 4-telescope fringe sensor concept is able to track fringes at least 90% of the time up to limiting magnitudes of 7.5 and 9.5 for the 1.8- and 8.2-meter VLTI telescopes respectively."
"We introduce and study a class of optimization problems we coin replenishment problems with fixed turnover times: a very natural model that has received little attention in the literature. Nodes with capacity for storing a certain commodity are located at various places; at each node the commodity depletes within a certain time, the turnover time, which is constant but can vary between locations. Nodes should never run empty, and to prevent this we may schedule nodes for replenishment every day. The natural feature that makes this problem interesting is that we may schedule a replenishment (well) before a node becomes empty, but then the next replenishment will be due earlier also. This added workload needs to be balanced against the cost of routing vehicles to do the replenishments. In this paper, we focus on the aspect of minimizing routing costs. However, the framework of recurring tasks, in which the next job of a task must be done within a fixed amount of time after the previous one is much more general and gives an adequate model for many practical situations.   Note that our problem has an infinite time horizon. However, it can be fully characterized by a compact input, containing only the location of each store and a turnover time. This makes determining its computational complexity highly challenging and indeed it remains essentially unresolved. We study the problem for two objectives: min-avg minimizes the average tour length and min-max minimizes the maximum tour length over all days. For min-max we derive a logarithmic factor approximation for the problem on general metrics and a 6-approximation for the problem on trees, for which we have a proof of NP-hardness. For min-avg we present a logarithmic approximation on general metrics, 2-approximation for trees, and a pseudopolynomial time algorithm for the line. Many intriguing problems remain open."
"Axions/axion-like particles (ALPs) are a well motivated extension of the Standard Model and are generic within String Theory. The X-ray transparency of the intracluster medium (ICM) in galaxy clusters is a powerful probe of light ALPs (with mass $<10^{-11}\,{\rm eV}$); as X-ray photons from an embedded or background source propagate through the magnetized ICM, they may undergo energy-dependent quantum mechanical conversion into ALPs (and vice versa), imprinting distortions on the X-ray spectrum. We present Chandra data for the active galactic nucleus NGC1275 at the center of the Perseus cluster. Employing a 490ks High-Energy Transmission Gratings (HETG) exposure, we obtain a high-quality 1-9keV spectrum free from photon pileup and ICM contamination. Apart from iron-band features, the spectrum is described by a power-law continuum, with any spectral distortions at the $<3\%$ level. We compute photon survival probabilities as a function of ALP mass $m_a$ and ALP-photon coupling constant $g_{a\gamma}$ for an ensemble of ICM magnetic field models, and then use the NGC1275 spectrum to constraint the $(m_a, g_{a\gamma})$-plane. Marginalizing over magnetic field realizations, the 99.7% credible region limits the ALP-photon coupling to $g_{a\gamma}<6-8\times 10^{-13}\, {\rm GeV}^{-1}$ (depending upon magnetic field model) for masses $m_a<1\times 10^{-12}\,{\rm eV}$. These are the most stringent limit to date on $g_{a\gamma}$ for these light ALPs, and have already reached the sensitivity limits of next-generation helioscopes and light-shining-through-wall experiments. We highlight the potential of these studies with the next-generation X-ray observatories Athena and Lynx, but note the critical importance of advances in relative calibration of these future X-ray spectrometers."
"As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. We demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We showcase an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation."
"We present a method for creating video summaries in real-time on commodity hardware. Real-time here refers to the fact that the time required for video summarization is less than the duration of the input video. First, low-level features are use to discard undesirable frames. Next, video is divided into segments, and segment-level features are extracted for each segment. Tree-based models trained on widely available video summarization and computational aesthetics datasets are then used to rank individual segments, and top-ranked segments are selected to generate the final video summary. We evaluate the proposed method on SUMME dataset and show that our method is able to achieve summarization accuracy that is comparable to that of a current state-of-the-art deep learning method, while posting significantly faster run-times. Our method on average is able to generate a video summary in time that is shorter than the duration of the video."
"We compute the 3-point correlation function for a general model of inflation driven by a single, minimally coupled scalar field. Our approach is based on the numerical evaluation of both the perturbation equations and the integrals which contribute to the 3-point function. Consequently, we can analyze models where the potential has a ""feature"", in the vicinity of which the slow roll parameters may take on large, transient values. This introduces both scale and shape dependent non-Gaussianities into the primordial perturbations. As an example of our methodology, we examine the ``step'' potentials which have been invoked to improve the fit to the glitch in the $<TT>$ $C_l$ for $l \sim 30$, present in both the one and three year WMAP data sets. We show that for the typical parameter values, the non-Gaussianities associated with the step are far larger than those in standard slow roll inflation, and may even be within reach of a next generation CMB experiment such as Planck. More generally, we use this example to explain that while adding features to potential can improve the fit to the 2-point function, these are generically associated with a greatly enhanced signal at the 3-point level. Moreover, this 3-point signal will have a very nontrivial shape and scale dependence, which is correlated with the form of the 2-point function, and may thus lead to a consistency check on the models of inflation with non-smooth potentials."
"Quantum buoyancy has been proposed as the mechanism protecting the generalized second law when an entropy--bearing object is slowly lowered towards a black hole and then dropped in. We point out that the original derivation of the buoyant force from a fluid picture of the acceleration radiation is invalid unless the object is almost at the horizon, because otherwise typical wavelengths in the radiation are larger than the object. The buoyant force is here calculated from the diffractive scattering of waves off the object, and found to be weaker than in the original theory. As a consequence, the argument justifying the generalized second law from buoyancy cannot be completed unless the optimal drop point is next to the horizon. The universal bound on entropy is always a sufficient condition for operation of the generalized second law, and can be derived from that law when the optimal drop point is close to the horizon. We also compute the quantum buoyancy of an elementary charged particle; it turns out to be negligible for energetic considerations. Finally, we speculate on the significance of the absence from the bound of any mention of the number of particle species in nature."
"Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true.   This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. Our proposed LSTM-E consists of three components: a 2-D and/or 3-D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between visual content and sentence semantics. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques."
"The duality between color and kinematics present in scattering amplitudes of Yang-Mills theory strongly suggest the existence of a hidden kinematic Lie algebra that controls the gauge theory. While associated BCJ numerators are known on closed forms to any multiplicity at tree level, the kinematic algebra has only been partially explored for the simplest of four-dimensional amplitudes: up to the MHV sector. In this paper we introduce a framework that allows us to characterize the algebra beyond the MHV sector. This allows us to both constrain some of the ambiguities of the kinematic algebra, and better control the generalized gauge freedom that is associated with the BCJ numerators. Specifically, in this paper, we work in dimension-agnostic notation and determine the kinematic algebra valid up to certain ${\cal O}\big((\varepsilon_i \cdot \varepsilon_j)^2\big)$ terms that in four dimensions compute the next-to-MHV sector involving two scalars. The kinematic algebra in this sector is simple, given that we introduce tensor currents that generalize standard Yang-Mills vector currents. These tensor currents controls the generalized gauge freedom, allowing us to generate multiple different versions of BCJ numerators from the same kinematic algebra. The framework should generalize to other sectors in Yang-Mills theory."
"We present a supersymmetric generalization of the MHV vertex expansion for all tree amplitudes in N=4 SYM theory. In addition to the choice of a reference spinor, this super MHV vertex expansion also depends on four reference Grassmann parameters. We demonstrate that a significant fraction of diagrams in the expansion vanishes for a judicious choice of these Grassmann parameters, which simplifies the computation of amplitudes. Even pure-gluon amplitudes require fewer diagrams than in the ordinary MHV vertex expansion.   We show that the super MHV vertex expansion arises from the recursion relation associated with a holomorphic all-line supershift. This is a supersymmetric generalization of the holomorphic all-line shift recently introduced in arXiv:0811.3624. We study the large-z behavior of generating functions under these all-line supershifts, and find that they generically provide 1/z^k falloff at (Next-to)^k MHV level. In the case of anti-MHV generating functions, we find that a careful choice of shift parameters guarantees a stronger 1/z^(k+4) falloff. These particular all-line supershifts may therefore play an important role in extending the super MHV vertex expansion to N=8 supergravity."
"The paper contains an essentially self-contained treatment of Khovanov homology, Khovanov-Lee homology as well as the Rasmussen invariant for virtual knots and virtual knot cobordisms which directly applies to classical knot and classical knot cobordisms. To do so, we give an alternate formulation for the Manturov definition of Khovanov homology for virtual knots and links with arbitrary coefficients. This approach uses cut loci on the knot diagram to induce a conjugation operator in the Frobenius algebra. We then discuss the implications of the maps induced in the aforementioned theory to the universal Frobenius algebra for virtual knots. Next we show how one can apply the Karoubi envelope approach of Bar-Natan and Morrison on abstract link diagrams with cross cuts to construct the canonical generators of the Khovanov-Lee homology. Using these canonical generators we derive a generalization of the Rasmussen invariant for virtual knot cobordisms and furthermore generalize Rasmussen's result on the slice genus for positive knots to the case of positive virtual knots. It should also be noted that this generalization of the Rasmussen invariant provides an easy to compute obstruction to knot cobordisms in $S_g \times I \times I$ in the sense of Turaev."
"We propose a model of inflation capable of generating a population of light black holes (about $10^{-16}$ - $10^{-14}$ solar masses) that might account for a significant fraction of the dark matter in the Universe. The effective potential of the model features an approximate inflection point arising from two-loop order logarithmic corrections in well-motivated and perturbative particle physics examples. This feature decelerates the inflaton before the end of inflation, enhancing the primordial spectrum of scalar fluctuations and triggering efficient black hole production with a peaked mass distribution. At larger field values, inflation occurs thanks to a generic small coupling between the inflaton and the curvature of spacetime. We compute accurately the peak mass and abundance of the primordial black holes using the Press-Schechter and Mukhanov-Sasaki formalisms, showing that the slow-roll approximation fails to reproduce the correct results by orders of magnitude. We study as well a qualitatively similar implementation of the idea, where the approximate inflection point is due to competing terms in a generic polynomial potential. In both models, requiring a significant part of the dark matter abundance to be in the form of black holes implies a small blue scalar tilt with a sizable negative running and a tensor spectrum that may be detected by the next-generation probes of the cosmic microwave background. We also comment on previous works on the topic."
"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models."
"Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization."
"Recent advances in autoencoders and generative models have given rise to effective video forgery methods, used for generating so-called ""deepfakes"". Mitigation research is mostly focused on post-factum deepfake detection and not on prevention. We complement these efforts by introducing a novel class of adversarial attacks---training-resistant attacks---which can disrupt face-swapping autoencoders whether or not its adversarial images have been included in the training set of said autoencoders. We propose the Oscillating GAN (OGAN) attack, a novel attack optimized to be training-resistant, which introduces spatial-temporal distortions to the output of face-swapping autoencoders. To implement OGAN, we construct a bilevel optimization problem, where we train a generator and a face-swapping model instance against each other. Specifically, we pair each input image with a target distortion, and feed them into a generator that produces an adversarial image. This image will exhibit the distortion when a face-swapping autoencoder is applied to it. We solve the optimization problem by training the generator and the face-swapping model simultaneously using an iterative process of alternating optimization. Next, we analyze the previously published Distorting Attack and show it is training-resistant, though it is outperformed by our suggested OGAN. Finally, we validate both attacks using a popular implementation of FaceSwap, and show that they transfer across different target models and target faces, including faces the adversarial attacks were not trained on. More broadly, these results demonstrate the existence of training-resistant adversarial attacks, potentially applicable to a wide range of domains."
"Textual information in a captured scene plays an important role in scene interpretation and decision making. Though there exist methods that can successfully detect and interpret complex text regions present in a scene, to the best of our knowledge, there is no significant prior work that aims to modify the textual information in an image. The ability to edit text directly on images has several advantages including error correction, text restoration and image reusability. In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. We propose two different neural network architectures - (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consistency with neighboring characters. Our method works as a unified platform for modifying text in images. We present the effectiveness of our method on COCO-Text and ICDAR datasets both qualitatively and quantitatively."
"Generating natural language descriptions for videos, i.e., video captioning, essentially requires step-by-step reasoning along the generation process. For example, to generate the sentence ""a man is shooting a basketball"", we need to first locate and describe the subject ""man"", next reason out the man is ""shooting"", then describe the object ""basketball"" of shooting. However, existing visual reasoning methods designed for visual question answering are not appropriate to video captioning, for it requires more complex visual reasoning on videos over both space and time, and dynamic module composition along the generation process. In this paper, we propose a novel visual reasoning approach for video captioning, named Reasoning Module Networks (RMN), to equip the existing encoder-decoder framework with the above reasoning capacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal reasoning modules, and 2) a dynamic and discrete module selector trained by a linguistic loss with a Gumbel approximation. Extensive experiments on MSVD and MSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art methods while providing an explicit and explainable generation process. Our code is available at https://github.com/tgc1997/RMN."
"Evolution and development operate at different timescales; generations for the one, a lifetime for the other. These two processes, the basis of much of life on earth, interact in many non-trivial ways, but their temporal hierarchy---evolution overarching development---is observed for all multicellular lifeforms. When designing robots however, this tenet lifts: it becomes---however natural---a design choice. We propose to inverse this temporal hierarchy and design a developmental process happening at the phylogenetic timescale. Over a classic evolutionary search aimed at finding good gaits for a tentacle robot, we add a developmental process over the robots' morphologies. In each generation, the morphology of the robots does not change. But from one generation to the next, the morphology develops. Much like we become bigger, stronger and heavier as we age, our robots are bigger, stronger and heavier with each passing generation. Our robots start with baby morphologies, and a few thousand generations later, end-up with adult ones. We show that this produces better and qualitatively different gaits than an evolutionary search with only adult robots, and that it prevents premature convergence by fostering exploration. This method is conceptually simple, and can be effective on small or large populations of robots, and intrinsic to the robot and its morphology, and thus not specific to the task and the fitness function it is evaluated on. Furthermore, by recasting the evolutionary search as a learning process, these results can be viewed in the context of developmental learning robotics."
"Aerial vehicles are revolutionizing the way film-makers can capture shots of actors by composing novel aerial and dynamic viewpoints. However, despite great advancements in autonomous flight technology, generating expressive camera behaviors is still a challenge and requires non-technical users to edit a large number of unintuitive control parameters. In this work, we develop a data-driven framework that enables editing of these complex camera positioning parameters in a semantic space (e.g. calm, enjoyable, establishing). First, we generate a database of video clips with a diverse range of shots in a photo-realistic simulator, and use hundreds of participants in a crowd-sourcing framework to obtain scores for a set of semantic descriptors for each clip. Next, we analyze correlations between descriptors and build a semantic control space based on cinematography guidelines and human perception studies. Finally, we learn a generative model that can map a set of desired semantic video descriptors into low-level camera trajectory parameters. We evaluate our system by demonstrating that our model successfully generates shots that are rated by participants as having the expected degrees of expression for each descriptor. We also show that our models generalize to different scenes in both simulation and real-world experiments. Data and video found at: https://sites.google.com/view/robotcam."
"Non-linear electrodynamic models are re-assessed in this paper to pursue an investigation of the kinematics of the Compton effect in a magnetic background. Before considering specific models, we start off by presenting a general non-linear Lagrangian built up in terms of the most general Lorentz- and gauge-invariant combinations of the electric and magnetic fields. The extended Maxwell-like equations and the energy-momentum tensor conservation are presented and discussed in their generality. We next expand the fields around a uniform and time-independent electric and magnetic backgrounds up to second order in the propagating wave, and compute dispersion relations which account for the effect of the external fields. We obtain thereby the refraction index and the group velocity for the propagating radiation in different situations. In particular, we focus on the kinematics of the Compton effect in presence of external magnetic fields. This yields constraints that relate the derivatives of the general Lagrangian with respect to the field invariants and the magnetic background under consideration. We carry out our inspection by focusing on some specific non-linear electrodynamic effective models: Hoffmann-Infeld, Euler-Heisenberg, generalized Born-Infeld and Logarithmic."
"Cellular Internet of Things (IoT) is considered as de facto paradigm to improve the communication and computation systems. Cellular IoT connects massive number of physical and virtual objects to the Internet using cellular networks. The latest generation of cellular networks, e.g. fifth-generation (5G), use evolutionary and revolutionary technologies to notably improve the performance of wireless networks. However, given the envisioned new use-cases, e.g., holographic communication, and the ever-increasing deployment of massive smart-physical end-devices in IoT, the volume of network traffic has considerably raised, and therefore, the current generation of mobile networks cannot wholly meet the ever-increasing demands. Hence, it is envisioned that the next generation, sixth generation (6G) networks, need to play a critical role to alleviate such challenges in IoT by providing new communication services, network capacity, and ultra-low latency communications (uRLLC). In this paper, first, the need for 6G networks is discussed. Then, the potential 6G requirements and trends, as well as the latest research activities related to 6G are introduced e.g., Tactile Internet and Terahertz (THz). Furthermore, the key performance indicators, applications, new services, and the potential key enabling technologies for 6G networks are presented. Finally, several potential unresolved challenges for future 6G networks are presented."
"We generalize soft theorems of the nonlinear sigma model beyond the $\mathcal{O} (p^2)$ amplitudes and the coset of $\text{SU} (N) \times \text{SU} (N) / \text{SU} (N) $. We first discuss the flavor ordering of the amplitudes for the Nambu-Goldstone bosons of a general symmetry group representation, so that we can reinterpret the known $\mathcal{O} (p^2)$ single soft theorem for $\text{SU} (N) \times \text{SU} (N) / \text{SU} (N) $ in the context of a general group representation. We then investigate the special case of the fundamental representation of $\text{SO} (N)$, where a special flavor ordering of the ""pair basis"" is available. We provide novel amplitude relations and a Cachazo-He-Yuan formula for such a basis, and derive the corresponding single soft theorem. Next, we extend the single soft theorem for a general group representation to $\mathcal{O} (p^4)$, where for at least two specific choices of the $\mathcal{O} (p^4)$ operators, the leading non-vanishing pieces can be interpreted as new extended theory amplitudes involving bi-adjoint scalars, and the corresponding soft factors are the same as at $\mathcal{O} (p^2)$. Finally, we compute the general formula for the double soft theorem, valid to all derivative orders, where the leading part in the soft momenta is fixed by the $\mathcal{O}(p^2)$ Lagrangian, while any possible corrections to the subleading part are determined by the $\mathcal{O}(p^4)$ Lagrangian alone. Higher order terms in the derivative expansion do not contribute any new corrections to the double soft theorem."
"This paper presents a new approach for synthesizing a novel street-view panorama given an overhead satellite image. Taking a small satellite image patch as input, our method generates a Google's omnidirectional street-view type panorama, as if it is captured from the same geographical location as the center of the satellite patch. Existing works tackle this task as an image generation problem which adopts generative adversarial networks to implicitly learn the cross-view transformations, while ignoring the domain relevance. In this paper, we propose to explicitly establish the geometric correspondences between the two-view images so as to facilitate the cross-view transformation learning. Specifically, we observe that when a 3D point in the real world is visible in both views, there is a deterministic mapping between the projected points in the two-view images given the height information of this 3D point. Motivated by this, we develop a novel Satellite to Street-view image Projection (S2SP) module which explicitly establishes such geometric correspondences and projects the satellite images to the street viewpoint. With these projected satellite images as network input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates images that better respect the scene geometry than existing approaches."
"Generation of scientific visualization from analytical natural language text is a challenging task. In this paper, we propose Text2Chart, a multi-staged chart generator method. Text2Chart takes natural language text as input and produce visualization as two-dimensional charts. Text2Chart approaches the problem in three stages. Firstly, it identifies the axis elements of a chart from the given text known as x and y entities. Then it finds a mapping of x-entities with its corresponding y-entities. Next, it generates a chart type suitable for the given text: bar, line or pie. Combination of these three stages is capable of generating visualization from the given analytical text. We have also constructed a dataset for this problem. Experiments show that Text2Chart achieves best performances with BERT based encodings with LSTM models in the first stage to label x and y entities, Random Forest classifier for the mapping stage and fastText embedding with LSTM for the chart type prediction. In our experiments, all the stages show satisfactory results and effectiveness considering formation of charts from analytical text, achieving a commendable overall performance."
"Molecular dynamics simulations play an increasingly important role in the rational design of (nano)-materials and in the study of biomacromolecules. However, generating input files and realistic starting coordinates for these simulations is a major bottleneck, especially for high throughput protocols and for complex multi-component systems. To eliminate this bottleneck, we present the polyply software suite that leverages 1) a multi-scale graph matching algorithm designed to generate parameters quickly and for arbitrarily complex polymeric topologies, and 2) a generic multi-scale random walk protocol capable of setting up complex systems efficiently and independent of the target force-field or model resolution. We benchmark quality and performance of the approach by creating melt simulations of six different polymers using two force-fields with different resolution. We further demonstrate the power of our approach by setting up a multi lamellar microphase-separated block copolymer system for next generation batteries, and by generating a liquid-liquid phase separated polyethylene oxide-dextran system inside a lipid vesicle, featuring both branching and molecular weight distribution of the dextran component."
"We present a new calculation of the CP violation parameter $\epsilon^{\prime}/ \epsilon$. The results reported in this paper have been obtained by using the $\Delta S=1$ effective Hamiltonian computed at the next-to-leading order, including QCD and QED penguins. The matrix elements of the relevant operators have been taken from lattice QCD, at a scale $\mu=2$ GeV. At this relatively large scale, the perturbative matching between the relevant operators and the corresponding coefficients is quite reliable.   The effect of the next-to-leading corrections is to lower the prediction obtained at the leading order, thus favouring the experimental result of E731. We analyze different contributions to the final result and compare the leading and next-to-leading cases."
"Future computing systems, from handhelds to supercomputers, will undoubtedly be more parallel and heterogeneous than todays systems to provide more performance and energy efficiency. Thus, GPUs are increasingly being used to accelerate general purpose applications, including applications with data dependent, irregular control flow and memory access patterns. However, the growing complexity, exposed memory hierarchy, incoherence, heterogeneity, and parallelism will make accelerator based systems progressively more difficult to program. In the foreseeable future, the vast majority of programmers will no longer be able to extract additional performance or energy savings from next generation systems be-cause the programming will be too difficult. Automatic performance analysis and optimization recommendation tools have the potential to avert this situation. They embody expert knowledge and make it available to software developers when needed. In this paper, we describe and evaluate such a tool."
"We compute the QCD corrections to the decay rate difference in the $B_s-\bar B_s$ system, $\Delta\Gamma_{B_s}$, in the next-to-leading logarithmic approximation using the heavy quark expansion approach. Going beyond leading order in QCD is essential to obtain a proper matching of the Wilson coefficients to the matrix elements of local operators from lattice gauge theory. The lifetime difference is reduced considerably at next-to-leading order. We find $(\Delta\Gamma/\Gamma)_{B_s}=(f_{B_s}/210 MeV)^2 [0.006 B(m_b)+0.150 B_S(m_b)-0.063]$ in terms of the bag parameters $B, B_S$ in the NDR scheme. As a further application of our analysis we also derive the next-to-leading order result for the mixing-induced CP asymmetry in inclusive $b\to u\bar ud$ decays, which measures $\sin 2\alpha$."
"The top quark cross section close to threshold in $e^+e^-$ annihilation is computed including the summation of logarithms of the velocity at next-to-next-to-leading-logarithmic order in QCD. The remaining theoretical uncertainty in the normalization of the total cross section is at the few percent level, an order of magnitude smaller than in previous next-to-next-to-leading order calculations. This uncertainty is smaller than the effects of a light standard model Higgs boson."
"We discuss the production of pion pairs with a large invariant mass in hadronic collisions. We present a study based on a perturbative QCD calculation at full next-to-leading order accuracy, implemented in the computer programme DIPHOX. We provide a comparison of various predictions with the corresponding observables measured by the E706 fixed target experiment. Discrepancies between data and next-to-leading order calculation are carefully analysed. We classify the theoretical next-to-leading order distributions with respect to their infra-red sensitivity, and explain which distributions need improvements. Further, we comment on the energy scale dependences of non pertubative effects."
"Access to vast amounts of data along with affordable computational power stimulated the reincarnation of neural networks. The progress could not be achieved without adequate software tools, lowering the entry bar for the next generations of researchers and developers. The paper introduces PyTorchPipe (PTP), a framework built on top of PyTorch. Answering the recent needs and trends in machine learning, PTP facilitates building and training of complex, multi-modal models combining language and vision (but is not limited to those two modalities). At its core, PTP employs a component-oriented approach and relies on the concept of a pipeline, defined as a directed acyclic graph of loosely coupled components. A user defines a pipeline using yaml-based (thus human-readable) configuration files, whereas PTP provides generic workers for their loading, training, and testing using all the computational power (CPUs and GPUs) that is available to the user. The paper covers the main concepts of PyTorchPipe, discusses its key features and briefly presents the currently implemented tasks, models and components."
"The Monte Carlo generator PPROPHECY4F provides a PROPer description of the Higgs dECaY into 4 Fermions within the Standard Model, the Standard Model with a fourth fermion generation, a simple Higgs-singlet extension of the Standard Model, and the Two-Higgs-Doublet Model. The fully differential predictions include the full QCD and electroweak next-to-leading-order corrections, all interference contributions between different WW/ZZ channels, and all off-shell effects of intermediate W/Z bosons. PPROPHECY4F computes the inclusive partial decay widths and allows for the computation of binned differential distributions of the decay products. For leptonic final states also unweighted events are provided."
"This paper presents a framework which uses computer vision algorithms to standardise images and analyse them for identifying crop diseases automatically. The tools are created to bridge the information gap between farmers, advisory call centres and agricultural experts using the images of diseased/infected crop captured by mobile-phones. These images are generally sensitive to a number of factors including camera type and lighting. We therefore propose a technique for standardising the colour of plant images within the context of the advisory system. Subsequently, to aid the advisory process, the disease recognition process is automated using image processing in conjunction with machine learning techniques. We describe our proposed leaf extraction, affected area segmentation and disease classification techniques. The proposed disease recognition system is tested using six mango diseases and the results show over 80% accuracy. The final output of our system is a list of possible diseases with relevant management advice."
"Using optimal phasor measurement unit placement as a prototypical problem, we assess the computational viability of the current generation D-Wave Systems 2000Q quantum annealer for power systems design problems. We reformulate minimum dominating set for the annealer hardware, solve the reformulation for a standard set of IEEE test systems, and benchmark solution quality and time to solution against the CPLEX Optimizer and simulated annealing. For some problem instances the 2000Q outpaces CPLEX. For instances where the 2000Q underperforms with respect to CPLEX and simulated annealing, we suggest hardware improvements for the next generation of quantum annealers."
"The negative multinomial distribution is a multivariate generalization of the negative binomial distribution. In this paper, we consider the problem of estimating an unknown matrix of probabilities on the basis of observations of negative multinomial variables under the standardized squared error loss. First, a general sufficient condition for a shrinkage estimator to dominate the UMVU estimator is derived and an empirical Bayes estimator satisfying the condition is constructed. Next, a hierarchical shrinkage prior is introduced, an associated Bayes estimator is shown to dominate the UMVU estimator under some conditions, and some remarks about posterior computation are presented. Finally, shrinkage estimators and the UMVU estimator are compared by simulation."
"As widely known, the basic reproduction number plays a key role in weighing birth/infection and death/recovery processes in several models of population dynamics. In this general setting, its characterization as the spectral radius of next generation operators is rather elegant, but simultaneously poses serious obstacles to its practical determination. In this work we address the problem numerically by reducing the relevant operators to matrices through a pseudospectral collocation, eventually computing the sought quantity by solving finite-dimensional eigenvalue problems. The approach is illustrated for two classes of models, respectively from ecology and epidemiology. Several numerical tests demonstrate experimentally important features of the method, like fast convergence and influence of the smoothness of the models' coefficients. Examples of robust analysis of instances of specific models are also presented to show potentialities and ease of application."
