text
Computing with CodeRunner at Coventry University: Automated summative assessment of Python and C++ code CodeRunner is a free open-source Moodle plugin for automatically marking student code. We describe our experience using CodeRunner for summative assessment in our first year undergraduate programming curriculum at Coventry University. We use it to assess both Python3 and C++14 code (CodeRunner supports other languages also). We give examples of our questions and report on how key metrics have changed following its use at Coventry.
"Chain Rules for Hessian and Higher Derivatives Made Easy by Tensor Calculus Computing multivariate derivatives of matrix-like expressions in the compact, coordinate free fashion is very important for both theory and applied computations (e.g. optimization and machine learning).   The critical components of such computations are \emph{chain and product rules} for derivatives. Although they are taught early in simple scenarios, practical applications involve high-dimensional arrays; in this context it is very hard to find easy accessible and compact explanation.   This paper discusses how to relatively simply carry such derivations based on the (simplified as adapted in applied computer science) concept of tensors. Numerical examples in modern Python libraries are provided. This discussion simplifies and illustrates an earlier exposition by Manton (2012)."
"Convolutional Mean: A Simple Convolutional Neural Network for Illuminant Estimation We present Convolutional Mean (CM) - a simple and fast convolutional neural network for illuminant estimation. Our proposed method only requires a small neural network model (1.1K parameters) and a 48 x 32 thumbnail input image. Our unoptimized Python implementation takes 1 ms/image, which is arguably 3-3750x faster than the current leading solutions with similar accuracy. Using two public datasets, we show that our proposed light-weight method offers accuracy comparable to the current leading methods' (which consist of thousands/millions of parameters) across several measures."
"Optimal binning: mathematical programming formulation The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target. We present a rigorous and extensible mathematical programming formulation to solving the optimal binning problem for a binary, continuous and multi-class target type, incorporating constraints not previously addressed. For all three target types, we introduce a convex mixed-integer programming formulation. Several algorithmic enhancements such as automatic determination of the most suitable monotonic trend via a Machine-Learning-based classifier and implementation aspects are thoughtfully discussed. The new mathematical programming formulations are carefully implemented in the open-source python library OptBinning."
"A tutorial on the range variant of asymmetric numeral systems This paper is intended to be a brief and accessible introduction to the range variant of asymmetric numeral systems (ANS), a system for lossless compression of sequences which can be used as a drop in replacement for arithmetic coding (AC). Because of the relative simplicity of ANS, we are able to provide enough mathematical detail to rigorously prove that ANS attains a compression rate close to the Shannon limit. Pseudo-code, intuitive interpretation and diagrams are given alongside the mathematical derivations. A working Python demo which accompanies this tutorial is available at https://raw.githubusercontent.com/j-towns/ans-notes/master/rans.py."
"LIBTwinSVM: A Library for Twin Support Vector Machines This paper presents LIBTwinSVM, a free, efficient, and open source library for Twin Support Vector Machines (TSVMs). Our library provides a set of useful functionalities such as fast TSVMs estimators, model selection, visualization, a graphical user interface (GUI) application, and a Python application programming interface (API). The benchmarks results indicate the effectiveness of the LIBTwinSVM library for large-scale classification problems. The source code of LIBTwinSVM library, installation guide, documentation, and usage examples are available at https://github.com/mir-am/LIBTwinSVM."
"Conditioning Autoencoder Latent Spaces for Real-Time Timbre Interpolation and Synthesis We compare standard autoencoder topologies' performances for timbre generation. We demonstrate how different activation functions used in the autoencoder's bottleneck distributes a training corpus's embedding. We show that the choice of sigmoid activation in the bottleneck produces a more bounded and uniformly distributed embedding than a leaky rectified linear unit activation. We propose a one-hot encoded chroma feature vector for use in both input augmentation and latent space conditioning. We measure the performance of these networks, and characterize the latent embeddings that arise from the use of this chroma conditioning vector. An open source, real-time timbre synthesis algorithm in Python is outlined and shared."
"On the distance between two neural networks and the stability of learning This paper relates parameter distance to gradient breakdown for a broad class of nonlinear compositional functions. The analysis leads to a new distance function called deep relative trust and a descent lemma for neural networks. Since the resulting learning rule seems to require little to no learning rate tuning, it may unlock a simpler workflow for training deeper and more complex neural networks. The Python code used in this paper is here: https://github.com/jxbz/fromage."
"Visualizing modular forms We examine several currently used techniques for visualizing complex-valued functions applied to modular forms. We plot several examples and study the benefits and limitations of each technique. We then introduce a method of visualization that can take advantage of colormaps in Python's matplotlib library, describe an implementation, and give more examples. Much of this discussion applies to general visualizations of complex-valued functions in the plane."
"A Purely-Reactive Manipulability-Maximising Motion Controller We present a novel approach to controlling the instantaneous velocity of a robot end-effector that is able to simultaneously maximise manipulability and avoid joint limits. It operates on non-redundant and redundant robots, which is achieved by adding artificial redundancy in the form of controlled path deviation. We formulate the problem as a quadratic programme and provide an open-source Python implementation that provides solutions in just a few milliseconds. It accepts a robot model expressed using URDF or Denavit-Hartenberg parameterisation. We compare our method to previous work in simulation and on a physical robot."
"Autoencoding Neural Networks as Musical Audio Synthesizers A method for musical audio synthesis using autoencoding neural networks is proposed. The autoencoder is trained to compress and reconstruct magnitude short-time Fourier transform frames. The autoencoder produces a spectrogram by activating its smallest hidden layer, and a phase response is calculated using real-time phase gradient heap integration. Taking an inverse short-time Fourier transform produces the audio signal. Our algorithm is light-weight when compared to current state-of-the-art audio-producing machine learning algorithms. We outline our design process, produce metrics, and detail an open-source Python implementation of our model."
"wisardpkg -- A library for WiSARD-based models In order to facilitate the production of codes using WiSARD-based models, LabZero developed an ML library C++/Python called wisardpkg. This library is an MIT-licensed open-source package hosted on GitHub under the license."
"DisCoPy: Monoidal Categories in Python We introduce DisCoPy, an open source toolbox for computing with monoidal categories. The library provides an intuitive syntax for defining string diagrams and monoidal functors. Its modularity allows the efficient implementation of computational experiments in the various applications of category theory where diagrams have become a lingua franca. As an example, we used DisCoPy to perform natural language processing on quantum hardware for the first time."
"On the use of Data-Driven Cost Function Identification in Parametrized NMPC In this paper, a framework with complete numerical investigation is proposed regarding the feasibility of constrained Nonlinear Model Predictive Control (NMPC) design using Data-Driven model of the cost function. Although the idea is very much in the air, this paper proposes a complete implementation using python modules that are made freely available on a GitHub repository. Moreover, a discussion regarding the different ways of deriving control via data-driven modeling is proposed that can be of interest to practitioners."
"Reinforcement Learning for Thermostatically Controlled Loads Control using Modelica and Python The aim of the project is to investigate and assess opportunities for applying reinforcement learning (RL) for power system control. As a proof of concept (PoC), voltage control of thermostatically controlled loads (TCLs) for power consumption regulation was developed using Modelica-based pipeline. The Q-learning RL algorithm has been validated for deterministic and stochastic initialization of TCLs. The latter modelling is closer to real grid behaviour, which challenges the control development, considering the stochastic nature of load switching. In addition, the paper shows the influence of Q-learning parameters, including discretization of state-action space, on the controller performance."
"ProFIPy: Programmable Software Fault Injection as-a-Service In this paper, we present a new fault injection tool (ProFIPy) for Python software. The tool is designed to be programmable, in order to enable users to specify their software fault model, using a domain-specific language (DSL) for fault injection. Moreover, to achieve better usability, ProFIPy is provided as software-as-a-service and supports the user through the configuration of the faultload and workload, failure data analysis, and full automation of the experiments using container-based virtualization and parallelization."
"kiwiPy: Robust, high-volume, messaging for big-data and computational science workflows In this work we present kiwiPy, a Python library designed to support robust message based communication for high-throughput, big-data, applications while being general enough to be useful wherever high-volumes of messages need to be communicated in a predictable manner. KiwiPy relies on the RabbitMQ protocol, an industry standard message broker, while providing a simple and intuitive interface that can be used in both multithreaded and coroutine based applications. To demonstrate some of kiwiPy's functionality we give examples from AiiDA, a high-throughput simulation platform, where kiwiPy is used as a key component of the workflow engine."
"Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates In a recent application of the Bokeh Python library for visualizing physico-chemical properties of chemical entities text-mined from the scientific literature, we found ourselves facing the task of smoothing hexagonally binned data in Cartesian coordinates. To the best of our knowledge, no documentation for how to do this exist in the public domain. This short paper shows how to accomplish this in general and for Bokeh in particular. We illustrate the method with a real-world example and discuss some potential advantages of using hexagonal bins in these and similar applications."
"SudoQ -- a quantum variant of the popular game We introduce SudoQ, a quantum version of the classical game Sudoku. Allowing the entries of the grid to be (non-commutative) projections instead of integers, the solution set of SudoQ puzzles can be much larger than in the classical (commutative) setting. We introduce and analyze a randomized algorithm for computing solutions of SudoQ puzzles. Finally, we state two important conjectures relating the quantum and the classical solutions of SudoQ puzzles, corroborated by analytical and numerical evidence."
"EinsteinPy: A Community Python Package for General Relativity This paper presents EinsteinPy (version 0.3), a community-developed Python package for gravitational and relativistic astrophysics. Python is a free, easy to use a high-level programming language which has seen a huge expansion in the number of its users and developers in recent years. Specifically, a lot of recent studies show that the use of Python in Astrophysics and general physics has increased exponentially. We aim to provide a very high level of abstraction, an easy to use interface and pleasing user experience. EinsteinPy is developed keeping in mind the state of a theoretical gravitational physicist with little or no background in computer programming and trying to work in the field of numerical relativity or trying to use simulations in their research. Currently, EinsteinPy supports simulation of time-like and null geodesics and calculates trajectories in different background geometries some of which are Schwarzschild, Kerr, and KerrNewmann along with coordinate inter-conversion pipeline. It has a partially developed pipeline for plotting and visualization with dependencies on libraries like Plotly, matplotlib, etc. One of the unique features of EinsteinPy is a sufficiently developed symbolic tensor manipulation utilities which are a great tool in itself for teaching yourself tensor algebra which for many beginner students can be overwhelmingly tricky. EinsteinPy also provides few utility functions for hypersurface embedding of Schwarzschild spacetime which further will be extended to model gravitational lensing simulation."
"DQM Tools and Techniques of the SND Detector SND detector operates at the VEPP-2000 collider (BINP, Novosibirsk). To improve events selection for physical analysis and facilitate online detector control we developed new data quality monitoring (DQM) system. The system includes online and reprocess control modules, automatic decision making scripts, interactive (web based) and program (python) access to various quality estimates. This access is implemented with node.js server with data in RDBMS MySQL. We describe here general system logics, its components and some implementation details."
"PlenoptiSign: an optical design tool for plenoptic imaging Plenoptic imaging enables a light-field to be captured by a single monocular objective lens and an array of micro lenses attached to an image sensor. Metric distances of the light-field's depth planes remain unapparent prior to acquisition. Recent research showed that sampled depth locations rely on the parameters of the system's optical components. This paper presents PlenoptiSign, which implements these findings as a Python software package to help assist in an experimental or prototyping stage of a plenoptic system."
"Transport of dust grain particles in the accretion disk Entrainment of dust particles in the flow inside and outside of the proto-planetary disk has implications for the disk evolution and composition of planets. Using quasi-stationary solutions in our star-disk simulations as a background, we add dust particles of different radii in post-processing of the results, using our Python tool DUSTER. The distribution and motion of particles in the disk is followed in the cases with and without the backflow in the disk. We also compare the results with and without the radiation pressure included in the computation."
"Regression Enrichment Surfaces: a Simple Analysis Technique for Virtual Drug Screening Models We present a new method for understanding the performance of a model in virtual drug screening tasks. While most virtual screening problems present as a mix between ranking and classification, the models are typically trained as regression models presenting a problem requiring either a choice of a cutoff or ranking measure. Our method, regression enrichment surfaces (RES), is based on the goal of virtual screening: to detect as many of the top-performing treatments as possible. We outline history of virtual screening performance measures and the idea behind RES. We offer a python package and details on how to implement and interpret the results."
"FastONN -- Python based open-source GPU implementation for Operational Neural Networks Operational Neural Networks (ONNs) have recently been proposed as a special class of artificial neural networks for grid structured data. They enable heterogenous non-linear operations to generalize the widely adopted convolution-based neuron model. This work introduces a fast GPU-enabled library for training operational neural networks, FastONN, which is based on a novel vectorized formulation of the operational neurons. Leveraging on automatic reverse-mode differentiation for backpropagation, FastONN enables increased flexibility with the incorporation of new operator sets and customized gradient flows. Additionally, bundled auxiliary modules offer interfaces for performance tracking and checkpointing across different data partitions and customized metrics."
"Adaptive Gradient Coding This paper focuses on mitigating the impact of stragglers in distributed learning system. Unlike the existing results designed for a fixed number of stragglers, we developed a new scheme called Adaptive Gradient Coding(AGC) with flexible tolerance of various number of stragglers. Our scheme gives an optimal tradeoff between computation load, straggler tolerance and communication cost. In particular, it allows to minimize the communication cost according to the real-time number of stragglers in the practical environments. Implementations on Amazon EC2 clusters using Python with mpi4py package verify the flexibility in several situations."
Imperfections and corrections The measurement and correction of optics parameters has been a major concern since the advent of strong focusing synchrotron accelerators. A review of typical imperfections in accelerator optics together with measurement and correction algorithms is given with emphasis on numerical implementations. Python examples are shown using existing libraries when possible.
"PyCrystalField: Software for Calculation, Analysis, and Fitting of Crystal Electric Field Hamiltonians We introduce PyCrystalField, a Python software package for calculating single-ion crystal electric field (CEF) Hamiltonians. This software can calculate a CEF Hamiltonian \textit{ab initio} from a point charge model for any transition or rare earth ion in either the $J$ basis or the $LS$ basis, perform symmetry analysis to identify nonzero CEF parameters, calculate the energy spectrum and observables such as neutron spectrum and magnetization, and fit CEF Hamiltonians to any experimental data. The theory, implementation, and examples of its use are discussed."
"Lale: Consistent Automated Machine Learning Automated machine learning makes it easier for data scientists to develop pipelines by searching over possible choices for hyperparameters, algorithms, and even pipeline topologies. Unfortunately, the syntax for automated machine learning tools is inconsistent with manual machine learning, with each other, and with error checks. Furthermore, few tools support advanced features such as topology search or higher-order operators. This paper introduces Lale, a library of high-level Python interfaces that simplifies and unifies automated machine learning in a consistent way."
Approximate Bayesian Computations to fit and compare insurance loss models Approximate Bayesian Computation (ABC) is a statistical learning technique to calibrate and select models by comparing observed data to simulated data. This technique bypasses the use of the likelihood and requires only the ability to generate synthetic data from the models of interest. We apply ABC to fit and compare insurance loss models using aggregated data. A state-of-the-art ABC implementation in Python is proposed. It uses sequential Monte Carlo to sample from the posterior distribution and the Wasserstein distance to compare the observed and synthetic data.
"ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians The computation of first and second-order derivatives is a staple in many computing applications, ranging from machine learning to scientific computing. We propose an algorithm to automatically differentiate algorithms written in a subset of C99 code and its efficient implementation as a Python script. We demonstrate that our algorithm enables automatic, reliable, and efficient differentiation of common algorithms used in physical simulation and geometry processing."
"ULYSSES: Universal LeptogeneSiS Equation Solver ULYSSES is a python package that calculates the baryon asymmetry produced from leptogenesis in the context of a type-I seesaw mechanism. The code solves the semi-classical Boltzmann equations for points in the model parameter space as specified by the user. We provide a selection of predefined Boltzmann equations as well as a plugin mechanism for externally provided models of leptogenesis. Furthermore, the ULYSSES code provides tools for multi-dimensional parameter space exploration. The emphasis of the code is on user flexibility and rapid evaluation. It is publicly available at https://github.com/earlyuniverse/ulysses"
"Optimal tool path planning for 3D printing with spatio-temporal and thermal constraints In this paper, we address the problem of synthesizing optimal path plans in a 2D subject to spatio-temporal and thermal constraints. Our solution consists of reducing the path planning problem to a Mixed Integer Linear Programming (MILP) problem. The challenge is in encoding the implication constraints in the path planning problem using only conjunctions that are permitted by the MILP formulation. Our experimental analysis using an implementation of the encoding in a Python toolbox demonstrates the feasibility of our approach in generating the optimal plans."
"Language Modelling for Source Code with Transformer-XL It has been found that software, like natural language texts, exhibits ""naturalness"", which can be captured by statistical language models. In recent years, neural language models have been proposed to represent the naturalness of software through deep learning. In this paper, we conduct an experimental evaluation of state-of-the-art neural language models for source code, including RNN-based models and Transformer-XL based models. Through experiments on a large-scale Python code corpus, we find that the Transformer-XL model outperforms RNN-based models (including LSTM and GRU models) in capturing the naturalness of software, with far less computational cost."
"SafePILCO: a software tool for safe and data-efficient policy synthesis SafePILCO is a software tool for safe and data-efficient policy search with reinforcement learning. It extends the known PILCO algorithm, originally written in MATLAB, to support safe learning. We provide a Python implementation and leverage existing libraries that allow the codebase to remain short and modular, which is appropriate for wider use by the verification, reinforcement learning, and control communities."
"EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy EagerPy is a Python framework that lets you write code that automatically works natively with PyTorch, TensorFlow, JAX, and NumPy. Library developers no longer need to choose between supporting just one of these frameworks or reimplementing the library for each framework and dealing with code duplication. Users of such libraries can more easily switch frameworks without being locked in by a specific 3rd party library. Beyond multi-framework support, EagerPy also brings comprehensive type annotations and consistent support for method chaining to any framework. The latest documentation is available online at https://eagerpy.jonasrauber.de and the code can be found on GitHub at https://github.com/jonasrauber/eagerpy."
"KS-pies: Kohn-Sham Inversion Toolkit A Kohn-Sham (KS) inversion determines a KS potential and orbitals corresponding to a given electron density, a procedure that has applications in developing and evaluating functionals used in density functional theory. Despite the utility of KS inversions, application of these methods among the research community is disproportionately small. We implement the KS inversion methods of Zhao-Morrison-Parr and Wu-Yang in a framework that simplifies analysis and conversion of the resulting potential in real-space. Fully documented Python scripts integrate with PySCF, a popular electronic structure prediction software, and Fortran alternatives are provided for computational hot spots."
"SuperSuit: Simple Microwrappers for Reinforcement Learning Environments In reinforcement learning, wrappers are universally used to transform the information that passes between a model and an environment. Despite their ubiquity, no library exists with reasonable implementations of all popular preprocessing methods. This leads to unnecessary bugs, code inefficiencies, and wasted developer time. Accordingly we introduce SuperSuit, a Python library that includes all popular wrappers, and wrappers that can easily apply lambda functions to the observations/actions/reward. It's compatible with the standard Gym environment specification, as well as the PettingZoo specification for multi-agent environments. The library is available at https://github.com/PettingZoo-Team/SuperSuit,and can be installed via pip."
"Developing and Defeating Adversarial Examples Breakthroughs in machine learning have resulted in state-of-the-art deep neural networks (DNNs) performing classification tasks in safety-critical applications. Recent research has demonstrated that DNNs can be attacked through adversarial examples, which are small perturbations to input data that cause the DNN to misclassify objects. The proliferation of DNNs raises important safety concerns about designing systems that are robust to adversarial examples. In this work we develop adversarial examples to attack the Yolo V3 object detector [1] and then study strategies to detect and neutralize these examples. Python code for this project is available at https://github.com/ianmcdiarmidsterling/adversarial"
"Extended Self-similar Solution for Circumstellar Material-Supernova Ejecta Interaction In this note, we present a detailed self-similar solution to the interaction of a uniformly expanding gas and a stationary ambient medium, with an application to supernovae interacting with preexisting circumstellar media (Type IIn SNe). We implement the generalized solution into the Modular Open Source Fitter for Transients (MOSFiT), an open-source Python package for fitting extragalactic transient light curves."
"Classifying minimal vanishing sums of roots of unity A vanishing sum of roots of unity is called minimal if no proper, nonempty sub-sum of it vanishes. This paper classifies all minimal vanishing sums of roots of unity of weight at most 16 by hand and then develops and codes an algorithm to extend the classification further, up to weight no greater than 21."
"CMEchaser, detecting line-of-sight occultations due to Coronal Mass Ejections We present a python-based tool to detect the occultation of background sources by foreground Solar coronal mass ejections. The tool takes as input standard celestial coordinates of the source and translates those to the Helioprojective plane, and is thus well suited for use with a wide variety of background astronomical sources. This tool provides an easy means to search through a large archival dataset for such crossings and relies on the well-tested Astropy and Sunpy modules."
Microsoft Recommenders: Tools to Accelerate Developing Recommender Systems The purpose of this work is to highlight the content of the Microsoft Recommenders repository and show how it can be used to reduce the time involved in developing recommender systems. The open source repository provides python utilities to simplify common recommender-related data science work as well as example Jupyter notebooks that demonstrate use of the algorithms and tools under various environments.
"Simphony: An open-source photonic integrated circuit simulation framework We present Simphony, a free and open-source software toolbox for abstracting and simulating photonic integrated circuits, implemented in Python. The toolbox is both fast and easily extensible; plugins can be written to provide compatibility with existing layout tools, and device libraries can be easily created without a deep knowledge of programming. We include several examples of photonic circuit simulations with novel features and demonstrate a speedup of more than 20x over a leading commercially available software tool."
"Multiplayer Support for the Arcade Learning Environment The Arcade Learning Environment (""ALE"") is a widely used library in the reinforcement learning community that allows easy programmatic interfacing with Atari 2600 games, via the Stella emulator. We introduce a publicly available extension to the ALE that extends its support to multiplayer games and game modes. This interface is additionally integrated with PettingZoo to allow for a simple Gym-like interface in Python to interact with these games. We additionally introduce experimental baselines for all environments included."
"Simulation model of spacetime with the Minkowski metric In this paper, we propose a simulation model of spacetime as a discrete model of physical space. The model is based on the ideas of Stephen Wolfram and uses non-numerical modelling. The simulation model is described as an ontology. We use object-oriented simulation (OOS), but the model is also suitable for agent-based simulation (ABS). We use UML2 SP (UML Scientific Profile), an object-oriented simulation language used in scientific fields. This paper describes several experiments that demonstrate time dilation and dynamic relativistic effects. The reproducibility of experimental results can be verified. We provide a link to the repository in this paper. The model is implemented in Python."
"Gaussian Process Molecule Property Prediction with FlowMO We present FlowMO: an open-source Python library for molecular property prediction with Gaussian Processes. Built upon GPflow and RDKit, FlowMO enables the user to make predictions with well-calibrated uncertainty estimates, an output central to active learning and molecular design applications. Gaussian Processes are particularly attractive for modelling small molecular datasets, a characteristic of many real-world virtual screening campaigns where high-quality experimental data is scarce. Computational experiments across three small datasets demonstrate comparable predictive performance to deep learning methods but with superior uncertainty calibration."
"Quantum Information for Particle Theorists Lectures given at the Theoretical Advanced Study Institute (TASI 2020), 1-26 June 2020. The topics covered include quantum circuits, entanglement, quantum teleportation, Bell inequalities, quantum entropy and decoherence, classical versus quantum measurement, the area law for entanglement entropy in quantum field theory, and simulating quantum field theory on a quantum computer. Along the way we confront the fundamental sloppiness of how we all learned (and some of us taught) quantum mechanics in college. Links to a Python notebook and Mathematica notebooks will allow the reader to reproduce and extend the calculations, as well as perform five experiments on a quantum simulator."
"Thermodynamic Properties of Ice: A Monte Carlo Study In this text, we implement a monte carlo algorithm to study thermodynamic properties of ice. Our program, written in Python, is open-sourced and available at https://github.com/AKnightWing/ColdAsIce. We develop a novel scheme to compute the residual entropy of a two dimensional lattice model of ice at absolute zero. A model of energetic ice is also considered and its thermodynamic properties are studied with temperature. We report a remarkable result for the energetic ice model, the presence of a phase transition at non zero temperatures."
Automated Prediction of Medieval Arabic Diacritics This study uses a character level neural machine translation approach trained on a long short-term memory-based bi-directional recurrent neural network architecture for diacritization of Medieval Arabic. The results improve from the online tool used as a baseline. A diacritization model have been published openly through an easy to use Python package available on PyPi and Zenodo. We have found that context size should be considered when optimizing a feasible prediction model.
A Modular Framework for Distributed Model Predictive Control of Nonlinear Continuous-Time Systems (GRAMPC-D) The modular open-source framework GRAMPC-D for model predictive control of distributed systems is presented in this paper. The modular concept allows to solve optimal control problems (OCP) in a centralized and distributed fashion using the same problem description. It is tailored to computational efficiency with the focus on embedded hardware. The distributed solution is based on the Alternating Direction Method of Multipliers (ADMM) and uses the concept of neighbor approximation to enhance convergence speed. The presented framework can be accessed through Cpp and Python and also supports plug-and-play and data exchange between agents over a network.
"Diffusive solver: a diffusion-equations solver based on FEniCS Many steady-state transport problems in condensed matter physics can be reduced to a set of coupled diffusion equations. This is true in particular when relaxation processes are sufficiently fast that the system is in the diffusive --opposite of ballistic-- regime. Here we describe a python package, based on FEniCS, that solves this type of problems with an arbitrary number degrees of freedom that can represent charge, spin, energy, band or valley flavours. Generalized conductivities and responsivities, characterizing completely the linear response of the system to external biases and sources, are automatically computed from the solutions. We solve two simple example of magneto-transport and thermoelectric transport for illustrative purpose."
"Deep Learning Framework From Scratch Using Numpy This work is a rigorous development of a complete and general-purpose deep learning framework from the ground up. The fundamental components of deep learning - automatic differentiation and gradient methods of optimizing multivariable scalar functions - are developed from elementary calculus and implemented in a sensible object-oriented approach using only Python and the Numpy library. Demonstrations of solved problems using the framework, named ArrayFlow, include a computer vision classification task, solving for the shape of a catenary, and a 2nd order differential equation."
"PeleNet: A Reservoir Computing Framework for Loihi High-level frameworks for spiking neural networks are a key factor for fast prototyping and efficient development of complex algorithms. Such frameworks have emerged in the last years for traditional computers, but programming neuromorphic hardware is still a challenge. Often low level programming with knowledge about the hardware of the neuromorphic chip is required. The PeleNet framework aims to simplify reservoir computing for the neuromorphic hardware Loihi. It is build on top of the NxSDK from Intel and is written in Python. The framework manages weight matrices, parameters and probes. In particular, it provides an automatic and efficient distribution of networks over several cores and chips. With this, the user is not confronted with technical details and can concentrate on experiments."
"What the new RooFit can do for your analysis RooFit is a toolkit for statistical modelling and fitting, and together with RooStats it is used for measurements and statistical tests by most experiments in particle physics. Since one year, RooFit is being modernised. In this talk, improvements already released with ROOT will be discussed, such as faster data loading, vectorised computations and more standard-like interfaces. These allow for speeding up unbinned fits by several factors, and make RooFit easier to use from both C++ and Python."
"Normalization of Different Swedish Dialects Spoken in Finland Our study presents a dialect normalization method for different Finland Swedish dialects covering six regions. We tested 5 different models, and the best model improved the word error rate from 76.45 to 28.58. Contrary to results reported in earlier research on Finnish dialects, we found that training the model with one word at a time gave best results. We believe this is due to the size of the training data available for the model. Our models are accessible as a Python package. The study provides important information about the adaptability of these methods in different contexts, and gives important baselines for further study."
"Handy Formulas for Binomial Moments Despite the relevance of the binomial distribution for probability theory and applied statistical inference, its higher-order moments are poorly understood. The existing formulas are either not general enough, or not structured or simplified enough for intended applications. This paper introduces novel formulas for binomial moments, in terms of \emph{variance} rather than success probability. The obtained formulas are arguably better structured and simpler compared to prior works. In addition, the paper presents algorithms to derive these formulas along with working implementation in the Python symbolic algebra package. The novel approach is a combinatorial argument coupled with clever algebraic simplifications which rely on symmetrization theory. As an interesting byproduct we establish \emph{asymptotically sharp estimates for central binomial moments}, improving upon partial results from prior works."
"Thermodynamic Coefficients of Ideal Fermi-gas We present analytical formulae for the first and second derivatives of the Helmholtz free energy of non-relativistic ideal Fermi-gas. Important thermodynamic quantities such as heat capacity, sound velocity, heat capacity ratio and others can be easily expressed through the derivatives. We demonstrate correct ideal Boltzmann gas and low--temperature Fermi-gas asymptotes and derive corrections to thermodynamic functions for these limiting cases. Numerical computations of thermodynamic properties of ideal Fermi-gas can be accurately performed using the developed freely available Python module ifg."
"Optimization Techniques to Improve Inference Performance of a Forward Propagating Neural Network on an FPGA This paper describes an optimized implementation of a Forward Propagating Classification Neural Network which has been previously trained. The implementation described highlights a novel means of using Python scripts to generate a Verilog hardware implementation. The characteristics of this implementation include optimizations to scale input data, use selected addends instead of multiplication functions, hardware friendly activation functions and simplified output selection. Inference performance comparison of a 28x28 pixel 'hand-written' recognition NN between a software implementation on an Intel i7 vs a Xilinx FPGA will be detailed."
"AMICI: High-Performance Sensitivity Analysis for Large Ordinary Differential Equation Models Ordinary differential equation models facilitate the understanding of cellular signal transduction and other biological processes. However, for large and comprehensive models, the computational cost of simulating or calibrating can be limiting. AMICI is a modular toolbox implemented in C++/Python/MATLAB that provides efficient simulation and sensitivity analysis routines tailored for scalable, gradient-based parameter estimation and uncertainty quantification.   AMICI is published under the permissive BSD-3-Clause license with source code publicly available on https://github.com/AMICI-dev/AMICI. Citeable releases are archived on Zenodo."
"aw_nas: A Modularized and Extensible NAS framework Neural Architecture Search (NAS) has received extensive attention due to its capability to discover neural network architectures in an automated manner. aw_nas is an open-source Python framework implementing various NAS algorithms in a modularized manner. Currently, aw_nas can be used to reproduce the results of mainstream NAS algorithms of various types. Also, due to the modularized design, one can simply experiment with different NAS algorithms for various applications with awnas (e.g., classification, detection, text modeling, fault tolerance, adversarial robustness, hardware efficiency, and etc.). Codes and documentation are available at https://github.com/walkerning/aw_nas."
"Monte Carlo calculations for simulating electron scattering in gas phase Here we present the derivation, description and results of a Monte Carlo-based algorithm for simulating inelastic scattering of photo-electrons when passing through some scattering medium, such as a gas atmosphere or a solid material. The code used to run these simulations was written in python and is freely available online (https://github.com/surfaceanalytics/montecarlo_electron_scattering)."
"BDNNSurv: Bayesian deep neural networks for survival analysis using pseudo values There has been increasing interest in modeling survival data using deep learning methods in medical research. In this paper, we proposed a Bayesian hierarchical deep neural networks model for modeling and prediction of survival data. Compared with previously studied methods, the new proposal can provide not only point estimate of survival probability but also quantification of the corresponding uncertainty, which can be of crucial importance in predictive modeling and subsequent decision making. The favorable statistical properties of point and uncertainty estimates were demonstrated by simulation studies and real data analysis. The Python code implementing the proposed approach was provided."
"Distributed Double Machine Learning with a Serverless Architecture This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation \texttt{DoubleML-Serverless} for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs."
Frequency Limited $\mathcal{H}_2$ Optimal Model Reduction of Large-Scale Sparse Dynamical Systems We mainly consider the frequency limited $\mathcal{H}_2$ optimal model order reduction of large-scale sparse generalized systems. For this purpose we need to solve two Sylvester equations. This paper proposes efficient algorithm to solve them efficiently. The ideas are also generalized to index-1 descriptor systems. Numerical experiments are carried out using Python Programming Language and the results are presented to demonstrate the approximation accuracy and computational efficiency of the proposed techniques.
"The full approximation storage multigrid scheme: A 1D finite element example This note describes the full approximation storage (FAS) multigrid scheme for an easy one-dimensional nonlinear boundary value problem. The problem is discretized by a simple finite element (FE) scheme. We apply both FAS V-cycles and F-cycles, with a nonlinear Gauss-Seidel smoother, to solve the resulting finite-dimensional problem. The mathematics of the FAS restriction and prolongation operators, in the FE case, are explained. A self-contained Python program implements the scheme. Optimal performance, i.e. work proportional to the number of unknowns, is demonstrated for both kinds of cycles, including convergence nearly to discretization error in a single F-cycle."
"UltraNest -- a robust, general purpose Bayesian inference engine UltraNest is a general-purpose Bayesian inference package for parameter estimation and model comparison. It allows fitting arbitrary models specified as likelihood functions written in Python, C, C++, Fortran, Julia or R. With a focus on correctness and speed (in that order), UltraNest is especially useful for multi-modal or non-Gaussian parameter spaces, computational expensive models, in robust pipelines. Parallelisation to computing clusters and resuming incomplete runs is available."
"From pixels to notes: a computational implementation of synaesthesia for cultural artefacts Synaesthesia is a condition that enables people to sense information in the form of several senses at once. This work describes a Python implementation of a simulation of synaesthesia between listening to music and viewing a painting. Based on Scriabin's definition, we developed a deterministic process to produce a melody after processing a painting, mimicking the production of notes from colours in the field of view of persons experiencing synaesthesia."
"REvolver: Automated running and matching of couplings and masses in QCD In this article we present REvolver, a C++ library for renormalization group evolution and automatic flavor matching of the QCD coupling and quark masses, as well as precise conversion between various quark mass renormalization schemes. The library systematically accounts for the renormalization group evolution of low-scale short-distance masses which depend linearly on the renormalization scale and sums logarithmic terms of high and low scales that are missed by the common logarithmic renormalization scale evolution. The library can also be accessed through Mathematica and Python interfaces and provides renormalization group evolution for complex renormalization scales as well."
"A computer algebra system for the study of commutativity up-to-coherent homotopies The Python package ComCH is a lightweight specialized computer algebra system that provides models for well known objects, the surjection and Barratt-Eccles operads, parameterizing the product structure of algebras that are commutative in a derived sense. The primary examples of such algebras treated by ComCH are the cochain complexes of spaces, for which it provides effective constructions of Steenrod cohomology operations at all prime."
"Quasi-Monte Carlo Software Practitioners wishing to experience the efficiency gains from using low discrepancy sequences need correct, well-written software. This article, based on our MCQMC 2020 tutorial, describes some of the better quasi-Monte Carlo (QMC) software available. We highlight the key software components required to approximate multivariate integrals or expectations of functions of vector random variables by QMC. We have combined these components in QMCPy, a Python open source library, which we hope will draw the support of the QMC community. Here we introduce QMCPy."
"MUDES: Multilingual Detection of Offensive Spans The interest in offensive content identification in social media has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a Python API for developers, and a user-friendly web-based interface. A detailed description of MUDES' components is presented in this paper."
"Automatic Code Generation using Pre-Trained Language Models Recent advancements in natural language processing \cite{gpt2} \cite{BERT} have led to near-human performance in multiple natural language tasks. In this paper, we seek to understand whether similar techniques can be applied to a highly structured environment with strict syntax rules. Specifically, we propose an end-to-end machine learning model for code generation in the Python language built on-top of pre-trained language models. We demonstrate that a fine-tuned model can perform well in code generation tasks, achieving a BLEU score of 0.22, an improvement of 46\% over a reasonable sequence-to-sequence baseline. All results and related code used for training and data processing are available on GitHub."
"Transport Services: A Modern API for an Adaptive Internet Transport Layer Transport services (TAPS) is a working group of the Internet's standardization body, the Internet Engineering Task Force (IETF). TAPS defines a new recommended API for the Internet's transport layer. This API gives access to a wide variety of services from various protocols, and it is protocol-independent: the transport layer becomes adaptive, and applications are no longer statically bound to a particular protocol and/or network interface. We give an overview of the TAPS API, and we demonstrate its flexibility and ease of use with an example using a Python-based open-source implementation."
"A reduced parallel transport equation on Lie Groups with a left-invariant metric This paper presents a derivation of the parallel transport equation expressed in the Lie algebra of a Lie group endowed with a left-invariant metric.The use of this equation is exemplified on the group of rigid body motions SE(3), using basic numerical integration schemes, and compared to the pole ladder algorithm. This results in a stable and efficient implementation of parallel transport. The implementation leverages the python package geomstats and is available online."
"Restoring Execution Environments of Jupyter Notebooks More than ninety percent of published Jupyter notebooks do not state dependencies on external packages. This makes them non-executable and thus hinders reproducibility of scientific results. We present SnifferDog, an approach that 1) collects the APIs of Python packages and versions, creating a database of APIs; 2) analyzes notebooks to determine candidates for required packages and versions; and 3) checks which packages are required to make the notebook executable (and ideally, reproduce its stored results). In its evaluation, we show that SnifferDog precisely restores execution environments for the largest majority of notebooks, making them immediately executable for end users."
"Chance constrained problems: a bilevel convex optimization perspective Chance constraints are a valuable tool for the design of safe decisions in uncertain environments; they are used to model satisfaction of a constraint with a target probability. However, because of possible non-convexity and non-smoothness, optimizing over a chance constrained set is challenging. In this paper, we establish an exact reformulation of chance constrained problems as a bilevel problems with convex lower-levels. We then derive a tractable penalty approach, where the penalized objective is a difference-of-convex function that we minimize with a suitable bundle algorithm. We release an easy-to-use open-source python toolbox implementing the approach, with a special emphasis on fast computational subroutines."
"Failure-Tolerant Contract-Based Design of an Automated Valet Parking System using a Directive-Response Architecture Increased complexity in cyber-physical systems calls for modular system design methodologies that guarantee correct and reliable behavior, both in normal operations and in the presence of failures. This paper aims to extend the contract-based design approach using a directive-response architecture to enable reactivity to failure scenarios. The architecture is demonstrated on a modular automated valet parking (AVP) system. The contracts for the different components in the AVP system are explicitly defined, implemented, and validated against a Python implementation."
"Thompson's $V$ in MCG of mixing SFT by PW-linear homeos In a recent paper, we showed that groups admitting ""veelike actions"" on a finite language embed in mapping class groups of certain two-sided subshifts. In this note, we illustrate this theorem for the embedding of Thompson's $V$ by exhibiting the piecewise linear local rules for the embedding. These turn out to split the embedding into the mapping class group, showing that $V$ even embeds in the mapping class group by homeomorphisms."
"GrASP: A Library for Extracting and Exploring Human-Interpretable Textual Patterns Data exploration is an important step of every data science and machine learning project, including those involving textual data. We provide a Python library for GrASP, an existing algorithm for drawing patterns from textual data. The library is equipped with a web-based interface empowering human users to conveniently explore the data and the extracted patterns. We also demonstrate the use of the library in two settings (spam detection and argument mining) and discuss future deployments of the library, e.g., beyond textual data exploration."
"Computer Algebra in R with caracas The capability of R to do symbolic mathematics is enhanced by the caracas package. This package uses the Python computer algebra library SymPy as a back-end but caracas is tightly integrated in the R environment, thereby enabling the R user with symbolic mathematics within R. Key components of the caracas package are illustrated in this paper. Examples are taken from statistics and mathematics. The caracas package integrates well with e.g. Rmarkdown, and as such creation of scientific reports and teaching is supported."
"Deep Deterministic Path Following This paper deploys the Deep Deterministic Policy Gradient (DDPG) algorithm for longitudinal and lateral control of a simulated car to solve a path following task. The DDPG agent was implemented using PyTorch and trained and evaluated on a custom kinematic bicycle environment created in Python. The performance was evaluated by measuring cross-track error and velocity error, relative to a reference path. Results show how the agent can learn a policy allowing for small cross-track error, as well as adapting the acceleration to minimize the velocity error."
"Code generation for productive portable scalable finite element simulation in Firedrake Creating scalable, high performance PDE-based simulations requires a suitable combination of discretizations, differential operators, preconditioners and solvers. The required combination changes with the application and with the available hardware, yet software development time is a severely limited resource for most scientists and engineers. Here we demonstrate that generating simulation code from a high-level Python interface provides an effective mechanism for creating high performance simulations from very few lines of user code. We demonstrate that moving from one supercomputer to another can require significant algorithmic changes to achieve scalable performance, but that the code generation approach enables these algorithmic changes to be achieved with minimal development effort."
"libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations Almost all applications stop scaling at some point; those that don't are seldom performant when considering time to solution on anything but aspirational/unicorn resources. Recognizing these tradeoffs as well as greater user functionality in a near-term exascale computing era, we present libEnsemble, a library aimed at particular scalability- and capability-stretching uses. libEnsemble enables running concurrent instances of an application in dynamically allocated ensembles through an extensible Python library. We highlight the structure, execution, and capabilities of the library on leading pre-exascale environments as well as advanced capabilities for exascale environments and beyond."
"Near-miss Identities and Spinor Genus Classification of Ternary Quadratic Forms with Congruence Conditions In this paper, near-miss identities for the number of representations of some integral ternary quadratic forms with congruence conditions are found and proven. The genus and spinor genus of the corresponding lattice cosets are then classified. Finally, a complete genus and spinor genus classification for all conductor 2 lattice cosets of 2-adically unimodular lattices is given."
"ZMCintegral-v5.1: Support for Multi-function Integrations on GPUs In this new version of ZMCintegral, we have added the functionality of multi-function integrations, i.e. the ability to integrate more than $10^{3}$ different functions on GPUs. The Python API remains the similar as the previous versions. For integrands less than 5 dimensions, it usually takes less than 10 minutes to finish the evaluation of $10^{3}$ integrations on one Tesla v100 card. The performance scales linearly with the increasing of the GPUs."
"HOSS! The Hall-D Online Skim System (HOSS) was developed to simultaneously solve two issues for the high intensity GlueX experiment. One was to parallelize the writing of raw data files to disk in order to improve bandwidth. The other was to distribute the raw data across multiple compute nodes in order to produce calibration \textit{skims} of the data online. The highly configurable system employs RDMA, RAM disks, and zeroMQ driven by Python to simultaneously store and process the full high intensity GlueX data stream."
"Theory and Simulation of the Ising Model We have provided a concise introduction to the Ising model as one of the most important models in statistical mechanics and in studying the phenomenon of phase transition. The required theoretical background and derivation of the Hamiltonian of the model have also been presented. We finally have discussed the computational method and details to numerically solve the two- and three-dimensional Ising problems using Monte Carlo simulations. The related computer codes in both Python and Fortran, as well as a simulation trick to visualize the spin lattice, have also been provided."
"LEGOEval: An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing We present LEGOEval, an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform, Amazon Mechanical Turk. Compared to existing toolkits, LEGOEval features a flexible task design by providing a Python API that maps to commonly used React.js interface components. Researchers can personalize their evaluation procedures easily with our built-in pages as if playing with LEGO blocks. Thus, LEGOEval provides a fast, consistent method for reproducing human evaluation results. Besides the flexible task design, LEGOEval also offers an easy API to review collected data."
"ADAM: A Sandbox for Implementing Language Learning We present ADAM, a software system for designing and running child language learning experiments in Python. The system uses a virtual world to simulate a grounded language acquisition process in which the language learner utilizes cognitively plausible learning algorithms to form perceptual and linguistic representations of the observed world. The modular nature of ADAM makes it easy to design and test different language learning curricula as well as learning algorithms. In this report, we describe the architecture of the ADAM system in detail, and illustrate its components with examples. We provide our code."
"On the Distribution of the Information Density of Gaussian Random Vectors: Explicit Formulas and Tight Approximations Based on the canonical correlation analysis we derive series representations of the probability density function (PDF) and the cumulative distribution function (CDF) of the information density of arbitrary Gaussian random vectors. Using the series representations we give closed-form expressions of the PDF and CDF for important special cases and derive tight approximations for the general case. Furthermore, we derive recurrence formulas, which allow very efficient numerical calculations with an arbitrarily high accuracy as demonstrated with an implementation in Python publicly available on GitLab. Finally, we discuss the (in)validity of Gaussian approximations of the information density."
"Deeply-Debiased Off-Policy Interval Estimation Off-policy evaluation learns a target policy's value with a historical dataset generated by a different behavior policy. In addition to a point estimate, many applications would benefit significantly from having a confidence interval (CI) that quantifies the uncertainty of the point estimate. In this paper, we propose a novel deeply-debiasing procedure to construct an efficient, robust, and flexible CI on a target policy's value. Our method is justified by theoretical results and numerical experiments. A Python implementation of the proposed procedure is available at https://github.com/RunzheStat/D2OPE."
"Diplomat: A conversational agent framework for goal-oriented group discussion Recent work in human-computer interaction has explored the use of conversational agents as facilitators for group goal-oriented discussions. Inspired by this work and by the apparent lack of tooling available to support it, we created Diplomat, a Python-based framework for building conversational agent facilitators. Diplomat is designed to support simple specification of agent functionality as well as customizable integration with online chat services. We document a preliminary user study we conducted to help inform the design of Diplomat. We also describe the architecture, capabilities, and limitations of our tool, which we have shared on GitHub."
"ATHENA: Advanced Techniques for High Dimensional Parameter Spaces to Enhance Numerical Analysis ATHENA is an open source Python package for reduction in parameter space. It implements several advanced numerical analysis techniques such as Active Subspaces (AS), Kernel-based Active Subspaces (KAS), and Nonlinear Level-set Learning (NLL) method. It is intended as a tool for regression, sensitivity analysis, and in general to enhance existing numerical simulations' pipelines tackling the curse of dimensionality. Source code, documentation, and several tutorials are available on GitHub at https://github.com/mathLab/ATHENA under the MIT license."
"Design, Construction and Implementation of Stewart Platform with Control of Rolling Ball on Platform through Artificial Vision Artificial vision (AV) has recently emerged as an extremely important tool to help control robots with or without minimal human interaction. This article presents the design and construction of a parallel robot called a Stewart platform. Using Python as the main programming language, we implement an AV module with modern feedback control techniques that guide the position of a rolling ball over a Stewart platform."
"Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting This companion paper supports the replication of the fashion trend forecasting experiments with the KERN (Knowledge Enhanced Recurrent Network) method that we presented in the ICMR 2020. We provide an artifact that allows the replication of the experiments using a Python implementation. The artifact is easy to deploy with simple installation, training and evaluation. We reproduce the experiments conducted in the original paper and obtain similar performance as previously reported. The replication results of the experiments support the main claims in the original paper."
"Self-Supervised Bug Detection and Repair Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software."
"Training of SSD(Single Shot Detector) for Facial Detection using Nvidia Jetson Nano In this project, we have used the computer vision algorithm SSD (Single Shot detector) computer vision algorithm and trained this algorithm from the dataset which consists of 139 Pictures. Images were labeled using Intel CVAT (Computer Vision Annotation Tool)   We trained this model for facial detection. We have deployed our trained model and software in the Nvidia Jetson Nano Developer kit. Model code is written in Pytorch's deep learning framework. The programming language used is Python."
"TensorFlow RiemOpt: a library for optimization on Riemannian manifolds The adoption of neural networks and deep learning in non-Euclidean domains has been hindered until recently by the lack of scalable and efficient learning frameworks. Existing toolboxes in this space were mainly motivated by research and education use cases, whereas practical aspects, such as deploying and maintaining machine learning models, were often overlooked.   We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python library for optimization on Riemannian manifolds in TensorFlow. The library is designed with the aim for a seamless integration with the TensorFlow ecosystem, targeting not only research, but also streamlining production machine learning pipelines."
"Low-cost Stereovision system (disparity map) for few dollars The paper presents an analysis of the latest developments in the field of stereo vision in the low-cost segment, both for prototypes and for industrial designs. We described the theory of stereo vision and presented information about cameras and data transfer protocols and their compatibility with various devices. The theory in the field of image processing for stereo vision processes is considered and the calibration process is described in detail. Ultimately, we presented the developed stereo vision system and provided the main points that need to be considered when developing such systems. The final, we presented software for adjusting stereo vision parameters in real-time in the python language in the Windows operating system."
"Content Addressable Parallel Processors on a FPGA In this short article, we report on the implementation of a Content Addressable Parallel Processor using a FPGA. While Content addressable memories have been implemented in FPGAs, to our knowledge this is the first implementation in FPGA of Caxton C. Foster's vision of parallel processing, particularly the notions of parallel write as well as the combining of output values, which are usually missing in more typical CAM implementations, such as the ones designed for network routing. The resulting CAPP is made accessible to a host computer over a USB/UART interface, using a straightforward serial protocol that is demonstrated using a Python-based driver."
"Cleaning large-dimensional covariance matrices for correlated samples A non-linear shrinkage estimator of large-dimensional covariance matrices is derived in a setting of auto-correlated samples, thus generalizing the recent formula by Ledoit-P\'{e}ch\'{e}. The calculation is facilitated by random matrix theory. The result is turned into an efficient algorithm, and an associated Python library, shrinkage, with help of Ledoit-Wolf kernel estimation technique. An example of exponentially-decaying auto-correlations is presented."
TNQMetro: Tensor-network based package for efficient quantum metrology computations TNQMetro is a numerical package written in Python for calculations of fundamental quantum bounds on measurement precision. Thanks to the usage of the tensor-network formalism it can beat the curse of dimensionality and provides an efficient framework to calculate bounds for finite size system as well as determine the asymptotic scaling of precision in systems where quantum enhancement amounts to a constant factor improvement over the Standard Quantum Limit. It is written in a user-friendly way so that the basic functions do not require any knowledge of tensor networks.
"Mathematical Modeling of Heat Conduction This report describes a mathematical model of heat conduction. The differential equation for heat conduction in one dimensional rod has been derived. The explicit finite difference numerical method is used to solve this differential equation. Then for simulation, a code was written in using python libraries via Jupyter notebook. The simulation carried out for Aluminum, Copper and Mild Steel rods and results were discussed."
"Tianshou: a Highly Modularized Deep Reinforcement Learning Library We present Tianshou, a highly modularized python library for deep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou aims to provide building blocks to replicate common RL experiments and has officially supported more than 15 classic algorithms succinctly. To facilitate related research and prove Tianshou's reliability, we release Tianshou's benchmark of MuJoCo environments, covering 9 classic algorithms and 9/13 Mujoco tasks with state-of-the-art performance. We open-sourced Tianshou at https://github.com/thu-ml/tianshou/, which has received over 3k stars and become one of the most popular PyTorch-based DRL libraries."
"Approximating Optimal Asset Allocations using Simulated Bifurcation This paper investigates the application of Simulated Bifurcation algorithms to approximate optimal asset allocations. It will provide the reader with an explanation of the physical principles underlying the method and a Python implementation of the latter applied to 441 assets belonging to the S&P500 index. In addition, the paper tackles the problem of the selection of an optimal sub-allocation; in this particular case, we find an adequate solution within an unrivaled timescale."
"JVMC: Versatile and performant variational Monte Carlo leveraging automated differentiation and GPU acceleration The introduction of Neural Quantum States (NQS) has recently given a new twist to variational Monte Carlo (VMC). The ability to systematically reduce the bias of the wave function ansatz renders the approach widely applicable. However, performant implementations are crucial to reach the numerical state of the art. Here, we present a Python codebase that supports arbitrary NQS architectures and model Hamiltonians. Additionally leveraging automatic differentiation, just-in-time compilation to accelerators, and distributed computing, it is designed to facilitate the composition of efficient NQS algorithms."
"A Global Optimisation Toolbox for Massively Parallel Engineering Optimisation A software platform for global optimisation, called PaGMO, has been developed within the Advanced Concepts Team (ACT) at the European Space Agency, and was recently released as an open-source project. PaGMO is built to tackle high-dimensional global optimisation problems, and it has been successfully used to find solutions to real-life engineering problems among which the preliminary design of interplanetary spacecraft trajectories - both chemical (including multiple flybys and deep-space maneuvers) and low-thrust (limited, at the moment, to single phase trajectories), the inverse design of nano-structured radiators and the design of non-reactive controllers for planetary rovers. Featuring an arsenal of global and local optimisation algorithms (including genetic algorithms, differential evolution, simulated annealing, particle swarm optimisation, compass search, improved harmony search, and various interfaces to libraries for local optimisation such as SNOPT, IPOPT, GSL and NLopt), PaGMO is at its core a C++ library which employs an object-oriented architecture providing a clean and easily-extensible optimisation framework. Adoption of multi-threaded programming ensures the efficient exploitation of modern multi-core architectures and allows for a straightforward implementation of the island model paradigm, in which multiple populations of candidate solutions asynchronously exchange information in order to speed-up and improve the optimisation process. In addition to the C++ interface, PaGMO's capabilities are exposed to the high-level language Python, so that it is possible to easily use PaGMO in an interactive session and take advantage of the numerous scientific Python libraries available."
"cphVB: A System for Automated Runtime Optimization and Parallelization of Vectorized Applications Modern processor architectures, in addition to having still more cores, also require still more consideration to memory-layout in order to run at full capacity. The usefulness of most languages is deprecating as their abstractions, structures or objects are hard to map onto modern processor architectures efficiently.   The work in this paper introduces a new abstract machine framework, cphVB, that enables vector oriented high-level programming languages to map onto a broad range of architectures efficiently. The idea is to close the gap between high-level languages and hardware optimized low-level implementations. By translating high-level vector operations into an intermediate vector bytecode, cphVB enables specialized vector engines to efficiently execute the vector operations.   The primary success parameters are to maintain a complete abstraction from low-level details and to provide efficient code execution across different, modern, processors. We evaluate the presented design through a setup that targets multi-core CPU architectures. We evaluate the performance of the implementation using Python implementations of well-known algorithms: a jacobi solver, a kNN search, a shallow water simulation and a synthetic stencil simulation. All demonstrate good performance."
"PDFgetX3: A rapid and highly automatable program for processing powder diffraction data into total scattering pair distribution functions PDFgetX3 is a new software application for converting X-ray powder diffraction data to atomic pair distribution function (PDF). PDFgetX3 has been designed for ease of use, speed and automated operation. The software can readily process hundreds of X-ray patterns within few seconds and is thus useful for high-throughput PDF studies, that measure numerous datasets as a function of time, temperature or other environment parameters. In comparison to the preceding programs, PDFgetX3 requires fewer inputs, less user experience and can be readily adopted by novice users. The live-plotting interactive feature allows to assess the effects of calculation parameters and select their optimum values. PDFgetX3 uses an ad-hoc data correction method, where the slowly-changing structure independent signal is filtered out to obtain coherent X-ray intensities that contain structure information. The outputs from PDFgetX3 have been verified by processing experimental PDFs from inorganic, organic and nanosized samples and comparing them to their counterparts from previous established software. In spite of different algorithm, the obtained PDFs were nearly identical and yielded highly similar results when used in structure refinement. PDFgetX3 is written in Python language and features well documented, reusable codebase. The software can be used either as standalone application or as a library of PDF-processing functions that can be called on from other Python scripts. The software is free for open academic research, but requires paid license for commercial use."
"Tribological Analysis of Ventral Scale Structure in a Python Regius in Relation to Laser Textured Surfaces Laser Texturing is one of the leading technologies applied to modify surface topography. To date, however, a standardized procedure to generate deterministic textures is virtually non-existent. In nature, especially in squamata, there are many examples of deterministic structured textures that allow species to control friction and condition their tribological response for efficient function. In this work, we draw a comparison between industrial surfaces and reptilian surfaces. We chose the python regius species as a bio-analogue with a deterministic surface. We first study the structural make up of the ventral scales of the snake (both construction and metrology). We further compare the metrological features of the ventral scales to experimentally recommended performance indicators of industrial surfaces extracted from open literature. The results indicate the feasibility of engineering a Laser Textured Surface based on the reptilian ornamentation constructs. It is shown that the metrological features, key to efficient function of a rubbing deterministic surface, are already optimized in the reptile. We further show that optimization in reptilian surfaces is based on synchronizing surface form, textures and aspects to condition the frictional response. Mimicking reptilian surfaces, we argue, may form a design methodology potentially capable of generating advanced deterministic surface constructs capable of efficient tribological function."
"PyRDM: A Python-based library for automating the management and online publication of scientific software and data The recomputability and reproducibility of results from scientific software requires access to both the source code and all associated input and output data. However, the full collection of these resources often does not accompany the key findings published in journal articles, thereby making it difficult or impossible for the wider scientific community to verify the correctness of a result or to build further research on it. This paper presents a new Python-based library, PyRDM, whose functionality aims to automate the process of sharing the software and data via online, citable repositories such as Figshare. The library is integrated into the workflow of an open-source computational fluid dynamics package, Fluidity, to demonstrate an example of its usage."
"libRoadRunner: A High Performance SBML Simulation and Analysis Library This paper presents libRoadRunner, an extensible, high-performance, cross-platform, open-source software library for the simulation and analysis of models \ expressed using Systems Biology Markup Language (SBML). SBML is the most widely used standard for representing dynamic networks, especially biochemical networks. libRoadRunner supports solution of both large models and multiple replicas of a single model on desktop, mobile and cluster computers. libRoadRunner is a self-contained library, able to run both as a component inside other tools via its C++ and C bindings andnteractively through its Python interface. The Python Application Programming Interface (API) is similar to the APIs of Matlab and SciPy, making it fast and easy to learn, even for new users. libRoadRunner uses a custom Just-In-Time (JIT) compiler built on the widely-used LLVM JIT compiler framework to compile SBML-specified models directly into very fast native machine code for a variety of processors, making it appropriate for solving very large models or multiple replicas of smaller models. libRoadRunner is flexible, supporting the bulk of the SBML specification (except for delay and nonlinear algebraic equations) and several of its extensions. It offers multiple deterministic and stochastic integrators, as well as tools for steady-state, stability analyses and flux balance analysis. We regularly update libRoadRunner binary distributions for Mac OS X, Linux and Windows and license them under Apache License Version 2.0. http://www.libroadrunner.org provides online documentation, full build instructions, binaries and a git source repository."
"ScalPy: A Python Package For Late Time Scalar Field Cosmology We present a python package ""ScalPy"" for studying the late time scalar field cosmology for a wide variety of scalar field models, namely the quintessence, tachyon and Galileon model. The package solves the autonomous system of equations for power law and exponential potential. But it can be easily generalized to add more complicated potential. For completeness, we also include the standard parameterization for dark energy models, e.g. the $\Lambda$CDM, $w$CDM, $w_{0}w_{a}$CDM as well as the GCG parameterization. The package also solves the linear growth equation for matter perturbations on sub-horizon scales. All the important observables related to background universe as well as to the perturbed universe, e.g. luminosity distance ($D_{L}(z)$), angular diameter distance ($D_{A}(z)$), normalized Hubble parameter ($h(z)$), lookback time ($t_{L}$), equation of state for the dark energy ($w(z)$), growth rate ($f=\frac{d \ln\delta}{d \ln a}$), linear matter power spectra ($P(k)$), and its normalization $\sigma_{8}$ can be obtained from this package. The code is further integrated with the publicly available MCMC hammer ""emcee"" to constrain the different models using the presently available observational data. The code is available online at \url{https://github.com/sum33it/scalpy}"
"ORBKIT - A Modular Python Toolbox for Cross-Platform Post-Processing of Quantum Chemical Wavefunction Data ORBKIT is a toolbox for post-processing electronic structure calculations based on a highly modular and portable Python architecture. The program allows computing a multitude of electronic properties of molecular systems on arbitrary spatial grids from the basis set representation of its electronic wavefunction, as well as several grid-independent properties. The required data can be extracted directly from the standard output of a large number of quantum chemical programs. ORBKIT can be used as a standalone program to determine standard quantities, for example, the electron density, molecular orbitals, and derivatives thereof. The cornerstone of ORBKIT is its modular structure. The existing basic functions can be arranged in an individual way and can be easily extended by user-written modules to determine any other desired quantities. ORBKIT offers multiple output formats that can be processed by common visualization tools (VMD, Molden, etc.). Additionally, ORBKIT possesses routines to order molecular orbitals computed at different nuclear configurations according to their electronic character and to interpolate the wavefunction between these configurations. The program is open-source under GNU-LGPLv3 license and freely available at http://sourceforge.net/projects/orbkit/. This article provides an overview of ORBKIT with particular focus on its capabilities and applicability, and includes several example calculations."
"The cosmic spiderweb: equivalence of cosmic, architectural, and origami tessellations For over twenty years, the term 'cosmic web' has guided our understanding of the large-scale arrangement of matter in the cosmos, accurately evoking the concept of a network of galaxies linked by filaments. But the physical correspondence between the cosmic web and structural-engineering or textile 'spiderwebs' is even deeper than previously known, and extends to origami tessellations as well. Here we explain that in a good structure-formation approximation known as the adhesion model, threads of the cosmic web form a spiderweb, i.e. can be strung up to be entirely in tension. The correspondence is exact if nodes sampling voids are included, and if structure is excluded within collapsed regions (walls, filaments and haloes), where dark-matter multistreaming and baryonic physics affect the structure. We also suggest how concepts arising from this link might be used to test cosmological models: for example, to test for large-scale anisotropy and rotational flows in the cosmos."
"Integration of an RFID Reader to a Wireless Sensor Network and using it to Identify an Individual Carrying RFID Tags The objective of this research is to integrate an RFID (Radio Frequency Identification) reader into a Wireless Sensor Network (WSN) to authorize or keep track of people carrying RFID tags. The objective was accomplished by integrating hardware and software. The hardware consisted of two WSN nodes - the RFID node connected to one of the WSN nodes, and a computer connected to the other WSN node. For the RFID equipment, we used the SM130-EK kit, which included the RFID reader and the RFID tags; and for the WSN, we used the Synapse Network Evaluation kit, which included the two sensor nodes. The software consisted of a program module developed in Python to control the microprocessors of the nodes; and a database controlled by a simple program to manage the tag IDs of people wearing them. The WSN and RFID nodes were connected through I2C interfacing. Also, the work of sending commands to the RFID node, to make it read a tag and send it back to the computer, was accomplished by the Python code developed which also controls the data signals. At the computer, the received tag ID is evaluated with other existing tag IDs on the database, to check if that tag has authorization or not to be in the covered area. Our research has the potential of being adapted for use with secure real-time access control applications involving WSN and RFID technologies."
"NIFTY - Numerical Information Field Theory - a versatile Python library for signal inference NIFTY, ""Numerical Information Field Theory"", is a software package designed to enable the development of signal inference algorithms that operate regardless of the underlying spatial grid and its resolution. Its object-oriented framework is written in Python, although it accesses libraries written in Cython, C++, and C for efficiency. NIFTY offers a toolkit that abstracts discretized representations of continuous spaces, fields in these spaces, and operators acting on fields into classes. Thereby, the correct normalization of operations on fields is taken care of automatically without concerning the user. This allows for an abstract formulation and programming of inference algorithms, including those derived within information field theory. Thus, NIFTY permits its user to rapidly prototype algorithms in 1D, and then apply the developed code in higher-dimensional settings of real world problems. The set of spaces on which NIFTY operates comprises point sets, n-dimensional regular grids, spherical spaces, their harmonic counterparts, and product spaces constructed as combinations of those. The functionality and diversity of the package is demonstrated by a Wiener filter code example that successfully runs without modification regardless of the space on which the inference problem is defined."
"An efficient way to assemble finite element matrices in vector languages Efficient Matlab codes in 2D and 3D have been proposed recently to assemble finite element matrices. In this paper we present simple, compact and efficient vectorized algorithms, which are variants of these codes, in arbitrary dimension, without the use of any lower level language. They can be easily implemented in many vector languages (e.g. Matlab, Octave, Python, Scilab, R, Julia, C++ with STL,...). The principle of these techniques is general, we present it for the assembly of several finite element matrices in arbitrary dimension, in the P1 finite element case. We also provide an extension of the algorithms to the case of a system of PDE's. Then we give an extension to piecewise polynomials of higher order. We compare numerically the performance of these algorithms in Matlab, Octave and Python, with that in FreeFEM++ and in a compiled language such as C. Examples show that, unlike what is commonly believed, the performance is not radically worse than that of C : in the best/worst cases, selected vector languages are respectively 2.3/3.5 and 2.9/4.1 times slower than C in the scalar and vector cases. We also present numerical results which illustrate the computational costs of these algorithms compared to standard algorithms and to other recent ones."
"UVMULTIFIT: A versatile tool for fitting astronomical radio interferometric data The analysis of astronomical interferometric data is often performed on the images obtained after deconvolution of the interferometer's point spread function (PSF). This strategy can be understood (especially for cases of sparse arrays) as fitting models to models, since the deconvolved images are already non-unique model representations of the actual data (i.e., the visibilities). Indeed, the interferometric images may be affected by visibility gridding, weighting schemes (e.g., natural vs. uniform), and the particulars of the (non-linear) deconvolution algorithms. Fitting models to the direct interferometric observables (i.e., the visibilities) is preferable in the cases of simple (analytical) sky intensity distributions. In this paper, we present UVMULTIFIT, a versatile library for fitting visibility data, implemented in a Python-based framework. Our software is currently based on the CASA package, but can be easily adapted to other analysis packages, provided they have a Python API. We have tested the software with synthetic data, as well as with real observations. In some cases (e.g., sources with sizes smaller than the diffraction limit of the interferometer), the results from the fit to the visibilities (e.g., spectra of close by sources) are far superior to the output obtained from the mere analysis of the deconvolved images. UVMULTIFIT is a powerful improvement of existing tasks to extract the maximum amount of information from visibility data, especially in cases close to the sensitivity/resolution limits of interferometric observations."
"gPhoton: The GALEX Photon Data Archive gPhoton is a new database product and software package that enables analysis of GALEX ultraviolet data at the photon level. The project's stand-alone, pure-Python calibration pipeline reproduces the functionality of the original mission pipeline to reduce raw spacecraft data to lists of time-tagged, sky-projected photons, which are then hosted in a publicly available database by the Mikulski Archive at Space Telescope (MAST). This database contains approximately 130 terabytes of data describing approximately 1.1 trillion sky-projected events with a timestamp resolution of five milliseconds. A handful of Python and command line modules serve as a front-end to interact with the database and to generate calibrated light curves and images from the photon-level data at user-defined temporal and spatial scales. The gPhoton software and source code are in active development and publicly available under a permissive license. We describe the motivation, design, and implementation of the calibration pipeline, database, and tools, with emphasis on divergence from prior work, as well as challenges created by the large data volume. We summarize the astrometric and photometric performance of gPhoton relative to the original mission pipeline. For a brief example of short time domain science capabilities enabled by gPhoton, we show new flares from the known M dwarf flare star CR Draconis. The gPhoton software has permanent object identifiers with the ASCL (ascl:1603.004) and DOI (doi:10.17909/T9CC7G). This paper describes the software as of version v1.27.2."
"Interactive Web Application for Exploring Matrices of Neural Connectivity We present here a browser-based application for visualizing patterns of connectivity in 3D stacked data matrices with large numbers of pairwise relations. Visualizing a connectivity matrix, looking for trends and patterns, and dynamically manipulating these values is a challenge for scientists from diverse fields, including neuroscience and genomics. In particular, high-dimensional neural data include those acquired via electroencephalography (EEG), electrocorticography (ECoG), magnetoencephalography (MEG), and functional MRI. Neural connectivity data contains multivariate attributes for each edge between different brain regions, which motivated our lightweight, open source, easy-to-use visualization tool for the exploration of these connectivity matrices to highlight connections of interest. Here we present a client-side, mobile-compatible visualization tool written entirely in HTML5/JavaScript that allows in-browser manipulation of user-defined files for exploration of brain connectivity. Visualizations can highlight different aspects of the data simultaneously across different dimensions. Input files are in JSON format, and custom Python scripts have been written to parse MATLAB or Python data files into JSON-loadable format. We demonstrate the analysis of connectivity data acquired via human ECoG recordings as a domain-specific implementation of our application. We envision applications for this interactive tool in fields seeking to visualize pairwise connectivity."
"Cosmic ray propagation with CRPropa 3 Solving the question of the origin of ultra-high energy cosmic rays (UHECRs) requires the development of detailed simulation tools in order to interpret the experimental data and draw conclusions on the UHECR universe. CRPropa is a public Monte Carlo code for the galactic and extragalactic propagation of cosmic ray nuclei above $\sim 10^{17}$ eV, as well as their photon and neutrino secondaries. In this contribution the new algorithms and features of CRPropa 3, the next major release, are presented. CRPropa 3 introduces time-dependent scenarios to include cosmic evolution in the presence of cosmic ray deflections in magnetic fields. The usage of high resolution magnetic fields is facilitated by shared memory parallelism, modulated fields and fields with heterogeneous resolution. Galactic propagation is enabled through the implementation of galactic magnetic field models, as well as an efficient forward propagation technique through transformation matrices. To make use of the large Python ecosystem in astrophysics CRPropa 3 can be steered and extended in Python."
"A High Performance Implementation of Spectral Clustering on CPU-GPU Platforms Spectral clustering is one of the most popular graph clustering algorithms, which achieves the best performance for many scientific and engineering applications. However, existing implementations in commonly used software platforms such as Matlab and Python do not scale well for many of the emerging Big Data applications. In this paper, we present a fast implementation of the spectral clustering algorithm on a CPU-GPU heterogeneous platform. Our implementation takes advantage of the computational power of the multi-core CPU and the massive multithreading and SIMD capabilities of GPUs. Given the input as data points in high dimensional space, we propose a parallel scheme to build a sparse similarity graph represented in a standard sparse representation format. Then we compute the smallest $k$ eigenvectors of the Laplacian matrix by utilizing the reverse communication interfaces of ARPACK software and cuSPARSE library, where $k$ is typically very large. Moreover, we implement a very fast parallelized $k$-means algorithm on GPUs. Our implementation is shown to be significantly faster compared to the best known Matlab and Python implementations for each step. In addition, our algorithm scales to problems with a very large number of clusters."
"DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering The level set tree approach of Hartigan (1975) provides a probabilistically based and highly interpretable encoding of the clustering behavior of a dataset. By representing the hierarchy of data modes as a dendrogram of the level sets of a density estimator, this approach offers many advantages for exploratory analysis and clustering, especially for complex and high-dimensional data. Several R packages exist for level set tree estimation, but their practical usefulness is limited by computational inefficiency, absence of interactive graphical capabilities and, from a theoretical perspective, reliance on asymptotic approximations. To make it easier for practitioners to capture the advantages of level set trees, we have written the Python package DeBaCl for DEnsity-BAsed CLustering. In this article we illustrate how DeBaCl's level set tree estimates can be used for difficult clustering tasks and interactive graphical data analysis. The package is intended to promote the practical use of level set trees through improvements in computational efficiency and a high degree of user customization. In addition, the flexible algorithms implemented in DeBaCl enjoy finite sample accuracy, as demonstrated in recent literature on density clustering. Finally, we show the level set tree framework can be easily extended to deal with functional data."
"DNest4: Diffusive Nested Sampling in C++ and Python In probabilistic (Bayesian) inferences, we typically want to compute properties of the posterior distribution, describing knowledge of unknown quantities in the context of a particular dataset and the assumed prior information. The marginal likelihood, also known as the ""evidence"", is a key quantity in Bayesian model selection. The Diffusive Nested Sampling algorithm, a variant of Nested Sampling, is a powerful tool for generating posterior samples and estimating marginal likelihoods. It is effective at solving complex problems including many where the posterior distribution is multimodal or has strong dependencies between variables. DNest4 is an open source (MIT licensed), multi-threaded implementation of this algorithm in C++11, along with associated utilities including: i) RJObject, a class template for finite mixture models, (ii) A Python package allowing basic use without C++ coding, and iii) Experimental support for models implemented in Julia. In this paper we demonstrate DNest4 usage through examples including simple Bayesian data analysis, finite mixture models, and Approximate Bayesian Computation."
"PyCDT: A Python toolkit for modeling point defects in semiconductors and insulators Point defects have a strong impact on the performance of semiconductor and insulator materials used in technological applications, spanning microelectronics to energy conversion and storage. The nature of the dominant defect types, how they vary with processing conditions, and their impact on materials properties are central aspects that determine the performance of a material in a certain application. This information is, however, difficult to access directly from experimental measurements. Consequently, computational methods, based on electronic density functional theory (DFT), have found widespread use in the calculation of point-defect properties. Here we have developed the Python Charged Defect Toolkit (PyCDT) to expedite the setup and post-processing of defect calculations with widely used DFT software. PyCDT has a user-friendly command-line interface and provides a direct interface with the Materials Project database. This allows for setting up many charged defect calculations for any material of interest, as well as post-processing and applying state-of-the-art electrostatic correction terms. Our paper serves as a documentation for PyCDT, and demonstrates its use in an application to the well-studied GaAs compound semiconductor. We anticipate that the PyCDT code will be useful as a framework for undertaking readily reproducible calculations of charged point-defect properties, and that it will provide a foundation for automated, high-throughput calculations."
"Learning Python Code Suggestion with a Sparse Pointer Network To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past."
"HyperTools: A Python toolbox for visualizing and manipulating high-dimensional data Data visualizations can reveal trends and patterns that are not otherwise obvious from the raw data or summary statistics. While visualizing low-dimensional data is relatively straightforward (for example, plotting the change in a variable over time as (x,y) coordinates on a graph), it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way. Here we present HypeTools, a Python toolbox for visualizing and manipulating large, high-dimensional datasets. Our primary approach is to use dimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to embed high-dimensional datasets in a lower-dimensional space, and plot the data using a simple (yet powerful) API with many options for data manipulation [e.g. hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot styling. The toolbox is designed around the notion of data trajectories and point clouds. Just as the position of an object moving through space can be visualized as a 3D trajectory, HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories may be plotted as interactive static plots or visualized as animations. These same dimensionality reduction and alignment algorithms can also reveal structure in static datasets (e.g. collections of observations or attributes). We present several examples showcasing how using our toolbox to explore data through trajectories and low-dimensional embeddings can reveal deep insights into datasets across a wide variety of domains."
"The igmspec Database of Public Spectra Probing the Intergalactic Medium We describe v02 of igmspec, a database of publically available ultraviolet, optical, and near-infrared spectra that probe the intergalactic medium (IGM). This database, a child of the specdb repository in the specdb github organization, comprises 403277 unique sources and 434686 spectra obtained with the world's greatest observatories. All of these data are distributed in a single ~25 GB HDF5 file maintained at the University of California Observatories and the University of California, Santa Cruz. The specdb software package includes Python scripts and modules for searching the source catalog and spectral datasets, and software links to the linetools package for spectral analysis. The repository also includes software to generate private spectral datasets that are compliant with International Virtual Observatory Alliance (IVOA) protocols and a Python-based interface for IVOA Simple Spectral Access queries. Future versions of igmspec will ingest other sources (e.g. gamma-ray burst afterglows) and other surveys as they become publicly available. The overall goal is to include every spectrum that effectively probes the IGM. Future databases of specdb may include publicly available galaxy spectra (exgalspec) and published supernovae spectra (snspec). The community is encouraged to join the effort on github: https://github.com/specdb"
"GLACiAR, an open-source python tool for simulations of source recovery and completeness in galaxy surveys The luminosity function is a fundamental observable for characterizing how galaxies form and evolve throughout the cosmic history. One key ingredient to derive this measurement from the number counts in a survey is the characterization of the completeness and redshift selection functions for the observations. In this paper we present GLACiAR, an open python tool available on GitHub to estimate the completeness and selection functions in galaxy surveys. The code is tailored for multiband imaging surveys aimed at searching for high-redshift galaxies through the Lyman Break technique, but it can be applied broadly. The code generates artificial galaxies that follow Sersic profiles with different indexes and with customizable size, redshift and spectral energy distribution properties, adds them to input images, and measures the recovery rate. To illustrate this new software tool, we apply it to quantify the completeness and redshift selection functions for J-dropouts sources (redshift z~10 galaxies) in the Hubble Space Telescope Brightest of Reionizing Galaxies Survey (BoRG). Our comparison with a previous completeness analysis on the same dataset shows overall agreement, but also highlights how different modelling assumptions for artificial sources can impact completeness estimates."
"Bernaise: A flexible framework for simulating two-phase electrohydrodynamic flows in complex domains Bernaise (Binary ElectRohydrodyNAmIc SolvEr) is a flexible high-level finite element solver of two-phase electrohydrodynamic flow in complex geometries. Two-phase flow with electrolytes is relevant across a broad range of systems and scales, from 'lab-on-a-chip' devices for medical diagnostics to enhanced oil recovery at the reservoir scale. For the strongly coupled multi-physics problem, we employ a recently developed thermodynamically consistent model which combines a generalized Nernst-Planck equation for ion transport, the Poisson equation for electrostatics, the Cahn-Hilliard equation for the phase field (describing the interface separating the phases), and the Navier-Stokes equations for fluid flow. As an efficient alternative to solving the coupled system of partial differential equations in a monolithic manner, we present a linear, decoupled numerical scheme which sequentially solves the three sets of equations. The scheme is validated by comparison to limiting cases where analytical solutions are available, benchmark cases, and by the method of manufactured solution. The solver operates on unstructured meshes and is therefore well suited to handle arbitrarily shaped domains and problem set-ups where, e.g., very different resolutions are required in different parts of the domain. Bernaise is implemented in Python via the FEniCS framework, which effectively utilizes MPI and domain decomposition, and should therefore be suitable for large-scale/high-performance computing. Further, new solvers and problem set-ups can be specified and added with ease to the Bernaise framework by experienced Python users."
"astroquery: An Astronomical Web-Querying Package in Python astroquery is a collection of tools for requesting data from databases hosted on remote servers with interfaces exposed on the internet, including those with web pages but without formal application program interfaces (APIs). These tools are built on the Python requests package, which is used to make HTTP requests, and astropy, which provides most of the data parsing functionality. astroquery modules generally attempt to replicate the web page interface provided by a given service as closely as possible, making the transition from browser-based to command-line interaction easy. astroquery has received significant contributions from throughout the astronomical community, including several significant contributions from telescope archives. astroquery enables the creation of fully reproducible workflows from data acquisition through publication. This paper describes the philosophy, basic structure, and development model of the astroquery package. The complete documentation for astroquery can be found at http://astroquery.readthedocs.io/."
"Multiple scales analysis of slow--fast quasilinear systems This article illustrates the application of multiple scales analysis to two archetypal quasilinear systems; i.e. to systems involving fast dynamical modes, called fluctuations, that are not directly influenced by fluctuation--fluctuation nonlinearities but nevertheless are strongly coupled to a slow variable whose evolution may be fully nonlinear. In the first case, fast waves drive a slow, spatially-inhomogeneous evolution of their celerity field. Multiple scales analysis confirms that, although the energy $E$, the angular frequency $\omega$, and the modal structure of the waves evolve, the wave action $E/\omega$ is conserved in the absence of forcing and dissipation. In the second system, the fast modes undergo an instability that is saturated through a feedback on the slow variable. A new multiscale analysis is developed to treat this case. The key technical point, confirmed by the analysis, is that the fluctuation energy and mode structure evolve slowly to ensure that the slow field remains in a state of near marginal stability. These two model systems appear to be generic, being representative of many if not all quasilinear systems. In each case, numerical simulations of both the full and reduced dynamical systems are performed to highlight the accuracy and efficiency of the multiple scales approach. Python codes are provided as supplementary material."
"PyTorch: An Imperative Style, High-Performance Deep Learning Library Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.   In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.   We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks."
"TypeWriter: Neural Type Prediction with Search-based Validation Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, IDE support is limited, and APIs are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic type prediction with search-based refinement of predicted types. TypeWriter's predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that TypeWriter's type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, TypeWriter can fully annotate between 14% to 44% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. TypeWriter currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes."
"A biomolecular electrostatics solver using Python, GPUs and boundary elements that can handle solvent-filled cavities and Stern layers The continuum theory applied to bimolecular electrostatics leads to an implicit-solvent model governed by the Poisson-Boltzmann equation. Solvers relying on a boundary integral representation typically do not consider features like solvent-filled cavities or ion-exclusion (Stern) layers, due to the added difficulty of treating multiple boundary surfaces. This has hindered meaningful comparisons with volume-based methods, and the effects on accuracy of including these features has remained unknown. This work presents a solver called PyGBe that uses a boundary-element formulation and can handle multiple interacting surfaces. It was used to study the effects of solvent-filled cavities and Stern layers on the accuracy of calculating solvation energy and binding energy of proteins, using the well-known APBS finite-difference code for comparison. The results suggest that if required accuracy for an application allows errors larger than about 2%, then the simpler, single-surface model can be used. When calculating binding energies, the need for a multi-surface model is problem-dependent, becoming more critical when ligand and receptor are of comparable size. Comparing with the APBS solver, the boundary-element solver is faster when the accuracy requirements are higher. The cross-over point for the PyGBe code is in the order of 1-2% error, when running on one GPU card (NVIDIA Tesla C2075), compared with APBS running on six Intel Xeon CPU cores. PyGBe achieves algorithmic acceleration of the boundary element method using a treecode, and hardware acceleration using GPUs via PyCuda from a user-visible code that is all Python. The code is open-source under MIT license."
"Unified functional network and nonlinear time series analysis for complex systems science: The pyunicorn package We introduce the \texttt{pyunicorn} (Pythonic unified complex network and recurrence analysis toolbox) open source software package for applying and combining modern methods of data analysis and modeling from complex network theory and nonlinear time series analysis. \texttt{pyunicorn} is a fully object-oriented and easily parallelizable package written in the language Python. It allows for the construction of functional networks such as climate networks in climatology or functional brain networks in neuroscience representing the structure of statistical interrelationships in large data sets of time series and, subsequently, investigating this structure using advanced methods of complex network theory such as measures and models for spatial networks, networks of interacting networks, node-weighted statistics or network surrogates. Additionally, \texttt{pyunicorn} provides insights into the nonlinear dynamics of complex systems as recorded in uni- and multivariate time series from a non-traditional perspective by means of recurrence quantification analysis (RQA), recurrence networks, visibility graphs and construction of surrogate time series. The range of possible applications of the library is outlined, drawing on several examples mainly from the field of climatology."
"pyBioSig: optimizing group discrimination using genetic algorithms for biosignature discovery In medical sciences, a biomarker is ""a characteristic that is objectively measured and evaluated as an indicator of normal biological processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention"". Molecular experiments are providing rapid and systematic approaches to search for biomarkers, but because single-molecule biomarkers have shown a disappointing lack of robustness for clinical diagnosis, researchers have begun searching for distinctive sets of molecules, called ""biosignatures"". However, the most popular statistics are not appropriate for their identification, and the number of possible biosignatures to be tested is frequently intractable. In the present work, we developed a ""multivariate filter"" using genetic algorithms (GA) as a feature (gene) selector to optimize a measure of intra-group cohesion and inter-group dispersion. This method was implemented using Python and R (pyBioSig, available at https://github.com/fredgca/pybiosig under LGPL) and can be manipulated via graphical interface or Python scripts. Using it, we were able to identify putative biosignatures composed by just a few genes and capable of recovering multiple groups simultaneously in a hierarchical clustering, even ones that were not recovered using the whole transcriptome, within a feasible length of time using a personal computer. Our results allowed us to conclude that using GA to optimize our new intra-group cohesion and inter-group dispersion measure is a clear, effective, and computationally feasible strategy for the identification of putative ""omical"" biosignatures that could support discrimination among multiple groups simultaneously."
"PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark for Big-Data Platforms The rise of big data systems has created a need for benchmarks to measure and compare the capabilities of these systems. Big data benchmarks present unique scalability challenges. The supercomputing community has wrestled with these challenges for decades and developed methodologies for creating rigorous scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline benchmark employs supercomputing benchmarking methodologies to create a scalable benchmark that is reflective of many real-world big data processing systems. The PageRank pipeline benchmark builds on existing prior scalable benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. The linear algebraic nature of PageRank makes it well suited to being implemented using the GraphBLAS standard. The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance has been measured."
"TRIPPy: Trailed Image Photometry in Python Photometry of moving sources typically suffers from reduced signal-to-noise (SNR) or flux measurements biased to incorrect low values through the use of circular apertures. To address this issue we present the software package, TRIPPy: TRailed Image Photometry in Python. TRIPPy introduces the pill aperture, which is the natural extension of the circular aperture appropriate for linearly trailed sources. The pill shape is a rectangle with two semicircular end-caps, and is described by three parameters, the trail length and angle, and the radius. The TRIPPy software package also includes a new technique to generate accurate model point-spread functions (PSF) and trailed point-spread functions (TSF) from stationary background sources in sidereally tracked images. The TSF is merely the convolution of the model PSF, which consists of a moffat profile, and super sampled lookup table. From the TSF, accurate pill aperture corrections can be estimated as a function of pill radius with a accuracy of 10 millimags for highly trailed sources. Analogous to the use of small circular apertures and associated aperture corrections, small radius pill apertures can be used to preserve signal-to-noise of low flux sources, with appropriate aperture correction applied to provide an accurate, unbiased flux measurement at all SNR."
"A Bayesian Approach to Modelling Fine-Scale Spatial Dynamics of Non-State Terrorism: World Study, 2002-2013 To this day, terrorism persists as a worldwide threat, as exemplified by the ongoing lethal attacks perpetrated by ISIS in Iraq, Syria, Al Qaeda in Yemen, and Boko Haram in Nigeria. In response, states deploy various counterterrorism policies, the costs of which could be reduced through efficient preventive measures. Statistical models able to account for complex spatio-temporal dependencies have not yet been applied, despite their potential for providing guidance to explain and prevent terrorism. In an effort to address this shortcoming, we employ hierarchical models in a Bayesian context, where the spatial random field is represented by a stochastic partial differential equation. Our results confirm the contagious nature of the lethality of terrorism and the number of lethal terrorist attacks in both space and time. Moreover, the frequency of lethal attacks tends to be higher in richer areas, close to large cities, and within democratic countries. In contrast, attacks are more likely to be lethal far away from large cities, at higher altitudes, in poorer areas, and in locations with higher ethnic diversity. We argue that, on a local scale, the lethality of terrorism and the frequency of lethal attacks are driven by antagonistic mechanisms."
"Grackle: a Chemistry and Cooling Library for Astrophysics We present the Grackle chemistry and cooling library for astrophysical simulations and models. Grackle provides a treatment of non-equilibrium primordial chemistry and cooling for H, D, and He species, including H2 formation on dust grains; tabulated primordial and metal cooling; multiple UV background models; and support for radiation transfer and arbitrary heat sources. The library has an easily implementable interface for simulation codes written in C, C++, and Fortran as well as a Python interface with added convenience functions for semi-analytical models. As an open-source project, Grackle provides a community resource for accessing and disseminating astrochemical data and numerical methods. We present the full details of the core functionality, the simulation and Python interfaces, testing infrastructure, performance, and range of applicability. Grackle is a fully open-source project and new contributions are welcome."
"Interactive (statistical) visualisation and exploration of a billion objects with Vaex With new catalogues arriving such as the Gaia DR1, containing more than a billion objects, new methods of handling and visualizing these data volumes are needed. In visualization, one problem is that the number of datapoints can become so large, that a scatter plot becomes cluttered. Another problem is that with over a billion objects, only a few cpu cycles are available per object if one wants to process them within a second, making traditional methods by rendering glyphs not viable. Instead, we show that by calculating statistics on a regular (N-dimensional) grid, visualizations of a billion objects can be done within a second on a modern desktop computer. This is achieved using memory mapping of hdf5 files together with a simple binning algorithm, which are part of a Python library called vaex. This enables efficient exploration or large datasets interactively, making science exploration of large catalogues feasible. Vaex is a Python library, which also integrates well in the Jupyter/Numpy/Astropy/matplotlib stack. Build on top of this is the vaex application, which allows for interactive exploration and visualization. The motivation for developing vaex is the catalogue of the Gaia satellite, however, vaex can also be used on SPH or N-body simulations, any other (future) catalogues such as SDSS, Pan-STARRS, LSST, WISE, 2MASS, etc. or other tabular data. The homepage for vaex is http://vaex.astro.rug.nl."
"Optimized brute-force algorithms for the bifurcation analysis of a spin-glass-like neural network model Bifurcation theory is a powerful tool for studying how the dynamics of a neural network model depends on its underlying neurophysiological parameters. However, bifurcation theory has been developed mostly for smooth dynamical systems and for continuous-time non-smooth models, which prevents us from understanding the changes of dynamics in some widely used classes of artificial neural network models. This article is an attempt to fill this gap, through the introduction of algorithms that perform a semi-analytical bifurcation analysis of a spin-glass-like neural network model with binary firing rates and discrete-time evolution. Our approach is based on a numerical brute-force search of the stationary and oscillatory solutions of the spin-glass model, from which we derive analytical expressions of its bifurcation structure by means of the state-to-state transition probability matrix. The algorithms determine how the network parameters affect the degree of multistability, the emergence and the period of the neural oscillations, and the formation of symmetry-breaking in the neural populations. While this technique can be applied to networks with arbitrary (generally asymmetric) connectivity matrices, in particular we introduce a highly efficient algorithm for the bifurcation analysis of sparse networks. We also provide some examples of the obtained bifurcation diagrams and a Python implementation of the algorithms."
"Parcels v0.9: prototyping a Lagrangian Ocean Analysis framework for the petascale age As Ocean General Circulation Models (OGCMs) move into the petascale age, where the output from global high-resolution model runs can be of the order of hundreds of terabytes in size, tools to analyse the output of these models will need to scale up too. Lagrangian Ocean Analysis, where virtual particles are tracked through hydrodynamic fields, is an increasingly popular way to analyse OGCM output, by mapping pathways and connectivity of biotic and abiotic particulates. However, the current software stack of Lagrangian Ocean Analysis codes is not dynamic enough to cope with the increasing complexity, scale and need for customisation of use-cases. Furthermore, most community codes are developed for stand-alone use, making it a nontrivial task to integrate virtual particles at runtime of the OGCM. Here, we introduce the new Parcels code, which was designed from the ground up to be sufficiently scalable to cope with petascale computing. We highlight its API design that combines flexibility and customisation with the ability to optimise for HPC workflows, following the paradigm of domain-specific languages. Parcels is primarily written in Python, utilising the wide range of tools available in the scientific Python ecosystem, while generating low-level C-code and using Just-In-Time compilation for performance-critical computation. We show a worked-out example of its API, and validate the accuracy of the code against seven idealised test cases. This version~0.9 of Parcels is focussed on laying out the API, with future work concentrating on optimisation, efficiency and at-runtime coupling with OGCMs."
"ExGUtils: A python package for statistical analysis with the ex-gaussian probability density The study of reaction times and their underlying cognitive processes is an important field in Psychology. Reaction times are usually modeled through the ex-Gaussian distribution, because it provides a good fit to multiple empirical data. The complexity of this distribution makes the use of computational tools an essential element in the field. Therefore, there is a strong need for efficient and versatile computational tools for the research in this area. In this manuscript we discuss some mathematical details of the ex-Gaussian distribution and apply the ExGUtils package, a set of functions and numerical tools, programmed for python, developed for numerical analysis of data involving the ex-Gaussian probability density. In order to validate the package, we present an extensive analysis of fits obtained with it, discuss advantages and differences between the least squares and maximum likelihood methods and quantitatively evaluate the goodness of the obtained fits (which is usually an overlooked point in most literature in the area). The analysis done allows one to identify outliers in the empirical datasets and criteriously determine if there is a need for data trimming and at which points it should be done."
"Bifrost: a Python/C++ Framework for High-Throughput Stream Processing in Astronomy Radio astronomy observatories with high throughput back end instruments require real-time data processing. While computing hardware continues to advance rapidly, development of real-time processing pipelines remains difficult and time-consuming, which can limit scientific productivity. Motivated by this, we have developed Bifrost: an open-source software framework for rapid pipeline development. Bifrost combines a high-level Python interface with highly efficient reconfigurable data transport and a library of computing blocks for CPU and GPU processing. The framework is generalizable, but initially it emphasizes the needs of high-throughput radio astronomy pipelines, such as the ability to process data buffers as if they were continuous streams, the capacity to partition processing into distinct data sequences (e.g., separate observations), and the ability to extract specific intervals from buffered data. Computing blocks in the library are designed for applications such as interferometry, pulsar dedispersion and timing, and transient search pipelines. We describe the design and implementation of the Bifrost framework and demonstrate its use as the backbone in the correlation and beamforming back end of the Long Wavelength Array station in the Sevilleta National Wildlife Refuge, NM."
"Static Graph Challenge: Subgraph Isomorphism The rise of graph analytic systems has created a need for ways to measure and compare the capabilities of these systems. Graph analytics present unique scalability difficulties. The machine learning, high performance computing, and visual analytics communities have wrestled with these difficulties for decades and developed methodologies for creating challenges to move these communities forward. The proposed Subgraph Isomorphism Graph Challenge draws upon prior challenges from machine learning, high performance computing, and visual analytics to create a graph challenge that is reflective of many real-world graph analytics processing systems. The Subgraph Isomorphism Graph Challenge is a holistic specification with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. Subgraph isomorphism is amenable to both vertex-centric implementations and array-based implementations (e.g., using the GraphBLAS.org standard). The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed graph challenge is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance have been measured. Specifications, data, and software are publicly available at GraphChallenge.org."
"Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is demonstrated to efficiently solve eigenvalue problems for graph Laplacians that appear in spectral clustering. For static graph partitioning, 10-20 iterations of LOBPCG without preconditioning result in ~10x error reduction, enough to achieve 100% correctness for all Challenge datasets with known truth partitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7) seconds, compared to over 5,000 (30,000) seconds needed by the baseline Python code. Our Python code 100% correctly determines 98 (160) clusters from the Challenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using 10GB (50GB) of memory. Our single-precision MATLAB code calculates the same clusters at half time and memory. For streaming graph partitioning, LOBPCG is initiated with approximate eigenvectors of the graph Laplacian already computed for the previous graph, in many cases reducing 2-3 times the number of required LOBPCG iterations, compared to the static case. Our spectral clustering is generic, i.e. assuming nothing specific of the block model or streaming, used to generate the graphs for the Challenge, in contrast to the base code. Nevertheless, in 10-stage streaming comparison with the base code for the 5K graph, the quality of our clusters is similar or better starting at stage 4 (7) for emerging edging (snowballing) streaming, while the computations are over 100-1000 faster."
"GALARIO: a GPU Accelerated Library for Analysing Radio Interferometer Observations We present GALARIO, a computational library that exploits the power of modern graphical processing units (GPUs) to accelerate the analysis of observations from radio interferometers like ALMA or the VLA. GALARIO speeds up the computation of synthetic visibilities from a generic 2D model image or a radial brightness profile (for axisymmetric sources). On a GPU, GALARIO is 150 faster than standard Python and 10 times faster than serial C++ code on a CPU. Highly modular, easy to use and to adopt in existing code, GALARIO comes as two compiled libraries, one for Nvidia GPUs and one for multicore CPUs, where both have the same functions with identical interfaces. GALARIO comes with Python bindings but can also be directly used in C or C++. The versatility and the speed of GALARIO open new analysis pathways that otherwise would be prohibitively time consuming, e.g. fitting high resolution observations of large number of objects, or entire spectral cubes of molecular gas emission. It is a general tool that can be applied to any field that uses radio interferometer observations. The source code is available online at https://github.com/mtazzari/galario under the open source GNU Lesser General Public License v3."
"pyLIMA : an open source package for microlensing modeling. I. presentation of the software and analysis on single lens models Microlensing is a unique tool, capable of detecting the 'cold' planets between 1-10 AU from their host stars, and even unbound 'free-floating' planets. This regime has been poorly sampled to date owing to the limitations of alternative planet-finding methods, but a watershed in discoveries is anticipated in the near future thanks to the planned microlensing surveys of WFIRST-AFTA and Euclid s Extended Mission. Of the many challenges inherent in these missions, the modeling of microlensing events will be of primary importance, yet is often time consuming, complex and perceived as a daunting barrier to participation in the field. The large scale of future survey data products will require thorough but efficient modeling software, but unlike other areas of exoplanet research, microlensing currently lacks a publicly-available, well-documented package to conduct this type of analysis. We present first version 1.0 of pyLIMA: Python Lightcurve Identification and Microlensing Analysis. This software is written in Python and uses existing packages as much as possible, to make it widely accessible. In this paper, we describe the overall architecture of the software and the core modules for modeling single-lens events. To verify the performance of this software, we use it to model both real datasets from events published in the literature and generated test data, produced using pyLIMA s simulation module. Results demonstrate that pyLIMA is an efficient tool for microlensing modeling. We will expand pyLIMA to consider more complex phenomena in the following papers."
"Simulating the WFIRST coronagraph Integral Field Spectrograph A primary goal of direct imaging techniques is to spectrally characterize the atmospheres of planets around other stars at extremely high contrast levels. To achieve this goal, coronagraphic instruments have favored integral field spectrographs (IFS) as the science cameras to disperse the entire search area at once and obtain spectra at each location, since the planet position is not known a priori. These spectrographs are useful against confusion from speckles and background objects, and can also help in the speckle subtraction and wavefront control stages of the coronagraphic observation. We present a software package, the Coronagraph and Rapid Imaging Spectrograph in Python (crispy) to simulate the IFS of the WFIRST Coronagraph Instrument (CGI). The software propagates input science cubes using spatially and spectrally resolved coronagraphic focal plane cubes, transforms them into IFS detector maps and ultimately reconstructs the spatio-spectral input scene as a 3D datacube. Simulated IFS cubes can be used to test data extraction techniques, refine sensitivity analyses and carry out design trade studies of the flight CGI-IFS instrument. crispy is a publicly available Python package and can be adapted to other IFS designs."
"SimulaQron - A simulator for developing quantum internet software We introduce a simulator for a quantum internet with the specific goal to support software development. A quantum internet consists of local quantum processors, which are interconnected by quantum communication channels that enable the transmission of qubits between the different processors. While many simulators exist for local quantum processors, there is presently no simulator for a quantum internet tailored towards software development. Quantum internet protocols require both classical as well as quantum information to be exchanged between the network nodes, next to the execution of gates and measurements on a local quantum processor. This requires quantum internet software to integrate classical communication programming practises with novel quantum ones.   SimulaQron is built to enable application development and explore software engineering practises for a quantum internet. SimulaQron can be run on one or more classical computers to simulate local quantum processors, which are transparently connected in the background to enable the transmission of qubits or the generation of entanglement between remote processors. Application software can access the simulated local quantum processors to execute local quantum instructions and measurements, but also to transmit qubits to remote nodes in the network. SimulaQron features a modular design that performs a distributed simulation based on any existing simulation of a quantum computer capable of integrating with Python. Programming libraries for Python and C are provided to facilitate application development."
"Basic concepts and tools for the Toki Pona minimal and constructed language: description of the language and main issues; analysis of the vocabulary; text synthesis and syntax highlighting; Wordnet synsets A minimal constructed language (conlang) is useful for experiments and comfortable for making tools. The Toki Pona (TP) conlang is minimal both in the vocabulary (with only 14 letters and 124 lemmas) and in the (about) 10 syntax rules. The language is useful for being a used and somewhat established minimal conlang with at least hundreds of fluent speakers. This article exposes current concepts and resources for TP, and makes available Python (and Vim) scripted routines for the analysis of the language, synthesis of texts, syntax highlighting schemes, and the achievement of a preliminary TP Wordnet. Focus is on the analysis of the basic vocabulary, as corpus analyses were found. The synthesis is based on sentence templates, relates to context by keeping track of used words, and renders larger texts by using a fixed number of phonemes (e.g. for poems) and number of sentences, words and letters (e.g. for paragraphs). Syntax highlighting reflects morphosyntactic classes given in the official dictionary and different solutions are described and implemented in the well-established Vim text editor. The tentative TP Wordnet is made available in three patterns of relations between synsets and word lemmas. In summary, this text holds potentially novel conceptualizations about, and tools and results in analyzing, synthesizing and syntax highlighting the TP language."
"astroplan: An Open Source Observation Planning Package in Python We present astroplan - an open source, open development, Astropy affiliated package for ground-based observation planning and scheduling in Python. astroplan is designed to provide efficient access to common observational quantities such as celestial rise, set, and meridian transit times and simple transformations from sky coordinates to altitude-azimuth coordinates without requiring a detailed understanding of astropy's implementation of coordinate systems. astroplan provides convenience functions to generate common observational plots such as airmass and parallactic angle as a function of time, along with basic sky (finder) charts. Users can determine whether or not a target is observable given a variety of observing constraints, such as airmass limits, time ranges, Moon illumination/separation ranges, and more. A selection of observation schedulers are included which divide observing time among a list of targets, given observing constraints on those targets. Contributions to the source code from the community are welcome."
"Tangos: the agile numerical galaxy organization system We present Tangos, a Python framework and web interface for database-driven analysis of numerical structure formation simulations. To understand the role that such a tool can play, consider constructing a history for the absolute magnitude of each galaxy within a simulation. The magnitudes must first be calculated for all halos at all timesteps and then linked using a merger tree; folding the required information into a final analysis can entail significant effort. Tangos is a generic solution to this information organization problem, aiming to free users from the details of data management. At the querying stage, our example of gathering properties over history is reduced to a few clicks or a simple, single-line Python command. The framework is highly extensible; in particular, users are expected to define their own properties which tangos will write into the database. A variety of parallelization options are available and the raw simulation data can be read using existing libraries such as pynbody or yt. Finally, tangos-based databases and analysis pipelines can easily be shared with collaborators or the broader community to ensure reproducibility. User documentation is provided separately."
"CANA: A python package for quantifying control and canalization in Boolean Networks Logical models offer a simple but powerful means to understand the complex dynamics of biochemical regulation, without the need to estimate kinetic parameters. However, even simple automata components can lead to collective dynamics that are computationally intractable when aggregated into networks. In previous work we demonstrated that automata network models of biochemical regulation are highly canalizing, whereby many variable states and their groupings are redundant (Marques-Pita and Rocha, 2013). The precise charting and measurement of such canalization simplifies these models, making even very large networks amenable to analysis. Moreover, canalization plays an important role in the control, robustness, modularity and criticality of Boolean network dynamics, especially those used to model biochemical regulation (Gates and Rocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new publicly-available Python package that provides the necessary tools to extract, measure, and visualize canalizing redundancy present in Boolean network models. It extracts the pathways most effective in controlling dynamics in these models, including their effective graph and dynamics canalizing map, as well as other tools to uncover minimum sets of control variables."
"IDEL: In-Database Entity Linking with Neural Embeddings We present a novel architecture, In-Database Entity Linking (IDEL), in which we integrate the analytics-optimized RDBMS MonetDB with neural text mining abilities. Our system design abstracts core tasks of most neural entity linking systems for MonetDB. To the best of our knowledge, this is the first defacto implemented system integrating entity-linking in a database. We leverage the ability of MonetDB to support in-database-analytics with user defined functions (UDFs) implemented in Python. These functions call machine learning libraries for neural text mining, such as TensorFlow. The system achieves zero cost for data shipping and transformation by utilizing MonetDB's ability to embed Python processes in the database kernel and exchange data in NumPy arrays. IDEL represents text and relational data in a joint vector space with neural embeddings and can compensate errors with ambiguous entity representations. For detecting matching entities, we propose a novel similarity function based on joint neural embeddings which are learned via minimizing pairwise contrastive ranking loss. This function utilizes a high dimensional index structures for fast retrieval of matching entities. Our first implementation and experiments using the WebNLG corpus show the effectiveness and the potentials of IDEL."
"A high-bias, low-variance introduction to Machine Learning for physicists Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute. (Notebooks are available at https://physics.bu.edu/~pankajm/MLnotebooks.html )"
"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of ~148K Python and ~120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language."
"Deploying Customized Data Representation and Approximate Computing in Machine Learning Applications Major advancements in building general-purpose and customized hardware have been one of the key enablers of versatility and pervasiveness of machine learning models such as deep neural networks. To sustain this ubiquitous deployment of machine learning models and cope with their computational and storage complexity, several solutions such as low-precision representation of model parameters using fixed-point representation and deploying approximate arithmetic operations have been employed. Studying the potency of such solutions in different applications requires integrating them into existing machine learning frameworks for high-level simulations as well as implementing them in hardware to analyze their effects on power/energy dissipation, throughput, and chip area. Lop is a library for design space exploration that bridges the gap between machine learning and efficient hardware realization. It comprises a Python module, which can be integrated with some of the existing machine learning frameworks and implements various customizable data representations including fixed-point and floating-point as well as approximate arithmetic operations.Furthermore, it includes a highly-parameterized Scala module, which allows synthesizing hardware based on the said data representations and arithmetic operations. Lop allows researchers and designers to quickly compare quality of their models using various data representations and arithmetic operations in Python and contrast the hardware cost of viable representations by synthesizing them on their target platforms (e.g., FPGA or ASIC). To the best of our knowledge, Lop is the first library that allows both software simulation and hardware realization using customized data representations and approximate computing techniques."
"BindsNET: A machine learning-oriented spiking neural networks library in Python The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared towards machine learning and reinforcement learning. Our software, called BindsNET, enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on top of the PyTorch deep neural networks library, enabling fast CPU and GPU computation for large spiking networks. The BindsNET framework can be adjusted to meet the needs of other existing computing and hardware environments, e.g., TensorFlow. We also provide an interface into the OpenAI gym library, allowing for training and evaluation of spiking networks on reinforcement learning problems. We argue that this package facilitates the use of spiking networks for large-scale machine learning experimentation, and show some simple examples of how we envision BindsNET can be used in practice. BindsNET code is available at https://github.com/Hananel-Hazan/bindsnet"
"Deductron -- A Recurrent Neural Network The current paper is a study in Recurrent Neural Networks (RNN), motivated by the lack of examples simple enough so that they can be thoroughly understood theoretically, but complex enough to be realistic. We constructed an example of structured data, motivated by problems from image-to-text conversion (OCR), which requires long-term memory to decode. Our data is a simple writing system, encoding characters 'X' and 'O' as their upper halves, which is possible due to symmetry of the two characters. The characters can be connected, as in some languages using cursive, such as Arabic (abjad). The string 'XOOXXO' may be encoded as '${\vee}{\wedge}\kern-1.5pt{\wedge}{\vee}\kern-1.5pt{\vee}{\wedge}$'. It follows that we may need to know arbitrarily long past to decode a current character, thus requiring long-term memory. Subsequently we constructed an RNN capable of decoding sequences encoded in this manner. Rather than by training, we constructed our RNN ""by inspection"", i.e. we guessed its weights. This involved a sequence of steps. We wrote a conventional program which decodes the sequences as the example above. Subsequently, we interpreted the program as a neural network (the only example of this kind known to us). Finally, we generalized this neural network to discover a new RNN architecture whose instance is our handcrafted RNN. It turns out to be a 3 layer network, where the middle layer is capable of performing simple logical inferences; thus the name ""deductron"". It is demonstrated that it is possible to train our network by simulated annealing. Also, known variants of stochastic gradient descent (SGD) methods are shown to work."
"pwv_kpno: A Python Package for Modeling the Atmospheric Transmission Function due to Precipitable Water Vapor We present a Python package, pwv_kpno, that provides models for the atmospheric transmission due to precipitable water vapor (PWV) at user specified sites. Using the package, ground-based photometric observations taken between $3,000$ and $12,000$ $\AA$ can be corrected for atmospheric effects due to PWV. Atmospheric transmission in the optical and near-infrared is highly dependent on the PWV column density along the line of sight. By measuring the delay of dual-band GPS signals through the atmosphere, the SuomiNet project provides accurate PWV measurements for hundreds of locations around the world. The pwv_kpno package uses published SuomiNet data in conjunction with MODTRAN models to determine the modeled, time-dependent atmospheric transmission. A dual-band GPS system was installed at Kitt Peak National Observatory (KPNO) in the spring of 2015. Using measurements from this receiver we demonstrate that we can successfully predict the PWV at KPNO from nearby dual-band GPS stations on the surrounding desert floor. The pwv_kpno package can thus provide atmospheric transmission functions for observations taken before the KPNO receiver was installed. Using PWV measurements from the desert floor, we correctly model PWV absorption features present in spectra taken at KPNO. We also demonstrate how to configure the package for use at other observatories."
"RadFil: a Python Package for Building and Fitting Radial Profiles for Interstellar Filaments We present RadFil, a publicly available Python package that gives users full control over how to build and fit radial density profiles for interstellar filaments. RadFil builds filament profiles by taking radial cuts across the spine of a filament, thereby preserving the radial structure of the filament across its entire length. Pre-existing spines can be inputted directly into RadFil, or can be computed using the FilFinder package as part of the RadFil workflow. We provide Gaussian and Plummer built-in fitting functions, in addition to a background subtraction estimator, which can be fit to the entire ensemble of radial cuts or an average radial profile for the filament. Users can tweak parameters like the radial cut sampling interval, the background subtraction estimation radii, and the Gaussian/Plummer fitting radii. As a result, RadFil can provide treatment of how the resulting filament properties rely on systematics in the building and fitting process. We walk through the typical RadFil workflow and compare our results to those from an independent radial density profile code obtained using the same data; we find that our results are entirely consistent. RadFil is open source and available on GitHub. We also provide a complete working tutorial of the code available as a Jupyter notebook which users can download and run themselves."
"An Open Framework Enabling Electromagnetic Tracking in Image-Guided Interventions Electromagnetic tracking (EMT) is a core platform technology in the navigation and visualisation of image-guided procedures. The technology provides high tracking accuracy in non-line-of-sight environments, allowing instrument navigation in locations where optical tracking is not feasible. Integration of EMT in complex procedures, often coupled with multi-modal imaging, is on the rise, yet the lack of exibility in the available hardware platforms has been noted by many researchers and system designers. Advances in the field of EMT include novel methods of improving tracking system accuracy, precision and error compensation capabilities, though such system-level improvements cannot be readily incorporated in current therapy applications due to the `blackbox' nature of commercial tracking solving algorithms. This paper defines a software framework to allow novel EMT designs and improvements become part of the global design process for image-guided interventions. In an effort to standardise EMT development, we define a generalised cross-platform software framework in terms of the four system functions common to all EMT systems; acquisition, filtering, modelling and solving. The interfaces between each software component are defined in terms of their input and output data structures. An exemplary framework is implemented in the Python programming language and demonstrated with the open-source Anser EMT system. Performance metrics are gathered from both Matlab and Python implementations of Anser EMT considering the host operating system, hardware configuration and acquisition settings used. Results show indicative system latencies of 5 ms can be achieved using the framework on a Windows operating system, with decreased system performance observed on UNIX-like platforms."
"PyDraw: a GUI drawing generator based on Tkinter and its design concept The emergence of GUI is a great progress in the history of computer science and software design. GUI makes human computer interaction more simple and interesting. Python, as a popular programming language in recent years, has not been realized in GUI design. Tkinter has the advantage of native support for Python, but there are too few visual GUI generators supporting Tkinter. This article presents a GUI generator based on Tkinter framework, PyDraw. The design principle of PyDraw and the powerful design concept behind it are introduced in detail. With PyDraw's GUI design philosophy, it can easily design a visual GUI rendering generator for any GUI framework with canvas functionality or programming language with screen display control. This article is committed to conveying PyDraw's GUI free design concept. Through experiments, we have proved the practicability and efficiency of PyDrawd. In order to better convey the design concept of PyDraw, let more enthusiasts join PyDraw update and evolution, we have the source code of PyDraw. At the end of the article, we summarize our experience and express our vision for future GUI design. We believe that the future GUI will play an important role in graphical software programming, the future of less code or even no code programming software design methods must become a focus and hot, free, like drawing GUI will be worth pursuing."
"High-accuracy mass, spin, and recoil predictions of generic black-hole merger remnants We present accurate fits for the remnant properties of generically precessing binary black holes, trained on large banks of numerical-relativity simulations. We use Gaussian process regression to interpolate the remnant mass, spin, and recoil velocity in the 7-dimensional parameter space of precessing black-hole binaries with mass ratios $q\leq2$, and spin magnitudes $\chi_1,\chi_2\leq0.8$. For precessing systems, our errors in estimating the remnant mass, spin magnitude, and kick magnitude are lower than those of existing fitting formulae by at least an order of magnitude (improvement is also reported in the extrapolated region at high mass ratios and spins). In addition, we also model the remnant spin and kick directions. Being trained directly on precessing simulations, our fits are free from ambiguities regarding the initial frequency at which precessing quantities are defined. We also construct a model for remnant properties of aligned-spin systems with mass ratios $q\leq8$, and spin magnitudes $\chi_1,\chi_2\leq0.8$. As a byproduct, we also provide error estimates for all fitted quantities, which can be consistently incorporated into current and future gravitational-wave parameter-estimation analyses. Our model(s) are made publicly available through a fast and easy-to-use Python module called surfinBH."
"Reference environments: A universal tool for reproducibility in computational biology The drive for reproducibility in the computational sciences has provoked discussion and effort across a broad range of perspectives: technological, legislative/policy, education, and publishing. Discussion on these topics is not new, but the need to adopt standards for reproducibility of claims made based on computational results is now clear to researchers, publishers and policymakers alike. Many technologies exist to support and promote reproduction of computational results: containerisation tools like Docker, literate programming approaches such as Sweave, knitr, iPython or cloud environments like Amazon Web Services. But these technologies are tied to specific programming languages (e.g. Sweave/knitr to R; iPython to Python) or to platforms (e.g. Docker for 64-bit Linux environments only). To date, no single approach is able to span the broad range of technologies and platforms represented in computational biology and biotechnology.   To enable reproducibility across computational biology, we demonstrate an approach and provide a set of tools that is suitable for all computational work and is not tied to a particular programming language or platform. We present published examples from a series of papers in different areas of computational biology, spanning the major languages and technologies in the field (Python/R/MATLAB/Fortran/C/Java). Our approach produces a transparent and flexible process for replication and recomputation of results. Ultimately, its most valuable aspect is the decoupling of methods in computational biology from their implementation. Separating the 'how' (method) of a publication from the 'where' (implementation) promotes genuinely open science and benefits the scientific community as a whole."
"An Initial Step Towards Organ Transplantation Based on GitHub Repository Organ transplantation, which is the utilization of codes directly related to some specific functionalities to complete ones own program, provides more convenience for developers than traditional component reuse. However, recent techniques are challenged with the lack of organs for transplantation. Hence, we conduct an empirical study on extracting organs from GitHub repository to explore transplantation based on large-scale dataset. We analyze statistics from 12 representative GitHub projects and get the conclusion that 1) there are abundant practical organs existing in commits with add as a key word in the comments; 2) organs in this repository mainly possess four kinds of contents; 3) approximately 70% of the organs are easy-to-transplant. Implementing our transplantation strategy for different kinds of organs, we manually extract 30 organs in three different programming languages, namely Java, Python, and C, and make unit tests for them utilizing four testing tools (two for Java, one for Python, and one for C). At last, we transplant three Java organs into a specific platform for a performance check to verify whether they can work well in the new system. All the 30 organs extracted by our strategy possess good performances in unit test with the highest passing rate reaching 97% and the lowest one still passing 80% and the three Java organs work well in the new system, providing three new functionalities for the host. All the results indicate the feasibility of organ transplantation based on open-source repository, bringing new idea for code reuse."
"flavio: a Python package for flavour and precision phenomenology in the Standard Model and beyond flavio is an open source tool for phenomenological analyses in flavour physics and other precision observables in the Standard Model and beyond. It consists of a library to compute predictions for a plethora of observables in quark and lepton flavour physics and electroweak precision tests, a database of experimental measurements of these observables, a statistics package that allows to construct Bayesian and frequentist likelihoods, and of convenient plotting and visualization routines. New physics effects are parameterised as Wilson coefficients of dimension-six operators in the weak effective theory below the electroweak scale or the Standard Model EFT above it. At present, observables implemented include numerous rare $B$ decays (including angular observables of exclusive decays, lepton flavour and lepton universality violating $B$ decays), meson-antimeson mixing observables in the $B_{d,s}$, $K$, and $D$ systems, tree-level semi-leptonic $B$, $K$, and $D$ decays (including possible lepton universality violation), rare $K$ decays, lepton flavour violating $\tau$ and $\mu$ decays, $Z$ pole electroweak precision observables, the neutron electric dipole moment, and anomalous magnetic moments of leptons. Not only central values but also theory uncertainties of all observables can be computed. Input parameters and their uncertainties can be easily modified by the user. Written in Python, the code does not require compilation and can be run in an interactive session. This document gives an overview of the features as of version 1.0 but does not represent a manual. The full documentation of the code can be found in its web site."
"gpuRIR: A Python Library for Room Impulse Response Simulation with GPU Acceleration The Image Source Method (ISM) is one of the most employed techniques to calculate acoustic Room Impulse Responses (RIRs), however, its computational complexity grows fast with the reverberation time of the room and its computation time can be prohibitive for some applications where a huge number of RIRs are needed. In this paper, we present a new implementation that dramatically improves the computation speed of the ISM by using Graphic Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and the computation of the images inside each RIR. Additional speedups were achieved by exploiting the mixed precision capabilities of the newer GPUs and by using lookup tables. We provide a Python library under GNU license that can be easily used without any knowledge about GPU programming and we show that it is about 100 times faster than other state of the art CPU libraries. It may become a powerful tool for many applications that need to perform a large number of acoustic simulations, such as training machine learning systems for audio signal processing, or for real-time room acoustics simulations for immersive multimedia systems, such as augmented or virtual reality."
"Stationary-State Statistics of a Binary Neural Network Model with Quenched Disorder We study the statistical properties of the stationary firing-rate states of a neural network model with quenched disorder. The model has arbitrary size, discrete-time evolution equations and binary firing rates, while the topology and the strength of the synaptic connections are randomly generated from known, generally arbitrary, probability distributions. We derived semi-analytical expressions of the occurrence probability of the stationary states and the mean multistability diagram of the model, in terms of the distribution of the synaptic connections and of the external stimuli to the network. Our calculations rely on the probability distribution of the bifurcation points of the stationary states with respect to the external stimuli, which can be calculated in terms of the permanent of special matrices, according to extreme value theory. While our semi-analytical expressions are exact for any size of the network and for any distribution of the synaptic connections, we also specialized our calculations to the case of statistically-homogeneous multi-population networks. In the specific case of this network topology, we calculated analytically the permanent, obtaining a compact formula that outperforms of several orders of magnitude the Balasubramanian-Bax-Franklin-Glynn algorithm. To conclude, by applying the Fisher-Tippett-Gnedenko theorem, we derived asymptotic expressions of the stationary-state statistics of multi-population networks in the large-network-size limit, in terms of the Gumbel (double exponential) distribution. We also provide a Python implementation of our formulas and some examples of the results generated by the code."
"ATM: An Open-Source Tool for Asteroid Thermal Modeling We publicly release ATM, a Python package designed to model asteroid flux measurements to estimate an asteroid's size, surface temperature distribution, and emissivity. The full multi-dimensional posterior pdf is found using Markov Chain Monte Carlo. Data files with $\sim$ 2.5 million WISE flux measurements for $\sim$ 150,000 asteroids and additional MPC data are also included with the package, as well as Python Jupyter Notebooks with examples of analysis. The entirety of the analysis presented here, including all the figures, tables, and catalogs, can be easily reproduced with these publicly released Notebooks. We show that ATM can match the best-fit size estimates for well-observed asteroids published in 2016 by the NEOWISE team (Mainzer et al. 2016) with a sub-percent bias and a scatter of only 6%. We estimate that the accuracy of WISE-based asteroid size estimates is approximately in the range of 15-20% for most objects. We also study optical data collected by the Sloan Digital Sky Survey (SDSS) and show that correlations of optical colors and WISE-based best-fit model parameters indicate robustness of the latter. Our analysis also gives support to the claim by Harris & Drube (2014) that candidate metallic asteroids can be selected using the best-fit temperature parameter and infrared albedo. We investigate a correlation between SDSS colors and optical albedo derived using WISE-based size estimates and show that this correlation can be used to estimate asteroid sizes with optical data alone, with a precision of about 21% relative to WISE-based size estimates. After accounting for systematic errors, the difference in accuracy between infrared and optical color-based size estimates becomes less than a factor of two. (abridged)"
"Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage."
"Wotan: Comprehensive time-series de-trending in Python The detection of transiting exoplanets in time-series photometry requires the removal or modeling of instrumental and stellar noise. While instrumental systematics can be reduced using methods such as pixel level decorrelation, removing stellar trends while preserving transit signals proves challenging. Due to vast archives of light curves from recent transit surveys, there is a strong need for accurate automatic detrending, without human intervention. A large variety of detrending algorithms are in active use, but their comparative performance for transit discovery is unexplored. We benchmark all commonly used detrending methods against hundreds of Kepler, K2, and TESS planets, selected to represent the most difficult cases for systems with small planet-to-star radius ratios. The full parameter range is explored for each method to determine the best choices for planet discovery. We conclude that the ideal method is a time-windowed slider with an iterative robust location estimator based on Tukey's biweight. This method recovers 99% and 94% of the shallowest Kepler and K2 planets, respectively. We include an additional analysis for young stars with extreme variability and conclude they are best treated using a spline-based method with a robust Huber estimator. All stellar detrending methods explored are available for public use in wotan, an open-source Python package on GitHub (see https://github.com/hippke/wotan)."
"freud: A Software Suite for High Throughput Analysis of Particle Simulation Data The freud Python package is a powerful library for analyzing simulation data. Written with modern simulation and data analysis workflows in mind, freud provides a Python interface to fast, parallelized C++ routines that run efficiently on laptops, workstations, and supercomputing clusters. The package provides the core tools for finding particle neighbors in periodic systems, and offers a uniform API to a wide variety of methods implemented using these tools. As such, freud users can access standard methods such as the radial distribution function as well as newer, more specialized methods such as the potential of mean force and torque and local crystal environment analysis with equal ease. While many comparable tools place a heavy emphasis on reading and operating on trajectory file formats, freud instead accepts numerical arrays of data directly as inputs. By remaining agnostic to its data source, freud is suitable for analyzing any coarse-grained particle simulation, regardless of the original data representation or simulation method. When used for on-the-fly analysis in conjunction with scriptable simulation software such as HOOMD-blue, freud enables smart simulations that adapt to the current state of the system, allowing users to study phenomena such as nucleation and growth."
"PyProcar: A Python library for electronic structure pre/post-processing The PyProcar Python package plots the band structure and the Fermi surface as a function of site and/or s,p,d,f - projected wavefunctions obtained for each $k$-point in the Brillouin zone and band in an electronic structure calculation. This can be performed on top of any electronic structure code, as long as the band and projection information is written in the PROCAR format, as done by the VASP and ABINIT codes. PyProcar can be easily modified to read other formats as well. This package is particularly suitable for understanding atomic effects into the band structure, Fermi surface, spin texture, etc. PyProcar can be conveniently used in a command line mode, where each one of the parameters define a plot property. In the case of Fermi-surfaces, the package is able to plot the surface with colors depending on other properties such as the electron velocity or spin projection. The mesh used to calculate the property does not need to be the same as the one used to obtain the Fermi surface. A file with a specific property evaluated for each $k$-point in a $k-$mesh and for each band can be used to project other properties such as electron-phonon mean path, Fermi velocity, electron effective mass, etc. Another existing feature refers to the band unfolding of supercell calculations into predefined unit cells."
"deepCR: Cosmic Ray Rejection with Deep Learning Cosmic ray (CR) identification and replacement are critical components of imaging and spectroscopic reduction pipelines involving solid-state detectors. We present deepCR, a deep learning based framework for CR identification and subsequent image inpainting based on the predicted CR mask. To demonstrate the effectiveness of this framework, we train and evaluate models on Hubble Space Telescope ACS/WFC images of sparse extragalactic fields, globular clusters, and resolved galaxies. We demonstrate that at a false positive rate of 0.5%, deepCR achieves close to 100% detection rates in both extragalactic and globular cluster fields, and 91% in resolved galaxy fields, which is a significant improvement over the current state-of-the-art method LACosmic. Compared to a multicore CPU implementation of LACosmic, deepCR CR mask predictions run up to 6.5 times faster on CPU and 90 times faster on a single GPU. For image inpainting, the mean squared errors of deepCR predictions are 20 times lower in globular cluster fields, 5 times lower in resolved galaxy fields, and 2.5 times lower in extragalactic fields, compared to the best performing non-neural technique tested. We present our framework and the trained models as an open-source Python project, with a simple-to-use API. To facilitate reproducibility of the results we also provide a benchmarking codebase."
"On Using Machine Learning to Identify Knowledge in API Reference Documentation Using API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) and deep learning approaches trained on manually annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87%. The deep learning and SVM classifiers seem complementary. For four knowledge types (Concept, Control, Pattern, and Non-Information), SVM clearly outperforms deep learning which, on the other hand, is more accurate for identifying the remaining types. When considering multiple knowledge types at once (i.e., multi-label classification) deep learning outperforms na\""ive baselines and traditional machine learning achieving a MacroAUC up to 79%. We also compared classifiers using embeddings pre-trained on generic text corpora and StackOverflow but did not observe significant improvements. Finally, to assess the generalizability of the classifiers, we re-tested them on a different, unseen Python documentation dataset. Classifiers for Functionality, Concept, Purpose, Pattern, and Directive seem to generalize from Java and .NET to Python documentation. The accuracy related to the remaining types seems API-specific. We discuss our results and how they inform the development of tools for supporting developers sharing and accessing API knowledge. Published article: https://doi.org/10.1145/3338906.3338943"
"A performance comparison of Dask and Apache Spark for data-intensive neuroimaging pipelines In the past few years, neuroimaging has entered the Big Data era due to the joint increase in image resolution, data sharing, and study sizes. However, no particular Big Data engines have emerged in this field, and several alternatives remain available. We compare two popular Big Data engines with Python APIs, Apache Spark and Dask, for their runtime performance in processing neuroimaging pipelines. Our evaluation uses two synthetic pipelines processing the 81GB BigBrain image, and a real pipeline processing anatomical data from more than 1,000 subjects. We benchmark these pipelines using various combinations of task durations, data sizes, and numbers of workers, deployed on an 8-node (8 cores ea.) compute cluster in Compute Canada's Arbutus cloud. We evaluate PySpark's RDD API against Dask's Bag, Delayed and Futures. Results show that despite slight differences between Spark and Dask, both engines perform comparably. However, Dask pipelines risk being limited by Python's GIL depending on task type and cluster configuration. In all cases, the major limiting factor was data transfer. While either engine is suitable for neuroimaging pipelines, more effort needs to be placed in reducing data transfer time."
"R-miss-tastic: a unified platform for missing values methods and workflows Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss, or biased analyses. Since the seminal work of Rubin (1976), there has been a burgeoning literature on missing values with heterogeneous aims and motivations. This has resulted in the development of various methods, formalizations, and tools (including a large number of R packages and Python modules). However, for practitioners, it remains challenging to decide which method is most suited for their problem, partially because handling missing data is still not a topic systematically covered in statistics or data science curricula.   To help address this challenge, we have launched a unified platform: ""R-miss-tastic"", which aims to provide an overview of standard missing values problems, methods, how to handle them in analyses, and relevant implementations of methodologies. In the same perspective, we have also developed several pipelines in R and Python to allow for a hands-on illustration of how to handle missing values in various statistical tasks such as estimation and prediction, while ensuring reproducibility of the analyses. This will hopefully also provide some guidance on deciding which method to choose for a specific problem and data. The objective of this work is not only to comprehensively organize materials, but also to create standardized analysis workflows, and to provide a common ground for discussions among the community. This platform is thus suited for beginners, students, more advanced analysts and researchers."
"ProSper -- A Python Library for Probabilistic Sparse Coding with Non-Standard Priors and Superpositions ProSper is a python library containing probabilistic algorithms to learn dictionaries. Given a set of data points, the implemented algorithms seek to learn the elementary components that have generated the data. The library widens the scope of dictionary learning approaches beyond implementations of standard approaches such as ICA, NMF or standard L1 sparse coding. The implemented algorithms are especially well-suited in cases when data consist of components that combine non-linearly and/or for data requiring flexible prior distributions. Furthermore, the implemented algorithms go beyond standard approaches by inferring prior and noise parameters of the data, and they provide rich a-posteriori approximations for inference. The library is designed to be extendable and it currently includes: Binary Sparse Coding (BSC), Ternary Sparse Coding (TSC), Discrete Sparse Coding (DSC), Maximal Causes Analysis (MCA), Maximum Magnitude Causes Analysis (MMCA), and Gaussian Sparse Coding (GSC, a recent spike-and-slab sparse coding approach). The algorithms are scalable due to a combination of variational approximations and parallelization. Implementations of all algorithms allow for parallel execution on multiple CPUs and multiple machines for medium to large-scale applications. Typical large-scale runs of the algorithms can use hundreds of CPUs to learn hundreds of dictionary elements from data with tens of millions of floating-point numbers such that models with several hundred thousand parameters can be optimized. The library is designed to have minimal dependencies and to be easy to use. It targets users of dictionary learning algorithms and Machine Learning researchers."
"Rethinking travel behavior modeling representations through embeddings This paper introduces the concept of travel behavior embeddings, a method for re-representing discrete variables that are typically used in travel demand modeling, such as mode, trip purpose, education level, family type or occupation. This re-representation process essentially maps those variables into a latent space called the \emph{embedding space}. The benefit of this is that such spaces allow for richer nuances than the typical transformations used in categorical variables (e.g. dummy encoding, contrasted encoding, principal components analysis). While the usage of latent variable representations is not new per se in travel demand modeling, the idea presented here brings several innovations: it is an entirely data driven algorithm; it is informative and consistent, since the latent space can be visualized and interpreted based on distances between different categories; it preserves interpretability of coefficients, despite being based on Neural Network principles; and it is transferrable, in that embeddings learned from one dataset can be reused for other ones, as long as travel behavior keeps consistent between the datasets.   The idea is strongly inspired on natural language processing techniques, namely the word2vec algorithm. Such algorithm is behind recent developments such as in automatic translation or next word prediction. Our method is demonstrated using a model choice model, and shows improvements of up to 60\% with respect to initial likelihood, and up to 20% with respect to likelihood of the corresponding traditional model (i.e. using dummy variables) in out-of-sample evaluation. We provide a new Python package, called PyTre (PYthon TRavel Embeddings), that others can straightforwardly use to replicate our results or improve their own models. Our experiments are themselves based on an open dataset (swissmetro)."
"V2: Fast Detection of Configuration Drift in Python Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.   We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift."
"Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive HPC Workflows We introduce the Balsam service to manage high-throughput task scheduling and execution on supercomputing systems. Balsam allows users to populate a task database with a variety of tasks ranging from simple independent tasks to dynamic multi-task workflows. With abstractions for the local resource scheduler and MPI environment, Balsam dynamically packages tasks into ensemble jobs and manages their scheduling lifecycle. The ensembles execute in a pilot ""launcher"" which (i) ensures concurrent, load-balanced execution of arbitrary serial and parallel programs with heterogeneous processor requirements, (ii) requires no modification of user applications, (iii) is tolerant of task-level faults and provides several options for error recovery, (iv) stores provenance data (e.g task history, error logs) in the database, (v) supports dynamic workflows, in which tasks are created or killed at runtime. Here, we present the design and Python implementation of the Balsam service and launcher. The efficacy of this system is illustrated using two case studies: hyperparameter optimization of deep neural networks, and high-throughput single-point quantum chemistry calculations. We find that the unique combination of flexible job-packing and automated scheduling with dynamic (pilot-managed) execution facilitates excellent resource utilization. The scripting overheads typically needed to manage resources and launch workflows on supercomputers are substantially reduced, accelerating workflow development and execution."
"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in Spatiotemporal Systems Extracting actionable insight from complex unlabeled scientific data is an open challenge and key to unlocking data-driven discovery in science. Complementary and alternative to supervised machine learning approaches, unsupervised physics-based methods based on behavior-driven theories hold great promise. Due to computational limitations, practical application on real-world domain science problems has lagged far behind theoretical development. We present our first step towards bridging this divide - DisCo - a high-performance distributed workflow for the behavior-driven local causal state theory. DisCo provides a scalable unsupervised physics-based representation learning method that decomposes spatiotemporal systems into their structurally relevant components, which are captured by the latent local causal state variables. Complex spatiotemporal systems are generally highly structured and organize around a lower-dimensional skeleton of coherent structures, and in several firsts we demonstrate the efficacy of DisCo in capturing such structures from observational and simulated scientific data. To the best of our knowledge, DisCo is also the first application software developed entirely in Python to scale to over 1000 machine nodes, providing good performance along with ensuring domain scientists' productivity. We developed scalable, performant methods optimized for Intel many-core processors that will be upstreamed to open-source Python library packages. Our capstone experiment, using newly developed DisCo workflow and libraries, performs unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data, processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64% strong-scaling efficiency."
"Modernizing Titan2D, a Parallel AMR Geophysical Flow Code to Support Multiple Rheologies and Extendability In this work, we report on strategies and results of our initial approach for modernization of Titan2D code. Titan2D is a geophysical mass flow simulation code designed for modeling of volcanic flows, debris avalanches and landslides over a realistic terrain model. It solves an underlying hyperbolic system of partial differential equations using parallel adaptive mesh Godunov scheme. The following work was done during code refactoring and modernization. To facilitate user input two level python interface was developed. Such design permits large changes in C++ and Python low-level while maintaining stable high-level interface exposed to the end user. Multiple diverged forks implementing different material models were merged back together. Data storage layout was changed from a linked list of structures to a structure of arrays representation for better memory access and in preparation for further work on better utilization of vectorized instruction. Existing MPI parallelization was augmented with OpenMP parallelization. The performance of a hash table used to store mesh elements and nodes references was improved by switching from a linked list for overflow entries to dynamic arrays allowing the implementation of the binary search algorithm. The introduction of the new data layout made possible to reduce the number of hash table look-ups by replacing them with direct use of indexes from the storage class. The modifications lead to 8-9 times performance improvement for serial execution."
"Distributed-Memory Tensor Completion for Generalized Loss Functions in Python using New Sparse Tensor Kernels Tensor computations are increasingly prevalent numerical techniques in data science, but pose unique challenges for high-performance implementation. We provide novel algorithms and systems infrastructure which enable efficient parallel implementation of algorithms for tensor completion with generalized loss functions. Specifically, we consider alternating minimization, coordinate minimization, and a quasi-Newton (generalized Gauss-Newton) method. By extending the Cyclops library, we implement all of these methods in high-level Python syntax. To make possible tensor completion for very sparse tensors, we introduce new multi-tensor primitives, for which we provide specialized parallel implementations. We compare these routines to pairwise contraction of sparse tensors by reduction to hypersparse matrix formats, and find that the multi-tensor routines are more efficient in theoretical cost and execution time in experiments. We provide microbenchmarking results on the Stampede2 supercomputer to demonstrate the efficiency of the new primitives and Cyclops functionality. We then study the performance of the tensor completion methods for a synthetic tensor with 10 billion nonzeros and the Netflix dataset, considering both least squares and Poisson loss functions."
"PiNN: A Python Library for Building Atomic Neural Networks of Molecules and Materials Atomic neural networks (ANNs) constitute a class of machine learning methods for predicting potential energy surfaces and physico-chemical properties of molecules and materials. Despite many successes, developing interpretable ANN architectures and implementing existing ones efficiently are still challenging. This calls for reliable, general-purpose and open-source codes. Here, we present a python library named PiNN as a solution toward this goal. In PiNN, we designed a new interpretable and high-performing graph convolutional neural network variant, PiNet, as well as implemented the established Behler-Parrinello high-dimensional neural network. These implementations were tested using datasets of isolated small molecules, crystalline materials, liquid water and an aqueous alkaline electrolyte. PiNN comes with a visualizer called PiNNBoard to extract chemical insight ""learned"" by ANNs, provides analytical stress tensor calculations and interfaces to both the Atomic Simulation Environment and a development version of the Amsterdam Modeling Suite. Moreover, PiNN is highly modularized which makes it useful not only as a standalone package but also as a chain of tools to develop and to implement novel ANNs. The code is distributed under a permissive BSD license and is freely accessible at https://github.com/Teoroo-CMC/PiNN/ with full documentation and tutorials."
"Active-Code Replacement in the OODIDA Data Analytics Platform OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for distributing and executing concurrent data analytics tasks. It targets fleets of reference vehicles in the automotive industry and has a particular focus on rapid prototyping. Its underlying message-passing infrastructure has been implemented in Erlang/OTP. External Python applications perform data analytics tasks. Most work is performed by clients (on-board). A central cloud server performs supplementary tasks (off-board). OODIDA can be automatically packaged and deployed, which necessitates restarting parts of the system, or all of it. This is potentially disruptive. To address this issue, we added the ability to execute user-defined Python modules on clients as well as the server. These modules can be replaced without restarting any part of the system and they can even be replaced between iterations of an ongoing assignment. This facilitates use cases such as iterative A/B testing of machine learning algorithms or modifying experimental algorithms on-the-fly."
"MiSTree: a Python package for constructing and analysing Minimum Spanning Trees The minimum spanning tree (MST), a graph constructed from a distribution of points, draws lines between pairs of points so that all points are linked in a single skeletal structure that contains no loops and has minimal total edge length. The MST has been used in a broad range of scientific fields such as particle physics (to distinguish classes of events in collider collisions), in astronomy (to detect mass segregation in star clusters) and cosmology (to search for filaments in the cosmic web). Its success in these fields has been driven by its sensitivity to the spatial distribution of points and the patterns within. MiSTree, a public Python package, allows a user to construct the MST in a variety of coordinates systems, including Celestial coordinates used in astronomy. The package enables the MST to be constructed quickly by initially using a k-nearest neighbour graph (kNN, rather than a matrix of pairwise distances) which is then fed to Kruskal's algorithm to construct the MST. MiSTree enables a user to measure the statistics of the MST and provides classes for binning the MST statistics (into histograms) and plotting the distributions. Applying the MST will enable the inclusion of high-order statistics information from the cosmic web which can provide additional information to improve cosmological parameter constraints. This information has not been fully exploited due to the computational cost of calculating N-point statistics. MiSTree was designed to be used in cosmology but could be used in any field which requires extracting non-Gaussian information from point distributions. The source code for MiSTree is available on GitHub at https://github.com/knaidoo29/mistree"
"MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn. The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization. Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model. With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: https://github.com/frankkramer-lab/MIScnn."
"Multi-resolution filtering: an empirical method for isolating faint, extended emission in Dragonfly data and other low resolution images We describe an empirical, self-contained method to isolate faint, large-scale emission in imaging data of low spatial resolution. Multi-resolution filtering (MRF) uses independent data of superior spatial resolution and point source depth to create a model for all compact and high surface brightness objects in the field. This model is convolved with an appropriate kernel and subtracted from the low resolution image. The halos of bright stars are removed in a separate step and artifacts are masked. The resulting image only contains extended emission fainter than a pre-defined surface brightness limit. The method was developed for the Dragonfly Telephoto Array, which produces images that have excellent low surface brightness sensitivity but poor spatial resolution. We demonstrate the MRF technique using Dragonfly images of a satellite of the spiral galaxy M101, the tidal debris surrounding M51, two ultra-diffuse galaxies in the Coma cluster, and the galaxy NGC5907. As part of the analysis we present a newly-identified very faint galaxy in the filtered Dragonfly image of the M101 field. We also discuss variations of the technique for cases when no low resolution data are available (self-MRF and cross-MRF). The method is implemented in mrf, an open-source MIT licensed Python package."
"emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC emcee is a Python library implementing a class of affine-invariant ensemble samplers for Markov chain Monte Carlo (MCMC). This package has been widely applied to probabilistic modeling problems in astrophysics where it was originally published, with some applications in other fields. When it was first released in 2012, the interface implemented in emcee was fundamentally different from the MCMC libraries that were popular at the time, such as PyMC, because it was specifically designed to work with ""black box"" models instead of structured graphical models. This has been a popular interface for applications in astrophysics because it is often non-trivial to implement realistic physics within the modeling frameworks required by other libraries. Since emcee's release, other libraries have been developed with similar interfaces, such as dynesty (Speagle 2019). The version 3.0 release of emcee is the first major release of the library in about 6 years and it includes a full re-write of the computational backend, several commonly requested features, and a set of new ""move"" implementations."
"Towards Scalable Dataframe Systems Dataframes are a popular abstraction to represent, prepare, and analyze data. Despite the remarkable success of dataframe libraries in Rand Python, dataframes face performance issues even on moderately large datasets. Moreover, there is significant ambiguity regarding dataframe semantics. In this paper we lay out a vision and roadmap for scalable dataframe systems. To demonstrate the potential in this area, we report on our experience building MODIN, a scaled-up implementation of the most widely-used and complex dataframe API today, Python's pandas. With pandas as a reference, we propose a simple data model and algebra for dataframes to ground discussion in the field. Given this foundation, we lay out an agenda of open research opportunities where the distinct features of dataframes will require extending the state of the art in many dimensions of data management. We discuss the implications of signature data-frame features including flexible schemas, ordering, row/column equivalence, and data/metadata fluidity, as well as the piecemeal, trial-and-error-based approach to interacting with dataframes."
"Vamsa: Automated Provenance Tracking in Data Science Scripts There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues."
"Hoki: Making BPASS accessible through Python We now know that a large number of stars are born in multiple systems. Additionally, more than 70% of massive stars are found in close binary systems, meaning that they will interact over the course of their lifetime. This has strong implications for their evolution as well as the transients (e.g supernovae) and the potential gravitational wave progenitors they produce. Therefore, in order to understand and correctly interpret astronomical observations of stellar populations, we must use theoretical models able to account for the effects of binary stars. This is the case of the Binary Population and Spectral Synthesis code (BPASS), which has been a staple of the field for over 10 years. As is the case for most other theoretical models, the data products of BPASS are large, varied and complex. As a result, their use requires a level of expertise that is not immediately accessible to a wider community that may hold key observational data. The goal of hoki is to bridge the gap between observation and theory, by providing a set of tools to make BPASS data easily accessible and facilitate analysis. The use of Python is deliberate as it is a ubiquitous language within Astronomy. This allows BPASS results to be used naturally within the pre-existing workflow of most astronomers."
"RDFFrames: Knowledge Graph Access for Machine Learning Tools Knowledge graphs represented as RDF datasets are integral to many machine learning applications. RDF is supported by a rich ecosystem of data management systems and tools, most notably RDF database systems that provide a SPARQL query interface. Surprisingly, machine learning tools for knowledge graphs do not use SPARQL, despite the obvious advantages of using a database system. This is due to the mismatch between SPARQL and machine learning tools in terms of data model and programming style. Machine learning tools work on data in tabular format and process it using an imperative programming style, while SPARQL is declarative and has as its basic operation matching graph patterns to RDF triples. We posit that a good interface to knowledge graphs from a machine learning software stack should use an imperative, navigational programming paradigm based on graph traversal rather than the SPARQL query paradigm based on graph patterns. In this paper, we present RDFFrames, a framework that provides such an interface. RDFFrames provides an imperative Python API that gets internally translated to SPARQL, and it is integrated with the PyData machine learning software stack. RDFFrames enables the user to make a sequence of Python calls to define the data to be extracted from a knowledge graph stored in an RDF database system, and it translates these calls into a compact SPQARL query, executes it on the database system, and returns the results in a standard tabular format. Thus, RDFFrames is a useful tool for data preparation that combines the usability of PyData with the flexibility and performance of RDF database systems."
"An Object-Oriented Library for Heat Transfer Modelling and Simulation in Open Cell Foams Metallic open cell foams have multiple applications in industry, e. g. as catalyst supports in chemical processes. Their regular or heterogeneous microscopic structure determines the macroscopic thermodynamic and chemical properties. We present an object-oriented python library that generates state space models for simulation and control from the microscopic foam data, which can be imported from the image processing tool iMorph. The foam topology and the 3D geometric data are the basis for discrete modeling of the balance laws using the cell method. While the material structure imposes a primal chain complex to define discrete thermodynamic driving forces, the internal energy balance is evaluated on a second chain complex, which is constructed by topological duality. The heat exchange between the solid and the fluid phase is described based on the available surface data. We illustrate in detail the construction of the dual chain complexes, and we show how the structured discrete model directly maps to the software objects of the python code. As a test case, we present simulation results for a foam with a Kelvin cell structure, and compare them to a surrogate finite element model with homogeneous parameters."
"Set Voronoi Tessellation for Particulate Systems in Two Dimensions Given a countable set of points in a continuous space, Voronoi tessellation is an intuitive way of partitioning the space according to the distance to the individual points. As a powerful approach to obtain structural information, it has a long history and widespread applications in diverse disciplines, from astronomy to urban planning. For particulate systems in real life, such as a pile of sand or a crowd of pedestrians, the realization of Voronoi tessellation needs to be modified to accommodate the fact that the particles cannot be simply treated as points. Here, we elucidate the use of Set Voronoi tessellation (i.e., considering for a non-spherical particle a set of points on its surface) to extract meaningful local information in a quasi-two-dimensional system of granular rods. In addition, we illustrate how it can be applied to arbitrarily shaped particles such as an assembly of honey bees or pedestrians for obtaining structural information. Details on the implementation of this algorithm with the strategy of balancing computational cost and accuracy are discussed. Furthermore, we provide our python code as open source in order to facilitate Set Voronoi calculations in two dimensions for arbitrarily shaped objects."
"{\sc precision}: A fast python pipeline for high-contrast imaging -- application to SPHERE observations of the red supergiant VX Sagitariae The search for extrasolar planets has driven rapid advances in instrumentation, resulting in cameras such as SPHERE at the VLT, GPI at Gemini South and SCExAO at Subaru, capable of achieving very high contrast ($\sim10^{6}$) around bright stars with small inner working angles ($\sim 0\farcs{1}$). The optimal exploitation of data from these instruments depends on the availability of easy-to-use software to process and analyse their data products. We present a pure-python pipeline, {\sc precision}, which provides fast, memory-efficient reduction of data from the SPHERE/IRDIS near-infrared imager, and can be readily extended to other instruments. We apply {\sc precision} to observations of the extreme red supergiant VX~Sgr, the inner outflow of which is revealed to host complex, asymmetric structure in the near-IR. In addition, optical polarimetric imaging reveals clear extended polarised emission on $\sim0.5^{\prime\prime}$ scales which varies significantly with azimuth, confirming the asymmetry. While not conclusive, this could suggest that the ejecta are confined to a disc or torus, which we are viewing nearly face on, although other non-spherical or clumpy configurations remain possible. VX~Sgr has no known companions, making such a geometry difficult to explain, as there is no obvious source of angular momentum in the system."
"PHOTONAI -- A Python API for Rapid Machine Learning Model Development PHOTONAI is a high-level Python API designed to simplify and accelerate machine learning model development. It functions as a unifying framework allowing the user to easily access and combine algorithms from different toolboxes into custom algorithm sequences. It is especially designed to support the iterative model development process and automates the repetitive training, hyperparameter optimization and evaluation tasks. Importantly, the workflow ensures unbiased performance estimates while still allowing the user to fully customize the machine learning analysis. PHOTONAI extends existing solutions with a novel pipeline implementation supporting more complex data streams, feature combinations, and algorithm selection. Metrics and results can be conveniently visualized using the PHOTONAI Explorer and predictive models are shareable in a standardized format for further external validation or application. A growing add-on ecosystem allows researchers to offer data modality specific algorithms to the community and enhance machine learning in the areas of the life sciences. Its practical utility is demonstrated on an exemplary medical machine learning problem, achieving a state-of-the-art solution in few lines of code. Source code is publicly available on Github, while examples and documentation can be found at www.photon-ai.com."
"A surface finite element method for computational modelling of cell blebbing Cell blebs are protrusions of the cell membrane and can be instrumental for cell migration. We derive a continuum model for the mechanical and geometrical aspects of the onset of blebbing in terms of a force balance. It is abstract and flexible in that it allows for amending force contributions related to membrane tension or the presence of linker molecules between membrane and cell cortex. The deforming membrane and all forces are expressed by means of a parametrisation over a stationary reference surface. A variational formulation is presented and analysed for well-posedness. For this purpose, we derive a semi-discrete scheme based on the surface finite element method. We provide a convergence result and estimates of the error due to the spatial discretisation. Furthermore, we present a computational framework where specific models can be implemented and later on conveniently amended if desired, using a domain specific language implemented in Python. While the high level program control can be done within the Python scripting environment, the actual computationally expensive step of evolving the solution over time is carried out by binding to an efficient software backend. Cell membrane geometries given in terms of a parametrisation or obtained from image data can be accounted for. A couple of numerical simulation results illustrate the approach."
"DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-chip Training DNN+NeuroSim is an integrated framework to benchmark compute-in-memory (CIM) accelerators for deep neural networks, with hierarchical design options from device-level, to circuit-level and up to algorithm-level. A python wrapper is developed to interface NeuroSim with a popular machine learning platform: Pytorch, to support flexible network structures. The framework provides automatic algorithm-to-hardware mapping, and evaluates chip-level area, energy efficiency and throughput for training or inference, as well as training/inference accuracy with hardware constraints. Our prior work (DNN+NeuroSim V1.1) was developed to estimate the impact of reliability in synaptic devices, and analog-to-digital converter (ADC) quantization loss on the accuracy and hardware performance of inference engines. In this work, we further investigated the impact of the analog emerging non-volatile memory non-ideal device properties for on-chip training. By introducing the nonlinearity, asymmetry, device-to-device and cycle-to-cycle variation of weight update into the python wrapper, and peripheral circuits for error/weight gradient computation in NeuroSim core, we benchmarked CIM accelerators based on state-of-the-art SRAM and eNVM devices for VGG-8 on CIFAR-10 dataset, revealing the crucial specs of synaptic devices for on-chip training. The proposed DNN+NeuroSim V2.0 framework is available on GitHub."
"Pressio: Enabling projection-based model reduction for large-scale nonlinear dynamical systems This work introduces Pressio, an open-source project aimed at enabling leading-edge projection-based reduced order models (ROMs) for large-scale nonlinear dynamical systems in science and engineering. Pressio provides model-reduction methods that can reduce both the number of spatial and temporal degrees of freedom for any dynamical system expressible as a system of parameterized ordinary differential equations (ODEs). We leverage this simple, expressive mathematical framework as a pivotal design choice to enable a minimal application programming interface (API) that is natural to dynamical systems. The core component of Pressio is a C++11 header-only library that leverages generic programming to support applications with arbitrary data types and arbitrarily complex programming models. This is complemented with Python bindings to expose these C++ functionalities to Python users with negligible overhead and no user-required binding code. We discuss the distinguishing characteristics of Pressio relative to existing model-reduction libraries, outline its key design features, describe how the user interacts with it, and present two test cases---including one with over 20 million degrees of freedom---that highlight the performance results of Pressio and illustrate the breath of problems that can be addressed with it."
"speckle-tracking: a Software Suite for Ptychographic X-ray Speckle Tracking In recent years, x-ray speckle tracking techniques have emerged as viable tools for wavefront metrology and sample imaging applications. These methods are based on the measurement of near-field images. Thanks to the simple experimental set-up, high angular sensitivity and compatibility with low coherence sources these methods have been actively developed for use with synchrotron and laboratory light sources. Not only do speckle-tracking techniques give the potential for high resolution imaging, but they also provide rapid and robust characterisation of aberrations of x-ray optical elements, focal spot profiles and the sample position and transmission properties. In order to realise these capabilities, we require software implementations that are equally rapid and robust. To address this need, a software suite has been developed for the ""ptychographic x-ray speckle tracking technique"" -- an x-ray speckle based method suitable for highly divergent wavefields. The software suite is written in Python 3, with an OpenCL back end for GPU and multi-CPU core processing. It is accessible as a Python module, through the command line or through a graphical user interface and is available as source code under version 3 or later of the GNU General Public License."
"A Survey of Computational Tools in Solar Physics The SunPy Project developed a 13-question survey to understand the software and hardware usage of the solar physics community. 364 members of the solar physics community, across 35 countries, responded to our survey. We found that 99$\pm$0.5% of respondents use software in their research and 66% use the Python scientific software stack. Students are twice as likely as faculty, staff scientists, and researchers to use Python rather than Interactive Data Language (IDL). In this respect, the astrophysics and solar physics communities differ widely: 78% of solar physics faculty, staff scientists, and researchers in our sample uses IDL, compared with 44% of astrophysics faculty and scientists sampled by Momcheva and Tollerud (2015). 63$\pm$4% of respondents have not taken any computer-science courses at an undergraduate or graduate level. We also found that most respondents utilize consumer hardware to run software for solar-physics research. Although 82% of respondents work with data from space-based or ground-based missions, some of which (e.g. the Solar Dynamics Observatory and Daniel K. Inouye Solar Telescope) produce terabytes of data a day, 14% use a regional or national cluster, 5% use a commercial cloud provider, and 29% use exclusively a laptop or desktop. Finally, we found that 73$\pm$4% of respondents cite scientific software in their research, although only 42$\pm$3% do so routinely."
"Geomstats: A Python Package for Riemannian Geometry in Machine Learning We introduce Geomstats, an open-source Python toolbox for computations and statistics on nonlinear manifolds, such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Among others, manifolds come equipped with families of Riemannian metrics, with associated exponential and logarithmic maps, geodesics and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends, namely NumPy, PyTorch and TensorFlow, enabling GPU acceleration. This paper presents the package, compares it with related libraries and provides relevant code examples. We show that Geomstats provides reliable building blocks to foster research in differential geometry and statistics, and to democratize the use of Riemannian geometry in machine learning applications. The source code is freely available under the MIT license at \url{geomstats.ai}."
"Introducing PyCross: PyCloudy Rendering Of Shape Software for pseudo 3D ionisation modelling of nebulae Research into the processes of photoionised nebulae plays a significant part in our understanding of stellar evolution. It is extremely difficult to visually represent or model ionised nebula, requiring astronomers to employ sophisticated modelling code to derive temperature, density and chemical composition. Existing codes are available that often require steep learning curves and produce models derived from mathematical functions. In this article we will introduce PyCross: PyCloudy Rendering Of Shape Software. This is a pseudo 3D modelling application that generates photoionisation models of optically thin nebulae, created using the Shape software. Currently PyCross has been used for novae and planetary nebulae, and it can be extended to Active Galactic Nuclei or any other type of photoionised axisymmetric nebulae. Functionality, an operational overview, and a scientific pipeline will be described with scenarios where PyCross has been adopted for novae (V5668 Sagittarii (2015) & V4362 Sagittarii (1994)) and a planetary nebula (LoTr1). Unlike the aforementioned photoionised codes this application does not require any coding experience, nor the need to derive complex mathematical models, instead utilising the select features from Cloudy/PyCloudy and Shape. The software was developed using a formal software development lifecycle, written in Python and will work without the need to install any development environments or additional python packages. This application, Shape models and PyCross archive examples are freely available to students, academics and research community on GitHub for download (https://github.com/karolfitzgerald/PyCross_OSX_App)."
"PypeIt: The Python Spectroscopic Data Reduction Pipeline PypeIt is a Python package for semi-automated reduction of astronomical, spectroscopic data. Its algorithms build on decades-long development of previous data reduction pipelines by the developers (Bernstein, Burles, & Prochaska, 2015; Bochanski et al., 2009). The reduction procedure -- including a complete list of the input parameters and available functionality -- is provided as online documentation hosted by Read the Docs, which is regularly updated. (https://pypeit.readthedocs.io/en/latest/). Release v1.0.3 serves the following spectrographs: Gemini/GNIRS, Gemini/GMOS, Gemini/FLAMINGOS 2, Lick/Kast, Magellan/MagE, Magellan/Fire, MDM/OSMOS, Keck/DEIMOS (600ZD, 830G, 1200G), Keck/LRIS, Keck/MOSFIRE (J and Y gratings tested), Keck/NIRES, Keck/NIRSPEC (low-dispersion), LBT/Luci-I, Luci-II, LBT/MODS (beta), NOT/ALFOSC (grism4), VLT/X-Shooter (VIS, NIR), VLT/FORS2 (300I, 300V), WHT/ISIS."
"Vulnerability Analysis of 2500 Docker Hub Images The use of container technology has skyrocketed during the last few years, with Docker as the leading container platform. Docker's online repository for publicly available container images, called Docker Hub, hosts over 3.5 million images at the time of writing, making it the world's largest community of container images. We perform an extensive vulnerability analysis of 2500 Docker images. It is of particular interest to perform this type of analysis because the vulnerability landscape is a rapidly changing category, the vulnerability scanners are constantly developed and updated, new vulnerabilities are discovered, and the volume of images on Docker Hub is increasing every day. Our main findings reveal that (1) the number of newly introduced vulnerabilities on Docker Hub is rapidly increasing; (2) certified images are the most vulnerable; (3) official images are the least vulnerable; (4) there is no correlation between the number of vulnerabilities and image features (i.e., number of pulls, number of stars, and days since the last update); (5) the most severe vulnerabilities originate from two of the most popular scripting languages, JavaScript and Python; and (6) Python 2.x packages and jackson-databind packages contain the highest number of severe vulnerabilities. We perceive our study as the most extensive vulnerability analysis published in the open literature in the last couple of years."
"CosTuuM: polarized thermal dust emission by magnetically oriented spheroidal grains We present the new open source C++-based Python library CosTuuM that can be used to generate infrared absorption and emission coefficients for arbitrary mixtures of spheroidal dust grains that are (partially) aligned with a magnetic field. We outline the algorithms underlying the software, demonstrate the accuracy of our results using benchmarks from literature, and use our tool to investigate some commonly used approximative recipes. We find that the linear polarization fraction for a partially aligned dust grain mixture can be accurately represented by an appropriate linear combination of perfectly aligned grains and grains that are randomly oriented, but that the commonly used picket fence alignment breaks down for short wavelengths. We also find that for a fixed dust grain size, the absorption coefficients and linear polarization fraction for a realistic mixture of grains with various shapes cannot both be accurately represented by a single representative grain with a fixed shape, but that instead an average over an appropriate shape distribution should be used. Insufficient knowledge of an appropriate shape distribution is the main obstacle in obtaining accurate optical properties. CosTuuM is available as a standalone Python library and can be used to generate optical properties to be used in radiative transfer applications."
"Simplified fast detector simulation in MadAnalysis 5 We introduce a new simplified fast detector simulator in the MadAnalysis 5 platform. The Python-like interpreter of the programme has been augmented by new commands allowing for a detector parametrisation through smearing and efficiency functions. On run time, an associated C++ code is automatically generated and executed to produce reconstructed-level events. In addition, we have extended the MadAnalysis 5 recasting infrastructure to support our detector emulator, and we provide predefined LHC detector configurations. We have compared predictions obtained with our approach to those resulting from the usage of the Delphes 3 software, both for Standard Model processes and a few new physics signals. Results generally agree to a level of about 10% or better, the largest differences in the predictions stemming from the different strategies that are followed to model specific detector effects. Equipped with these new functionalities, MadAnalysis 5 now offers a new user-friendly way to include detector effects when analysing collider events, the simulation of the detector and the analysis being both handled either through a set of intuitive Python commands or directly within the C++ core of the platform."
"lensingGW: a Python package for lensing of gravitational waves Advanced LIGO and Advanced Virgo could observe the first lensed gravitational waves in the coming years, while the future Einstein Telescope could observe hundreds of lensed events. Ground-based gravitational-wave detectors can resolve arrival time differences of the order of the inverse of the observed frequencies. As LIGO/Virgo frequency band spans from a few $\rm Hz$ to a few $ \rm kHz$, the typical time resolution of current interferometers is of the order of milliseconds. When microlenses are embedded in galaxies or galaxy clusters, lensing can become more prominent and result in observable time delays at LIGO/Virgo frequencies. Therefore, gravitational waves could offer an exciting alternative probe of microlensing. However, currently, only a few lensing configurations have been worked out in the context of gravitational-wave lensing. In this paper, we present lensingGW, a Python package designed to handle both strong and microlensing of compact binaries and the related gravitational-wave signals. This synergy paves the way for systematic parameter space investigations and the detection of arbitrary lens configurations and compact sources. We demonstrate the working mechanism of lensingGW and its use to study microlenses embedded in galaxies."
"Feature Extraction for Novelty Detection in Network Traffic Data representation plays a critical role in the performance of novelty detection (or ``anomaly detection'') methods in machine learning. The data representation of network traffic often determines the effectiveness of these models as much as the model itself. The wide range of novel events that network operators need to detect (e.g., attacks, malware, new applications, changes in traffic demands) introduces the possibility for a broad range of possible models and data representations. In each scenario, practitioners must spend significant effort extracting and engineering features that are most predictive for that situation or application. While anomaly detection is well-studied in computer networking, much existing work develops specific models that presume a particular representation -- often IPFIX/NetFlow. Yet, other representations may result in higher model accuracy, and the rise of programmable networks now makes it more practical to explore a broader range of representations. To facilitate such exploration, we develop a systematic framework, open-source toolkit, and public Python library that makes it both possible and easy to extract and generate features from network traffic and perform and end-to-end evaluation of these representations across most prevalent modern novelty detection models. We first develop and publicly release an open-source tool, an accompanying Python library (NetML), and end-to-end pipeline for novelty detection in network traffic. Second, we apply this tool to five different novelty detection problems in networking, across a range of scenarios from attack detection to novel device detection. Our findings general insights and guidelines concerning which features appear to be more appropriate for particular situations."
"The Scikit HEP Project -- overview and prospects Scikit-HEP is a community-driven and community-oriented project with the goal of providing an ecosystem for particle physics data analysis in Python. Scikit-HEP is a toolset of approximately twenty packages and a few ""affiliated"" packages. It expands the typical Python data analysis tools for particle physicists. Each package focuses on a particular topic, and interacts with other packages in the toolset, where appropriate. Most of the packages are easy to install in many environments; much work has been done this year to provide binary ""wheels"" on PyPI and conda-forge packages. The Scikit-HEP project has been gaining interest and momentum, by building a user and developer community engaging collaboration across experiments. Some of the packages are being used by other communities, including the astroparticle physics community. An overview of the overall project and toolset will be presented, as well as a vision for development and sustainability."
"ExoTiC-ISM: A Python package for marginalised exoplanet transit parameters across a grid of systematic instrument models To address the the problem of calibration of instrument systematics in transit light curves, we present the Python package ExoTiC-ISM. Transit spectroscopy can reveal many different chemical components in exoplanet atmospheres, but such results depend on well-calibrated transit light curve observations. Each transit data set will contain instrument systematics that depend on the instrument used and will need to be calibrated out with an instrument systematic model. The proposed solution in Wakeford et al. (2016) (arXiv:1601.02587 [astro-ph.EP]) is to use a marginalisation across a grid of systematic models in order to retrieve marginalised transit parameters. Doing this over observations in multiple wavelengths yields a robust transmission spectrum of an exoplanet. ExoTiC-ISM provides tools to perform this analysis, and its current capability contains a systematic grid that is applicable to the Wide Field Camera 3 (WFC3) detector on the Hubble Space Telescope (HST), particularly for the two infrared grisms G141 and G102. By modularisation of the code and implementation of more systematic grids, ExoTiC-ISM can be used for other instruments, and an implementation for select detectors on the James Webb Space Telescope (JWST) will provide robust transit spectra in the future."
"Towards a Sustainable Microgrid on Alderney Island Using a Python-based Energy Planning Tool In remote or islanded communities, the use of microgrids (MGs) is necessary to ensure electrification and resilience of supply. However, even in small-scale systems, it is computationally and mathematically challenging to design low-cost, optimal, sustainable solutions taking into consideration all the uncertainties of load demands and power generations from renewable energy sources (RESs). This paper uses the open-source Python-based Energy Planning (PyEPLAN) tool, developed for the design of sustainable MGs in remote areas, on the Alderney island, the 3$^{rd}$ largest of the Channel Islands with a population of about 2000 people. A two-stage stochastic model is used to optimally invest in battery storage, solar power, and wind power units. Moreover, the AC power flow equations are modelled by a linearised version of the DistFlow model in PyEPLAN, where the investment variables are here-and-now decisions and not a function of uncertain parameters while the operation variables are wait-and-see decisions and a function of uncertain parameters. The $k$-means clustering technique is used to generate a set of best (risk-seeker), nominal (risk-neutral), and worst (risk-averse) scenarios capturing the uncertainty spectrum using the yearly historical patterns of load demands and solar/wind power generations. The proposed investment planning tool is a mixed-integer linear programming (MILP) model and is coded with Pyomo in PyEPLAN."
"GraphRepo: Fast Exploration in Software Repository Mining Mining and storage of data from software repositories is typically done on a per-project basis, where each project uses a unique combination of data schema, extraction tools, and (intermediate) storage infrastructure. We introduce GraphRepo, a tool that enables a unified approach to extract data from Git repositories, store it, and share it across repository mining projects. GraphRepo usesNeo4j, an ACID-compliant graph database management system, and allows modular plug-in of components for repository extraction (drillers), analysis (miners), and export (mappers). The graph enables a natural way to query the data by removing the need for data normalisation. GraphRepo is built in Python and offers multiple ways to interface with the rich Python ecosystem and with big data solutions. The schema of the graph database is generic and extensible. Using GraphRepo for software repository mining offers several advantages versus creating project-specific infrastructure: (i) high performance for short-iteration exploration and scalability to large data sets (ii) easy distribution of extracted data(e.g., for replication) or sharing of extracted data among projects, and (iii) extensibility and interoperability. A set of benchmarks on four open source projects demonstrate that GraphRepo allows very fast querying of repository data, once extracted and indexed. More information can be found in the project's documentation (available at https://tinyurl.com/grepodoc) and in the project's repository (available at https://tinyurl.com/grrepo). A video demonstration isalso available online (https://tinyurl.com/grrepov)"
"Characterising the Gaia Radial Velocity sample selection function in its native photometry The Gaia DR2 radial velocity sample (GDR2RVS), which provides six-dimensional phase-space information on 7.2 million stars, is of great value for inferring properties of the Milky Way. Yet a quantitative and accurate modelling of this sample is hindered without knowledge and inclusion of a well-characterized selection function. Here we derive the selection function through estimates of the internal completeness, i.e. the ratio of GDR2RVS sources compared to all Gaia DR2 sources (GDR2all). We show that this selection function or ""completeness"" depends on basic observables, in particular the apparent magnitude GRVS and colour G-GRP, but also on the surrounding source density and on sky position, where the completeness exhibits distinct small-scale structure. We identify a region of magnitude and colour that has high completeness, providing an approximate but simple way of implementing the selection function. For a more rigorous and detailed description we provide python code to query our selection function, as well as tools and ADQL queries that produce custom selection functions with additional quality cuts."
"Introducing students to research codes: A short course on solving partial differential equations in Python Recent releases of open-source research codes and solvers for numerically solving partial differential equations in Python present a great opportunity for educators to integrate these codes into the classroom in a variety of ways. The ease with which a problem can be implemented and solved using these codes reduce the barrier to entry for users. We demonstrate how one of these codes,FiPy, can be introduced to students through a short course using progression as the guiding philosophy. Four exercises of increasing complexity were developed. Basic concepts from more advanced numerical methods courses are also introduced at appropriate points. To further engage students, we demonstrate how an open research problem can be readily implemented and also incorporate the use of ParaView to post-process their results. Student engagement and learning outcomes were evaluated through a pre and post-course survey and a focus group discussion. Students broadly found the course to be engaging and useful with the ability to easily visualise the solution to PDEs being greatly valued. Due to the introductory nature of the course, due care in terms of set-up and the design of learning activities during the course is essential. This course, if integrated with appropriate level of support, can encourage students to use the provided codes and improve their understanding of concepts used in numerical analysis and PDEs."
"A Python Library for Exploratory Data Analysis on Twitter Data based on Tokens and Aggregated Origin-Destination Information Twitter is perhaps the social media more amenable for research. It requires only a few steps to obtain information, and there are plenty of libraries that can help in this regard. Nonetheless, knowing whether a particular event is expressed on Twitter is a challenging task that requires a considerable collection of tweets. This proposal aims to facilitate, to a researcher interested, the process of mining events on Twitter. The events could be related to natural disasters, health issues, and people's mobility, among other studies that can be pursued with the library proposed. Different applications are presented in this contribution to illustrate the library's capabilities: an exploratory analysis of the topics discovered in tweets, a study on similarity among dialects of the Spanish language, and a mobility report on different countries. In summary, the Python library presented is applied to different domains and retrieves a plethora of information processed from Twitter (since December 2015) in terms of words, bi-grams of words, and their frequencies by day for Arabic, English, Spanish, and Russian languages. The mobility information is related to the number of travels among locations for more than 200 countries or territories; our library also provides access to this information."
"Denoising convolutional neural networks for photoacoustic microscopy Photoacoustic imaging is a new imaging technology in recent years, which combines the advantages of high resolution and rich contrast of optical imaging with the advantages of high penetration depth of acoustic imaging. Photoacoustic imaging has been widely used in biomedical fields, such as brain imaging, tumor detection and so on. The signal-to-noise ratio (SNR) of image signals in photoacoustic imaging is generally low due to the limitation of laser pulse energy, electromagnetic interference in the external environment and system noise. In order to solve the problem of low SNR of photoacoustic images, we use feedforward denoising convolutional neural network to further process the obtained images, so as to obtain higher SNR images and improve image quality. We use Python language to manage the referenced Python external library through Anaconda, and build a feedforward noise-reducing convolutional neural network on Pycharm platform.We first processed and segmated a training set containing 400 images, and then used it for network training. Finally, we tested it with a series of cerebrovascular photoacoustic microscopy images.The results show that the peak signal-to-noise ratio (PSNR) of the image increases significantly before and after denoising.The experimental results verify that the feed-forward noise reduction convolutional neural network can effectively improve the quality of photoacoustic microscopic images, which provides a good foundation for the subsequent biomedical research."
"TheHaloMod: An online calculator for the halo model The halo model is a successful framework for describing the distribution of matter in the Universe -- from weak lensing observables to galaxy 2-point correlation functions. We review the basic formulation of the halo model and several of its components in the context of galaxy two-point statistics, developing a coherent framework for its application.   We use this framework to motivate the presentation of a new Python tool for simple and efficient calculation of halo model quantities, and their extension to galaxy statistics via a \textit{halo occupation distribution}, called \halomod. This tool is efficient, simple to use, comprehensive and importantly provides a great deal of flexibility in terms of custom extensions. This Python tool is complemented by a new web-application at https://thehalomod.app that supports the generation of many halo model quantities directly from the browser -- useful for educators, students, theorists and observers."
"AiiDAlab -- an ecosystem for developing, executing, and sharing scientific workflows Cloud platforms allow users to execute tasks directly from their web browser and are a key enabling technology not only for commerce but also for computational science. Research software is often developed by scientists with limited experience in (and time for) user interface design, which can make research software difficult to install and use for novices. When combined with the increasing complexity of scientific workflows (involving many steps and software packages), setting up a computational research environment becomes a major entry barrier. AiiDAlab is a web platform that enables computational scientists to package scientific workflows and computational environments and share them with their collaborators and peers. By leveraging the AiiDA workflow manager and its plugin ecosystem, developers get access to a growing range of simulation codes through a python API, coupled with automatic provenance tracking of simulations for full reproducibility. Computational workflows can be bundled together with user-friendly graphical interfaces and made available through the AiiDAlab app store. Being fully compatible with open-science principles, AiiDAlab provides a complete infrastructure for automated workflows and provenance tracking, where incorporating new capabilities becomes intuitive, requiring only Python knowledge."
"comp-syn: Perceptually Grounded Word Embeddings with Color Popular approaches to natural language processing create word embeddings based on textual co-occurrence patterns, but often ignore embodied, sensory aspects of language. Here, we introduce the Python package comp-syn, which provides grounded word embeddings based on the perceptually uniform color distributions of Google Image search results. We demonstrate that comp-syn significantly enriches models of distributional semantics. In particular, we show that (1) comp-syn predicts human judgments of word concreteness with greater accuracy and in a more interpretable fashion than word2vec using low-dimensional word-color embeddings, and (2) comp-syn performs comparably to word2vec on a metaphorical vs. literal word-pair classification task. comp-syn is open-source on PyPi and is compatible with mainstream machine-learning Python packages. Our package release includes word-color embeddings for over 40,000 English words, each associated with crowd-sourced word concreteness judgments."
"c-lasso -- a Python package for constrained sparse and robust regression and classification We introduce c-lasso, a Python package that enables sparse and robust linear regression and classification with linear equality constraints. The underlying statistical forward model is assumed to be of the following form: \[ y = X \beta + \sigma \epsilon \qquad \textrm{subject to} \qquad C\beta=0 \] Here, $X \in \mathbb{R}^{n\times d}$is a given design matrix and the vector $y \in \mathbb{R}^{n}$ is a continuous or binary response vector. The matrix $C$ is a general constraint matrix. The vector $\beta \in \mathbb{R}^{d}$ contains the unknown coefficients and $\sigma$ an unknown scale. Prominent use cases are (sparse) log-contrast regression with compositional data $X$, requiring the constraint $1_d^T \beta = 0$ (Aitchion and Bacon-Shone 1984) and the Generalized Lasso which is a special case of the described problem (see, e.g, (James, Paulson, and Rusmevichientong 2020), Example 3). The c-lasso package provides estimators for inferring unknown coefficients and scale (i.e., perspective M-estimators (Combettes and M\""uller 2020a)) of the form \[ \min_{\beta \in \mathbb{R}^d, \sigma \in \mathbb{R}_{0}} f\left(X\beta - y,{\sigma} \right) + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad C\beta = 0 \] for several convex loss functions $f(\cdot,\cdot)$. This includes the constrained Lasso, the constrained scaled Lasso, and sparse Huber M-estimators with linear equality constraints."
"PRISA: a simple software for determining refractive index, extinction co-efficient, dispersion energy, band gap, and thickness of semiconductor and dielectric thin films A simple user-friendly software named PRISA has been developed to determine optical constants (refractive index and extinction co-efficient), dispersion parameters (oscillator energy and dispersion energy), absorption co-efficient, band gap and thickness of semiconductor and dielectric thin films from their measured transmission spectrum, only. The thickness, refractive index, and extinction co-efficient of the films have been derived using Envelope method proposed by Swanepoel. The absorption co-efficient in the strong absorption region is calculated using the method proposed by Connel and Lewis. Subsequently, both direct and indirect bandgap of the films is estimated from the absorption co-efficient spectrum using Tauc plot. The codes for the software are written in Python and the graphical user interface is programmed with tkinter package of Python. It provides convenient input and output of the measured and derived data. The software has a feature to retrieve transmission spectrum using the derived parameters in order to check their reliability. The performance of the software is verified by analyzing numerically generated transmission spectra of a-Si:H amorphous semiconductor thin films, and experimentally measured transmission spectra of electron beam evaporated HfO2 dielectric thin films as examples. PRISA is found to be much simpler and accurate as compared to the other freely available softwares. To help other researchers working on thin films, the software is made freely available at https://www.shuvendujena.tk/download."
"Efficient space-time reduced order model for linear dynamical systems in Python using less than 120 lines of code A classical reduced order model (ROM) for dynamical problems typically involves only the spatial reduction of a given problem. Recently, a novel space-time ROM for linear dynamical problems has been developed, which further reduces the problem size by introducing a temporal reduction in addition to a spatial reduction without much loss in accuracy. The authors show an order of a thousand speed-up with a relative error of less than 0.00001 for a large-scale Boltzmann transport problem. In this work, we present for the first time the derivation of the space-time Petrov-Galerkin projection for linear dynamical systems and its corresponding block structures. Utilizing these block structures, we demonstrate the ease of construction of the space-time ROM method with two model problems: 2D diffusion and 2D convection diffusion, with and without a linear source term. For each problem, we demonstrate the entire process of generating the full order model (FOM) data, constructing the space-time ROM, and predicting the reduced-order solutions, all in less than 120 lines of Python code. We compare our Petrov-Galerkin method with the traditional Galerkin method and show that the space-time ROMs can achieve O(100) speed-ups with O(0.001) to O(0.0001) relative errors for these problems. Finally, we present an error analysis for the space-time Petrov-Galerkin projection and derive an error bound, which shows an improvement compared to traditional spatial Galerkin ROM methods."
"A 55-line code for large-scale parallel topology optimization in 2D and 3D This paper presents a 55-line code written in python for 2D and 3D topology optimization (TO) based on the open-source finite element computing software (FEniCS), equipped with various finite element tools and solvers. PETSc is used as the linear algebra back-end, which results in significantly less computational time than standard python libraries. The code is designed based on the popular solid isotropic material with penalization (SIMP) methodology. Extensions to multiple load cases, different boundary conditions, and incorporation of passive elements are also presented. Thus, this implementation is the most compact implementation of SIMP based topology optimization for 3D as well as 2D problems.   Utilizing the concept of Euclidean distance matrix to vectorize the computation of the weight matrix for the filter, we have achieved a substantial reduction in the computational time and have also made it possible for the code to work with complex ground structure configurations. We have also presented the code's extension to large-scale topology optimization problems with support for parallel computations on complex structural configuration, which could help students and researchers explore novel insights into the TO problem with dense meshes. Appendix-A contains the complete code, and the website: \url{https://github.com/iitrabhi/topo-fenics} also contains the complete code."
"Tutorial: Introduction to computational causal inference using reproducible Stata, R and Python code The purpose of many health studies is to estimate the effect of an exposure on an outcome. It is not always ethical to assign an exposure to individuals in randomised controlled trials, instead observational data and appropriate study design must be used. There are major challenges with observational studies, one of which is confounding that can lead to biased estimates of the causal effects. Controlling for confounding is commonly performed by simple adjustment for measured confounders; although, often this is not enough. Recent advances in the field of causal inference have dealt with confounding by building on classical standardisation methods. However, these recent advances have progressed quickly with a relative paucity of computational-oriented applied tutorials contributing to some confusion in the use of these methods among applied researchers. In this tutorial, we show the computational implementation of different causal inference estimators from a historical perspective where different estimators were developed to overcome the limitations of the previous one. Furthermore, we also briefly introduce the potential outcomes framework, illustrate the use of different methods using an illustration from the health care setting, and most importantly, we provide reproducible and commented code in Stata, R and Python for researchers to apply in their own observational study. The code can be accessed at https://github.com/migariane/TutorialCausalInferenceEstimators"
"A scalable transient detection pipeline for the Australian SKA Pathfinder VAST survey The Australian Square Kilometre Array Pathfinder (ASKAP) collects images of the sky at radio wavelengths with an unprecedented field of view, combined with a high angular resolution and sub-millijansky sensitivities. The large quantity of data produced is used by the ASKAP Variables and Slow Transients (VAST) survey science project to study the dynamic radio sky. Efficient pipelines are vital in such research, where searches often form a `needle in a haystack' type of problem to solve. However, the existing pipelines developed among the radio-transient community are not suitable for the scale of ASKAP datasets.   In this paper we provide a technical overview of the new ""VAST Pipeline"": a modern and scalable Python-based data pipeline for transient searches, using up-to-date dependencies and methods. The pipeline allows source association to be performed at scale using the Pandas DataFrame interface and the well-known Astropy crossmatch functions. The Dask Python framework is used to parallelise operations as well as scale them both vertically and horizontally, by means of a cluster of workers. A modern web interface for data exploration and querying has also been developed using the latest Django web framework combined with Bootstrap."
"PyEquIon: A Python Package For Automatic Speciation Calculations of Aqueous Electrolyte Solutions In several industrial applications, such as crystallization, pollution control, and flow assurance, an accurate understanding of the aqueous electrolyte solutions is crucial. Electrolyte equilibrium calculation contributes with the design and optimization of processes by providing important information, such as species concentration, solution pH and potential for solid formation. In this work, a pure Python library distributed under BSD-3 license was developed for the calculation of aqueous electrolyte equilibrium. The package takes as inputs the feed components of a given solution, and it automatically identifies its composing ions and the chemical reactions involved to calculate equilibrium conditions. Moreover, there is no established electrolyte activity coefficient model for a broad range of operational conditions. Hence, in this package, built-in activity coefficient models are structured in a modular approach, so that the non-ideality calculation can be performed by a user provided function, which allows further research in the topic. The package can be used by researchers to readily identify the equilibrium reactions and possible solid phases in a user friendly language."
"The adaptive optics simulation analysis tool(kit) (AOSAT) AOSAT is a python package for the analysis of single-conjugate adaptive optics (SCAO) simulation results. Python is widely used in the astronomical community these days, and AOSAT may be used stand-alone, integrated into a simulation environment, or can easily be extended according to a user's needs. Standalone operation requires the user to provide the residual wavefront frames provided by the SCAO simulation package used, the aperture mask (pupil) used for the simulation, and a custom setup file describing the simulation/analysis configuration. In its standard form, AOSAT's ""tearsheet"" functionality will then run all standard analyzers, providing an informative plot collection on properties such as the point-spread function (PSF) and its quality, residual tip-tilt, the impact of pupil fragmentation, residual optical aberration modes both static and dynamic, the expected high-contrast performance of suitable instrumentation with and without coronagraphs, and the power spectral density of residual wavefront errors.   AOSAT fills the gap between the simple numerical outputs provided by most simulation packages, and the full-scale deployment of instrument simulators and data reduction suites operating on SCAO residual wavefronts. It enables instrument designers and end-users to quickly judge the impact of design or configuration decisions on the final performance of down-stream instrumentation."
"Extracting Rationale for Open Source Software Development Decisions -- A Study of Python Email Archives A sound Decision-Making (DM) process is key to the successful governance of software projects. In many Open Source Software Development (OSSD) communities, DM processes lie buried amongst vast amounts of publicly available data. Hidden within this data lie the rationale for decisions that led to the evolution and maintenance of software products. While there have been some efforts to extract DM processes from publicly available data, the rationale behind how the decisions are made have seldom been explored. Extracting the rationale for these decisions can facilitate transparency (by making them known), and also promote accountability on the part of decision-makers. This work bridges this gap by means of a large-scale study that unearths the rationale behind decisions from Python development email archives comprising about 1.5 million emails. This paper makes two main contributions. First, it makes a knowledge contribution by unearthing and presenting the rationale behind decisions made. Second, it makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs multiple heuristics, and follows a data-driven, bottom-up approach to infer the rationale behind specific decisions (e.g., whether a new module is implemented based on core developer consensus or benevolent dictator's pronouncement). Our approach can be applied to extract rationale in other OSSD communities that have similar governance structures."
"ordpy: A Python package for data analysis with permutation entropy and ordinal network methods Since Bandt and Pompe's seminal work, permutation entropy has been used in several applications and is now an essential tool for time series analysis. Beyond becoming a popular and successful technique, permutation entropy inspired a framework for mapping time series into symbolic sequences that triggered the development of many other tools, including an approach for creating networks from time series known as ordinal networks. Despite the increasing popularity, the computational development of these methods is fragmented, and there were still no efforts focusing on creating a unified software package. Here we present ordpy, a simple and open-source Python module that implements permutation entropy and several of the principal methods related to Bandt and Pompe's framework to analyze time series and two-dimensional data. In particular, ordpy implements permutation entropy, Tsallis and R\'enyi permutation entropies, complexity-entropy plane, complexity-entropy curves, missing ordinal patterns, ordinal networks, and missing ordinal transitions for one-dimensional (time series) and two-dimensional (images) data as well as their multiscale generalizations. We review some theoretical aspects of these tools and illustrate the use of ordpy by replicating several literature results."
"A Python Framework for Fast Modelling and Simulation of Cellular Nonlinear Networks and other Finite-difference Time-domain Systems This paper introduces and evaluates a freely available cellular nonlinear network simulator optimized for the effective use of GPUs, to achieve fast modelling and simulations. Its relevance is demonstrated for several applications in nonlinear complex dynamical systems, such as slow-growth phenomena as well as for various image processing applications such as edge detection. The simulator is designed as a Jupyter notebook written in Python and functionally tested and optimized to run on the freely available cloud platform Google Collaboratory. Although the simulator, in its actual form, is designed to model the FitzHugh Nagumo Reaction-Diffusion cellular nonlinear network, it can be easily adapted for any other type of finite-difference time-domain model. Four implementation versions are considered, namely using the PyCUDA, NUMBA respectively CUPY libraries (all three supporting GPU computations) as well as a NUMPY-based implementation to be used when GPU is not available. The specificities and performances for each of the four implementations are analyzed concluding that the PyCUDA implementation ensures a very good performance being capable to run up to 14000 Mega cells per seconds (each cell referring to the basic nonlinear dynamic system composing the cellular nonlinear network)."
"CoinTossX: An open-source low-latency high-throughput matching engine We deploy and demonstrate the CoinTossX low-latency, high-throughput, open-source matching engine with orders sent using the Julia and Python languages. We show how this can be deployed for small-scale local desk-top testing and discuss a larger scale, but local hosting, with multiple traded instruments managed concurrently and managed by multiple clients. We then demonstrate a cloud based deployment using Microsoft Azure, with large-scale industrial and simulation research use cases in mind. The system is exposed and interacted with via sockets using UDP SBE message protocols and can be monitored using a simple web browser interface using HTTP. We give examples showing how orders can be be sent to the system and market data feeds monitored using the Julia and Python languages. The system is developed in Java with orders submitted as binary encodings (SBE) via UDP protocols using the Aeron Media Driver as the low-latency, high throughput message transport. The system separates the order-generation and simulation environments e.g. agent-based model simulation, from the matching of orders, data-feeds and various modularised components of the order-book system. This ensures a more natural and realistic asynchronicity between events generating orders, and the events associated with order-book dynamics and market data-feeds. We promote the use of Julia as the preferred order submission and simulation environment."
"Distributed statistical inference with pyhf enabled through funcX In High Energy Physics facilities that provide High Performance Computing environments provide an opportunity to efficiently perform the statistical inference required for analysis of data from the Large Hadron Collider, but can pose problems with orchestration and efficient scheduling. The compute architectures at these facilities do not easily support the Python compute model, and the configuration scheduling of batch jobs for physics often requires expertise in multiple job scheduling services. The combination of the pure-Python libraries pyhf and funcX reduces the common problem in HEP analyses of performing statistical inference with binned models, that would traditionally take multiple hours and bespoke scheduling, to an on-demand (fitting) ""function as a service"" that can scalably execute across workers in just a few minutes, offering reduced time to insight and inference. We demonstrate execution of a scalable workflow using funcX to simultaneously fit 125 signal hypotheses from a published ATLAS search for new physics using pyhf with a wall time of under 3 minutes. We additionally show performance comparisons for other physics analyses with openly published probability models and argue for a blueprint of fitting as a service systems at HPC centers."
"Software Development During COVID-19 Pandemic: an Analysis of Stack Overflow and GitHub The new coronavirus became a severe health issue for the world. This situation has motivated studies of different areas to combat this pandemic. In software engineering, we point out data visualization projects to follow the disease evolution, machine learning to estimate the pandemic behavior, and computer vision processing radiologic images. Most of these projects are stored in version control systems, and there are discussions about them in Question & Answer websites. In this work, we conducted a Mining Software Repository on a large number of questions and projects aiming to find trends that could help researchers and practitioners to fight against the coronavirus. We analyzed 1,190 questions from Stack Overflow and Data Science Q\&A and 60,352 GitHub projects. We identified a correlation between the questions and projects throughout the pandemic. The main questions about coronavirus are how-to, related to web scraping and data visualization, using Python, JavaScript, and R. The most recurrent GitHub projects are machine learning projects, using JavaScript, Python, and Java."
"Using Relative Lines of Code to Guide Automated Test Generation for Python Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this paper, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead.We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more), and improve fault detection by an even larger margin (usually more than 75%, and up to 400% or more). The LOC heuristic is also easy to combine with other approaches, and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing."
"DDUO: General-Purpose Dynamic Analysis for Differential Privacy Differential privacy enables general statistical analysis of data with formal guarantees of privacy protection at the individual level. Tools that assist data analysts with utilizing differential privacy have frequently taken the form of programming languages and libraries. However, many existing programming languages designed for compositional verification of differential privacy impose significant burden on the programmer (in the form of complex type annotations). Supplementary library support for privacy analysis built on top of existing general-purpose languages has been more usable, but incapable of pervasive end-to-end enforcement of sensitivity analysis and privacy composition. We introduce DDUO, a dynamic analysis for enforcing differential privacy. DDUO is usable by non-experts: its analysis is automatic and it requires no additional type annotations. DDUO can be implemented as a library for existing programming languages; we present a reference implementation in Python which features moderate runtime overheads on realistic workloads. We include support for several data types, distance metrics and operations which are commonly used in modern machine learning programs. We also provide initial support for tracking the sensitivity of data transformations in popular Python libraries for data analysis. We formalize the novel core of the DDUO system and prove it sound for sensitivity analysis via a logical relation for metric preservation. We also illustrate DDUO's usability and flexibility through various case studies which implement state-of-the-art machine learning algorithms."
"A Uniform Spherical Goat (Problem): Explicit Solution for Homologous Collapse's Radial Evolution in Time The homologous collapse from rest of a uniform density sphere under its self gravity is a well-known toy model for the formation dynamics of astronomical objects ranging from stars to galaxies. Equally well-known is that the evolution of the radius with time cannot be explicitly obtained because of the transcendental nature of the differential equation solution. Rather, both radius and time are written parametrically in terms of the development angle $\theta$. We here present an explicit integral solution for radius as a function of time, exploiting methods from complex analysis recently applied to the mathematically-similar 'geometric goat problem'. Our solution can be efficiently evaluated using a Fast Fourier Transform and allows for arbitrary sampling in time, with a simple Python implementation that is $\sim$$100\times$ faster than using numerical root-finding to achieve arbitrary sampling. Our explicit solution is advantageous relative to the usual approach of first generating a uniform grid in $\theta$, since this latter results in a non-uniform radial or time sampling, less useful for applications such as generation of sub-grid physics models."
"PySTACHIO: Python Single-molecule TrAcking stoiCHiometry Intensity and simulatiOn, a flexible, extensible, beginner-friendly and optimized program for analysis of single-molecule microscopy As camera pixel arrays have grown larger and faster, and optical microscopy techniques ever more refined, there has been an explosion in the quantity of data acquired during routine light microcopy. At the single-molecule level, analysis involves multiple steps and can rapidly become computationally expensive, in some cases intractable on office workstations. Complex bespoke software can present high activation barriers to entry for new users. Here, we redevelop our quantitative single-molecule analysis routines into an optimized and extensible Python program, with GUI and command-line implementations to facilitate use on local machines and remote clusters, by beginners and advanced users alike. We demonstrate that its performance is on par with previous MATLAB implementations but runs an order of magnitude faster. We tested it against challenge data and demonstrate its performance is comparable to state-of-the-art analysis platforms. We show the code can extract fluorescence intensity values for single reporter dye molecules and, using these, estimate molecular stoichiometries and cellular copy numbers of fluorescently-labeled biomolecules. It can evaluate 2D diffusion coefficients for the characteristically short single-particle tracking data. To facilitate benchmarking we include data simulation routines to compare different analysis programs. Finally, we show that it works with 2-color data and enables colocalization analysis based on overlap integration, to infer interactions between differently labelled biomolecules. By making this freely available we aim to make complex light microscopy single-molecule analysis more democratized."
"ast2vec: Utilizing Recursive Neural Encodings of Python Programs Educational datamining involves the application of datamining techniques to student activity. However, in the context of computer programming, many datamining techniques can not be applied because they expect vector-shaped input whereas computer programs have the form of syntax trees. In this paper, we present ast2vec, a neural network that maps Python syntax trees to vectors and back, thereby facilitating datamining on computer programs as well as the interpretation of datamining results. Ast2vec has been trained on almost half a million programs of novice programmers and is designed to be applied across learning tasks without re-training, meaning that users can apply it without any need for (additional) deep learning. We demonstrate the generality of ast2vec in three settings: First, we provide example analyses using ast2vec on a classroom-sized dataset, involving visualization, student motion analysis, clustering, and outlier detection, including two novel analyses, namely a progress-variance-projection and a dynamical systems analysis. Second, we consider the ability of ast2vec to recover the original syntax tree from its vector representation on the training data and two further large-scale programming datasets. Finally, we evaluate the predictive capability of a simple linear regression on top of ast2vec, obtaining similar results to techniques that work directly on syntax trees. We hope ast2vec can augment the educational datamining toolbelt by making analyses of computer programs easier, richer, and more efficient."
"Efficient and Accurate In-Database Machine Learning with SQL Code Generation in Python Following an analysis of the advantages of SQL-based Machine Learning (ML) and a short literature survey of the field, we describe a novel method for In-Database Machine Learning (IDBML). We contribute a process for SQL-code generation in Python using template macros in Jinja2 as well as the prototype implementation of the process. We describe our implementation of the process to compute multidimensional histogram (MDH) probability estimation in SQL. For this, we contribute and implement a novel discretization method called equal quantized rank binning (EQRB) and equal-width binning (EWB). Based on this, we provide data gathered in a benchmarking experiment for the quantitative empirical evaluation of our method and system using the Covertype dataset. We measured accuracy and computation time and compared it to Scikit Learn state of the art classification algorithms. Using EWB, our multidimensional probability estimation was the fastest of all tested algorithms, while being only 1-2% less accurate than the best state of the art methods found (decision trees and random forests). Our method was significantly more accurate than Naive Bayes, which assumes independent one-dimensional probabilities and/or densities. Also, our method was significantly more accurate and faster than logistic regression. This motivates for further research in accuracy improvement and in IDBML with SQL code generation for big data and larger-than-memory datasets."
"kuibit: Analyzing Einstein Toolkit simulations with Python In the era of gravitational-wave astronomy, general-relativistic simulations of compact objects play a role of paramount importance. These calculations can be performed with the Einstein Toolkit, an open-source and community-supported software for numerical-relativity and relativistic astrophysics. The code comes with multiple solvers for Einstein's equations and for the equations of general-relativistic magneto-hydrodynamics, along with a series of useful diagnostics. However, analyzing the output of the Einstein Toolkit can be a challenging task. Usually, the process involves a series of technical obstacles, like combining data from different restarts or working with HDF5 files. Here, we present kuibit, a Python library that takes care of all these low-level details (and many other more) and that provides high-level, intuitive, representations of the data. Kuibit ships with a wide range of features that include full support for 1-3D ASCII and HDF5 grid data, time and frequency series, gravitational waves, and apparent horizons. With kuibit, users can inspect most of the content of a simulation with just a few lines of code. Importantly, kuibit is designed to be a code for the community: it is user-friendly and does not require any proprietary software to run, it has documentation and examples, and it is openly developed with emphasis on extensibility and maintainability."
"Similarity Downselection: A Python implementation of a heuristic search algorithm for finding the set of the n most dissimilar items with an application in conformer sampling Finding the set of the n items most dissimilar from each other out of a larger population becomes increasingly difficult and computationally expensive as either n or the population size grows large. Finding the set of the n most dissimilar items is different than simply sorting an array of numbers because there exists a pairwise relationship between each item and all other items in the population. For instance, if you have a set of the most dissimilar n=4 items, one or more of the items from n=4 might not be in the set n=5. An exact solution would have to search all possible combinations of size n in the population, exhaustively. We present an open-source software called similarity downselection (SDS), written in Python and freely available on GitHub. SDS implements a heuristic algorithm for quickly finding the approximate set(s) of the n most dissimilar items. We benchmark SDS against a Monte Carlo method, which attempts to find the exact solution through repeated random sampling. We show that for SDS to find the set of n most dissimilar conformers, our method is not only orders of magnitude faster, but is also more accurate than running the Monte Carlo for 1,000,000 iterations, each searching for set sizes n=3-7 out of a population of 50,000. We also benchmark SDS against the exact solution for example small populations, showing SDS produces a solution close to the exact solution in these instances."
"Why Aren't Regular Expressions a Lingua Franca? An Empirical Study on the Re-use and Portability of Regular Expressions This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics?   In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language. We experimentally evaluated the riskiness of this practice using a novel regex corpus -- 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences.   We report that developers' belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules."
"AtomAI: A Deep Learning Framework for Analysis of Image and Spectroscopy Data in (Scanning) Transmission Electron Microscopy and Beyond AtomAI is an open-source software package bridging instrument-specific Python libraries, deep learning, and simulation tools into a single ecosystem. AtomAI allows direct applications of the deep convolutional neural networks for atomic and mesoscopic image segmentation converting image and spectroscopy data into class-based local descriptors for downstream tasks such as statistical and graph analysis. For atomically-resolved imaging data, the output is types and positions of atomic species, with an option for subsequent refinement. AtomAI further allows the implementation of a broad range of image and spectrum analysis functions, including invariant variational autoencoders (VAEs). The latter consists of VAEs with rotational and (optionally) translational invariance for unsupervised and class-conditioned disentanglement of categorical and continuous data representations. In addition, AtomAI provides utilities for mapping structure-property relationships via im2spec and spec2im type of encoder-decoder models. Finally, AtomAI allows seamless connection to the first principles modeling with a Python interface, including molecular dynamics and density functional theory calculations on the inferred atomic position. While the majority of applications to date were based on atomically resolved electron microscopy, the flexibility of AtomAI allows straightforward extension towards the analysis of mesoscopic imaging data once the labels and feature identification workflows are established/available. The source code and example notebooks are available at https://github.com/pycroscopy/atomai."
"Agilepy: A Python framework for scientific analysis of AGILE data The Italian AGILE space mission, with its Gamma-Ray Imaging Detector (GRID) instrument sensitive in the 30 MeV-50 GeV gamma-ray energy band, has been operating since 2007. Agilepy is an open-source Python package to analyse AGILE/GRID data. The package is built on top of the command-line version of the AGILE Science Tools, developed by the AGILE Team, publicly available and released by ASI/SSDC. The primary purpose of the package is to provide an easy to use high-level interface to analyse AGILE/GRID data by simplifying the configuration of the tasks and ensuring straightforward access to the data. The current features are the generation and display of sky maps and light curves, the access to gamma-ray sources catalogues, the analysis to perform spectral model and position fitting, the wavelet analysis. Agilepy also includes an interface tool providing the time evolution of the AGILE off-axis viewing angle for a chosen sky region. The Flare Advocate team also uses the tool to analyse the data during the daily monitoring of the gamma-ray sky. Agilepy (and its dependencies) can be easily installed using Anaconda."
"Gaussian Process Regression for foreground removal in HI intensity mapping experiments We apply for the first time Gaussian Process Regression (GPR) as a foreground removal technique in the context of single-dish, low redshift HI intensity mapping, and present an open-source python toolkit for doing so. We use MeerKAT and SKA1-MID-like simulations of 21cm foregrounds (including polarisation leakage), HI cosmological signal and instrumental noise. We find that it is possible to use GPR as a foreground removal technique in this context, and that it is better suited in some cases to recover the HI power spectrum than Principal Component Analysis (PCA), especially on small scales. GPR is especially good at recovering the radial power spectrum, outperforming PCA when considering the full bandwidth of our data. Both methods are worse at recovering the transverse power spectrum, since they rely on frequency-only covariance information. When halving our data along frequency, we find that GPR performs better in the low frequency range, where foregrounds are brighter. It performs worse than PCA when frequency channels are missing, to emulate RFI flagging. We conclude that GPR is an excellent foreground removal option for the case of single-dish, low redshift HI intensity mapping. Our python toolkit gpr4im and the data used in this analysis are publicly available on GitHub. The GitHub symbol in the caption of each figure links to a jupyter notebook showing how the figure was produced."
"DEWPython: A Python Implementation of the Deep Earth Water Model and Application to Ocean Worlds There are two main methods of calculating the thermodynamic properties of water and solutes: mass action (including the Helgeson-Kirkham-Flowers (HKF) equations of state and model) and Gibbs free energy minimization (e.g. Leal et al., 2016). However, in certain regions of pressure and temperature the HKF model inaccurately predicts the speciation and concentration of solutes (e.g. Miron et al., 2019). The Deep Earth Water (DEW) model uses a series of HKF-type equations to calculate the properties of water and solute concentrations at high temperatures (373 - 1473 K) and pressures (0.1 - 6 GPa) (e.g. Huang and Sverjensky, 2019; Pan et al., 2013; Sverjensky et al., 2014). The DEW model is synthesized in an Excel spreadsheet and calculates Gibbs energies of formation, equilibrium constants, and standard volume changes for reactions. Here we present an object-oriented Python implementation of the DEW model, called DEWPython. Our model expands on DEW by increasing model efficiency, streamlining the input process, and incorporating SUPCRT in-line. Additionally, our model builds in minerals and aqueous species from the thermodynamic database slop16.dat (Boyer, 2019) which would normally be calculated separately. We also present a set of reactions relevant to icy ocean world interiors calculated with the DEWPython. The favorability of these reactions indicates likely formation of certain organic species under extreme pressures relevant to ocean worlds."
"Programming Puzzles We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input $x$ which makes $f$ output ""True"". The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f(x)$ is all that is needed to test a candidate solution $x$. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping. We develop baseline enumerative program synthesis and GPT-3 solvers that are capable of solving easy puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Based on a small user study, we find puzzle difficulty to correlate between human programmers and the baseline AI solvers."
"WimPyDD: an object-oriented Python code for the calculation of WIMP direct detection signals We introduce WimPyDD, a modular, object-oriented and customizable Python code that calculates accurate predictions for the expected rates in Weakly Interacting Massive Particle (WIMP) direct-detection experiments within the framework of Galilean-invariant non-relativistic effective theory in virtually any scenario, including inelastic scattering, an arbitrary WIMP spin and a generic WIMP velocity distribution in the Galactic halo. WimPyDD exploits the factorization of the three main components that enter in the calculation of direct detection signals: i) the Wilson coefficients that encode the dependence of the signals on the ultraviolet completion of the effective theory; ii) a response function that depends on the nuclear physics and on the main features of the experimental detector (acceptance, energy resolution, response to nuclear recoils); iii) a halo function that depends on the WIMP velocity distribution and that encodes the astrophysical inputs. In WimPyDD these three components are calculated and stored separately for later interpolation and combined together only as the last step of the signal evaluation procedure. This makes the phenomenological study of the direct detection scattering rate with WimPyDD transparent and fast also when the parameter space of the WIMP model has a large dimensionality."
"Instrument for simultaneous measurement of Seebeck coefficient and thermal conductivity in the temperature range 300-800 K with python interfacing Fabrication and characterization of instrument for high-temperature simultaneous measurement of Seebeck coefficient (S) and thermal conductivity ($\kappa$) has been carried out with python automation. The steady-state based Fourier's law of thermal conduction is employed for $\kappa$ measurement. The parallel thermal conductance technique is implemented for heat loss measurement. Introducing the thin heater and insulating heater base minimize the heat loss and make the way easier to arrive at high temperature. Measurement of S is carried out using differential method. Same thermocouples are used to measure temperature as well as voltage for S measurement. Care of temperature dependent S of thermocouple has also been taken. Simple design, small size, lightweightmake this instrument more robust. All the components for making sample holder are easily available in the market and can be replaced as per the user demand. This instrument can measure samples with various dimensions and shapes in the temperature range 300 $-$ 800 K. The instrument is validated using different class of samples, such as nickel, gadolinium, Fe$_{2}$VAl and LaCoO$_{3}$. Wide range of S from $\sim$ $-$20 to $\sim$600 $\mu$V/K and $\kappa$ from $\sim$1.1 to $\sim$23.5 W/m-K are studied. The measured values of S and k are in good agreement with the reported data."
"PyLUSAT: An open-source Python toolkit for GIS-based land use suitability analysis Desktop GIS applications, such as ArcGIS and QGIS, provide tools essential for conducting suitability analysis, an activity that is central in formulating a land-use plan. But, when it comes to building complicated land-use suitability models, these applications have several limitations, including operating system-dependence, lack of dedicated modules, insufficient reproducibility, and difficult, if not impossible, deployment on a computing cluster. To address the challenges, this paper introduces PyLUSAT: Python for Land Use Suitability Analysis Tools. PyLUSAT is an open-source software package that provides a series of tools (functions) to conduct various tasks in a suitability modeling workflow. These tools were evaluated against comparable tools in ArcMap 10.4 with respect to both accuracy and computational efficiency. Results showed that PyLUSAT functions were two to ten times more efficient depending on the job's complexity, while generating outputs with similar accuracy compared to the ArcMap tools. PyLUSAT also features extensibility and cross-platform compatibility. It has been used to develop fourteen QGIS Processing Algorithms and implemented on a high-performance computational cluster (HiPerGator at the University of Florida) to expedite the process of suitability analysis. All these properties make PyLUSAT a competitive alternative solution for urban planners/researchers to customize and automate suitability analysis as well as integrate the technique into a larger analytical framework."
"Enabling Un-/Semi-Supervised Machine Learning for MDSE of the Real-World CPS/IoT Applications In this paper, we propose a novel approach to support domain-specific Model-Driven Software Engineering (MDSE) for the real-world use-case scenarios of smart Cyber-Physical Systems (CPS) and the Internet of Things (IoT). We argue that the majority of available data in the nature for Artificial Intelligence (AI), specifically Machine Learning (ML) are unlabeled. Hence, unsupervised and/or semi-supervised ML approaches are the practical choices. However, prior work in the literature of MDSE has considered supervised ML approaches, which only work with labeled training data. Our proposed approach is fully implemented and integrated with an existing state-of-the-art MDSE tool to serve the CPS/IoT domain. Moreover, we validate the proposed approach using a portion of the open data of the REFIT reference dataset for the smart energy systems domain. Our model-to-code transformations (code generators) provide the full source code of the desired IoT services out of the model instances in an automated manner. Currently, we generate the source code in Java and Python. The Python code is responsible for the ML functionalities and uses the APIs of several ML libraries and frameworks, namely Scikit-Learn, Keras and TensorFlow. For unsupervised and semi-supervised learning, the APIs of Scikit-Learn are deployed. In addition to the pure MDSE approach, where certain ML methods, e.g., K-Means, Mini-Batch K-Means, DB-SCAN, Spectral Clustering, Gaussian Mixture Model, Self-Training, Label Propagation and Label Spreading are supported, a more flexible, hybrid approach is also enabled to support the practitioner in deploying a pre-trained ML model with any arbitrary architecture and learning algorithm."
"How to Identify Class Comment Types? A Multi-language Approach for Class Comment Classification Most software maintenance and evolution tasks require developers to understand the source code of their software systems. Software developers usually inspect class comments to gain knowledge about program behavior, regardless of the programming language they are using. Unfortunately, (i) different programming languages present language-specific code commenting notations/guidelines; and (ii) the source code of software projects often lacks comments that adequately describe the class behavior, which complicates program comprehension and evolution activities.   To handle these challenges, this paper investigates the different language-specific class commenting practices of three programming languages: Python, Java, and Smalltalk. In particular, we systematically analyze the similarities and differences of the information types found in class comments of projects developed in these languages.   We propose an approach that leverages two techniques, namely Natural Language Processing and Text Analysis, to automatically identify various types of information from class comments i.e., the specific types of semantic information found in class comments. To the best of our knowledge, no previous work has provided a comprehensive taxonomy of class comment types for these three programming languages with the help of a common automated approach. Our results confirm that our approach can classify frequent class comment information types with high accuracy for Python, Java, and Smalltalk programming languages. We believe this work can help to monitor and assess the quality and evolution of code comments in different program languages, and thus support maintenance and evolution tasks."
"PyCharge: An open-source Python package for self-consistent electrodynamics simulations of Lorentz oscillators and moving point charges PyCharge is a computational electrodynamics Python simulator that can calculate the electromagnetic fields and potentials generated by moving point charges and can self-consistently simulate dipoles modeled as Lorentz oscillators. To calculate the total fields and potentials along a discretized spatial grid at a specified time, PyCharge computes the retarded time of the point charges at each grid point, which are subsequently used to compute the analytical solutions to Maxwell's equations for each point charge. The Lorentz oscillators are driven by the electric field in the system and PyCharge self-consistently determines the reaction of the radiation on the dipole moment at each time step. PyCharge treats the two opposite charges in the dipole as separate point charge sources and calculates their individual contributions to the total electromagnetic fields and potentials. The expected coupling that arises between dipoles is captured in the PyCharge simulation, and the modified radiative properties of the dipoles (radiative decay rate and frequency shift) can be extracted using the dipole's energy at each time step throughout the simulation. The modified radiative properties of two dipoles separated in the near-field, which requires a full dipole response to yield the correct physics, are calculated by PyCharge in excellent agreement with the analytical Green's function results ($< 0.2\%$ relative error, over a wide range of spatial separations). Moving dipoles can also be modeled by specifying the dipole's origin position as a function of time. PyCharge includes a parallelized version of the dipole simulation method to enable the parallel execution of computationally demanding simulations on high performance computing environments to significantly improve run time."
"Advanced modeling of materials with PAOFLOW 2.0: New features and software design Recent research in materials science opens exciting perspectives to design novel quantum materials and devices, but it calls for quantitative predictions of properties which are not accessible in standard first principles packages. PAOFLOW is a software tool that constructs tight-binding Hamiltonians from self-consistent electronic wavefunctions by projecting onto a set of atomic orbitals. The electronic structure provides numerous materials properties that otherwise would have to be calculated via phenomenological models. In this paper, we describe recent re-design of the code as well as the new features and improvements in performance. In particular, we have implemented symmetry operations for unfolding equivalent k-points, which drastically reduces the runtime requirements of first principles calculations, and we have provided internal routines of projections onto atomic orbitals enabling generation of real space atomic orbitals. Moreover, we have included models for non-constant relaxation time in electronic transport calculations, doubling the real space dimensions of the Hamiltonian as well as the construction of Hamiltonians directly from analytical models. Importantly, PAOFLOW has been now converted into a Python package, and is streamlined for use directly within other Python codes. The new object oriented design treats PAOFLOWs computational routines as class methods, providing an API for explicit control of each calculation."
"CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favorable outcomes in the future (e.g., insurance approval). Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available opensource implementations, the decision in favor of certain models is primarily based on what is readily available. Going forward - to guarantee meaningful comparisons across explanation methods - we present CARLA (Counterfactual And Recourse LibrAry), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods. We have open-sourced CARLA and our experimental results on Github, making them available as competitive baselines. We welcome contributions from other research groups and practitioners."
"UniGPS: A Unified Programming Framework for Distributed Graph Processing The industry and academia have proposed many distributed graph processing systems. However, the existing systems are not friendly enough for users like data analysts and algorithm engineers. On the one hand, the programing models and interfaces differ a lot in the existing systems, leading to high learning costs and program migration costs. On the other hand, these graph processing systems are tightly bound to the underlying distributed computing platforms, requiring users to be familiar with distributed computing. To improve the usability of distributed graph processing, we propose a unified distributed graph programming framework UniGPS. Firstly, we propose a unified cross-platform graph programming model VCProg for UniGPS. VCProg hides details of distributed computing from users. It is compatible with the popular graph programming models Pregel, GAS, and Push-Pull. VCProg programs can be executed by compatible distributed graph processing systems without modification, reducing the learning overheads of users. Secondly, UniGPS supports Python as the programming language. We propose an interprocess-communication-based execution environment isolation mechanism to enable Java/C++-based systems to call user-defined methods written in Python. The experimental results show that UniGPS enables users to process big graphs beyond the memory capacity of a single machine without sacrificing usability. UniGPS shows near-linear data scalability and machine scalability."
"The STAR-MELT Python package for emission line analysis of YSOs We introduce the STAR-MELT Python package that we developed to facilitate the analysis of time-resolved emission line spectroscopy of young stellar objects. STAR-MELT automatically extracts, identifies and fits emission lines. We summarise our analysis methods that utilises the time domain of high-resolution stellar spectra to investigate variability in the line profiles and corresponding emitting regions. This allows us to probe the innermost disc and accretion structures of YSOs. Local temperatures and densities can be determined using Boltzmann statistics, the Saha equation, and the Sobolev large velocity gradient approximation. STAR-MELT allows for new results to be obtained from archival data, as well as facilitating timely analysis of new data as it is obtained. We present the results of applying STAR-MELT to three YSOs, using spectra from UVES, XSHOOTER, FEROS, HARPS, and ESPaDOnS. We demonstrate what can be achieved for data with disparate time sampling, for stars with different inclinations and variability types. For EX Lupi, we confirm the presence of a localised and stable stellar-surface hot spot associated with the footprint of the accretion column. For GQ Lupi A, we find that the maximum infall rate from an accretion column is correlated with lines produced in the lowest temperatures. For CVSO109 we investigate the rapid temporal variability of a redshifted emission wing, indicative of rotating and infalling material in the inner disc. Our results show that STAR-MELT is a useful tool for such analysis, as well as other applications for emission lines."
"CMB Anisotropy Constraints on Flat-Lambda and Open CDM Cosmogonies from DMR, UCSB South Pole, Python, ARGO, MAX, White Dish, OVRO, and SuZIE Data We use joint likelihood analyses of combinations of fifteen cosmic microwave background (CMB) anisotropy data sets from the DMR, UCSB South Pole 1994, Python I--III, ARGO, MAX 4 and 5, White Dish, OVRO, and SuZIE experiments to constrain cosmogonies. We consider open and spatially-flat-Lambda cold dark matter cosmogonies, with nonrelativistic-mass density parameter Omega_0 in the range 0.1--1, baryonic-mass density parameter Omega_B in the range (0.005--0.029) h^{-2}, and age of the universe t_0 in the range (10--20) Gyr.   Marginalizing over all parameters but Omega_0, the data favor Omega_0 \simeq 0.9--1 (0.4--0.6) flat-Lambda (open) models. The range in deduced Omega_0 values is partially a consequence of the different combinations of smaller-angular-scale CMB anisotropy data sets used in the analyses, but more significantly a consequence of whether the DMR quadrupole moment is accounted for or ignored in the analysis. For both flat-Lambda and open models, after marginalizing over all other parameters, a lower Omega_B h^2 \simeq 0.005--0.009 is favored. This is also marginally at odds with estimates from more recent CMB anisotropy data and some estimates from standard nucleosynthesis theory and observed light element abundances. For both sets of models a younger universe with t_0 \simeq 12--15 Gyr is favored, consistent with other recent non-CMB indicators. We emphasize that since we consider only a small number of data sets, these results are tentative. More importantly, the analyses here do not rule out the currently favored flat-Lambda model with Omega_0 \sim 0.3, nor the larger Omega_B h^2 values favored by some other data."
"Numerical aspects of eigenvalue and eigenfunction computations for chaotic quantum systems We give an introduction to some of the numerical aspects in quantum chaos. The classical dynamics of two--dimensional area--preserving maps on the torus is illustrated using the standard map and a perturbed cat map. The quantization of area--preserving maps given by their generating function is discussed and for the computation of the eigenvalues a computer program in Python is presented. We illustrate the eigenvalue distribution for two types of perturbed cat maps, one leading to COE and the other to CUE statistics. For the eigenfunctions of quantum maps we study the distribution of the eigenvectors and compare them with the corresponding random matrix distributions. The Husimi representation allows for a direct comparison of the localization of the eigenstates in phase space with the corresponding classical structures. Examples for a perturbed cat map and the standard map with different parameters are shown. Billiard systems and the corresponding quantum billiards are another important class of systems (which are also relevant to applications, for example in mesoscopic physics). We provide a detailed exposition of the boundary integral method, which is one important method to determine the eigenvalues and eigenfunctions of the Helmholtz equation. We discuss several methods to determine the eigenvalues from the Fredholm equation and illustrate them for the stadium billiard. The occurrence of spurious solutions is discussed in detail and illustrated for the circular billiard, the stadium billiard, and the annular sector billiard. We emphasize the role of the normal derivative function to compute the normalization of eigenfunctions, momentum representations or autocorrelation functions in a very efficient and direct way. Some examples for these quantities are given and discussed."
"The MeqTrees software system and its use for third-generation calibration of radio interferometers The formulation of the radio interferometer measurement equation (RIME) by Hamaker et al. has provided us with an elegant mathematical apparatus for better understanding, simulation and calibration of existing and future instruments. The calibration of the new radio telescopes (LOFAR, SKA) would be unthinkable without the RIME formalism, and new software to exploit it. MeqTrees is designed to implement numerical models such as the RIME, and to solve for arbitrary subsets of their parameters. The technical goal of MeqTrees is to provide a tool for rapid implementation of such models, while offering performance comparable to hand-written code. We are also pursuing the wider goal of increasing the rate of evolution of radio astronomical software, by offering a tool for rapid experimentation and exchange of ideas.   MeqTrees is implemented as a Python-based front-end called the meqbrowser, and an efficient (C++-based) computational back-end called the meqserver. Numerical models are defined on the front-end via a Python-based Tree Definition Language (TDL), then rapidly executed on the back-end. The use of TDL facilitates an extremely short turn-around time for experimentation with new ideas. This is also helped by unprecedented visualization capabilities for all final and intermediate results. A flexible data model and a number of important optimizations in the back-end ensures that the numerical performance is comparable to that of hand-written code.   MeqTrees is already widely used as the simulation tool for new instruments (LOFAR, SKA) and technologies (focal plane arrays). It has demonstrated that it can achieve a noise-limited dynamic range in excess of a million, on WSRT data. It is the only package that is specifically designed to handle what we propose to call third-generation calibration (3GC), which is needed for the new generation of giant radio telescopes."
"Developing a comprehensive framework for multimodal feature extraction Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions---ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (video, images, audio, and text), and is expressly with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of complex feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its major advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct sophisticated feature extraction workflows while increasing code clarity and maintainability."
"Modeling the frequency response of microwave radiometers with QUCS Characterization of the frequency response of coherent radiometric receivers is a key element in estimating the flux of astrophysical emissions, since the measured signal depends on the convolution of the source spectral emission with the instrument band shape.   Laboratory Radio Frequency (RF) measurements of the instrument bandpass often require complex test setups and are subject to a number of systematic effects driven by thermal issues and impedance matching, particularly if cryogenic operation is involved.   In this paper we present an approach to modeling radiometers bandpasses by integrating simulations and RF measurements of individual components. This method is based on QUCS (Quasi Universal Circuit Simulator), an open-source circuit simulator, which gives the flexibility of choosing among the available devices, implementing new analytical software models or using measured S-parameters. Therefore an independent estimate of the instrument bandpass is achieved using standard individual component measurements and validated analytical simulations.   In order to automate the process of preparing input data, running simulations and exporting results we developed the Python package python-qucs and released it under GNU Public License.   We discuss, as working cases, bandpass response modeling of the COFE and Planck Low Frequency Instrument (LFI) radiometers and compare results obtained with QUCS and with a commercial circuit simulator software. The main purpose of bandpass modeling in COFE is to optimize component matching, while in LFI they represent the best estimation of frequency response, since end-to-end measurements were strongly affected by systematic effects."
"PyNeb: a new tool for analyzing emission lines. I. Code description and validation of results Analysis of emission lines in gaseous nebulae yields direct measures of physical conditions and chemical abundances and is the cornerstone of nebular astrophysics. Although the physical problem is conceptually simple, its practical complexity can be overwhelming since the amount of data to be analyzed steadily increases; furthermore, results depend crucially on the input atomic data, whose determination also improves each year. To address these challenges we created PyNeb, an innovative code for analyzing emission lines. PyNeb computes physical conditions and ionic and elemental abundances, and produces both theoretical and observational diagnostic plots. It is designed to be portable, modular, and largely customizable in aspects such as the atomic data used, the format of the observational data to be analyzed, and the graphical output. It gives full access to the intermediate quantities of the calculation, making it possible to write scripts tailored to the specific type of analysis one wants to carry out. In the case of collisionally excited lines, PyNeb works by solving the equilibrium equations for an n-level atom; in the case of recombination lines, it works by interpolation in emissivity tables. The code offers a choice of extinction laws and ionization correction factors, which can be complemented by user-provided recipes. It is entirely written in the python programming language and uses standard python libraries. It is fully vectorized, making it apt for analyzing huge amounts of data. The code is stable and has been benchmarked against IRAF/NEBULAR. It is public, fully documented, and has already been satisfactorily used in a number of published papers."
"Astropy: A Community Python Package for Astronomy We present the first public version (v0.2) of the open-source and community-developed Python package, Astropy. This package provides core astronomy-related functionality to the community, including support for domain-specific file formats such as Flexible Image Transport System (FITS) files, Virtual Observatory (VO) tables, and common ASCII table formats, unit and physical quantity conversions, physical constants specific to astronomy, celestial coordinate and time transformations, world coordinate system (WCS) support, generalized containers for representing gridded as well as tabular data, and a framework for cosmological transformations and conversions. Significant functionality is under active development, such as a model fitting framework, VO client and server tools, and aperture and point spread function (PSF) photometry tools. The core development team is actively making additions and enhancements to the current code base, and we encourage anyone interested to participate in the development of future Astropy versions."
"VIDE: The Void IDentification and Examination toolkit We present VIDE, the Void IDentification and Examination toolkit, an open-source Python/C++ code for finding cosmic voids in galaxy redshift surveys and N-body simulations, characterizing their properties, and providing a platform for more detailed analysis. At its core, VIDE uses a substantially enhanced version of ZOBOV (Neyinck 2008) to calculate a Voronoi tessellation for estimating the density field and a performing a watershed transform to construct voids. Additionally, VIDE provides significant functionality for both pre- and post-processing: for example, vide can work with volume- or magnitude-limited galaxy samples with arbitrary survey geometries, or dark matter particles or halo catalogs in a variety of common formats. It can also randomly subsample inputs and includes a Halo Occupation Distribution model for constructing mock galaxy populations. VIDE uses the watershed levels to place voids in a hierarchical tree, outputs a summary of void properties in plain ASCII, and provides a Python API to perform many analysis tasks, such as loading and manipulating void catalogs and particle members, filtering, plotting, computing clustering statistics, stacking, comparing catalogs, and fitting density profiles. While centered around ZOBOV, the toolkit is designed to be as modular as possible and accommodate other void finders. VIDE has been in development for several years and has already been used to produce a wealth of results, which we summarize in this work to highlight the capabilities of the toolkit. VIDE is publicly available at http://bitbucket.org/cosmicvoids/vide public and http://www.cosmicvoids.net."
"Forward Modeling of Large-Scale Structure: An open-source approach with Halotools We present the first stable release of Halotools (v0.2), a community-driven Python package designed to build and test models of the galaxy-halo connection. Halotools provides a modular platform for creating mock universes of galaxies starting from a catalog of dark matter halos obtained from a cosmological simulation. The package supports many of the common forms used to describe galaxy-halo models: the halo occupation distribution (HOD), the conditional luminosity function (CLF), abundance matching, and alternatives to these models that include effects such as environmental quenching or variable galaxy assembly bias. Satellite galaxies can be modeled to live in subhalos, or to follow custom number density profiles within their halos, including spatial and/or velocity bias with respect to the dark matter profile. The package has an optimized toolkit to make mock observations on a synthetic galaxy population, including galaxy clustering, galaxy-galaxy lensing, galaxy group identification, RSD multipoles, void statistics, pairwise velocities and others, allowing direct comparison to observations. Halotools is object-oriented, enabling complex models to be built from a set of simple, interchangeable components, including those of your own creation. Halotools has an automated testing suite and is exhaustively documented on http://halotools.readthedocs.io, which includes quickstart guides, source code notes and a large collection of tutorials. The documentation is effectively an online textbook on how to build and study empirical models of galaxy formation with Python."
"Present and projected sensitivities of Dark Matter direct detection experiments to effective WIMP-nucleus couplings Assuming for Weakly Interacting Massive Particles (WIMPs) a Maxwellian velocity distribution in the Galaxy we explore in a systematic way the relative sensitivity of an extensive set of existing and projected Dark Matter (DM) direct detection experiments to each of the 14 couplings that parameterize the most general non-relativistic (NR) effective Hamiltonian allowed by Galilean invariance for a contact interaction driving the elastic scattering off nuclei of WIMPs of spin 1/2. We perform our analysis in terms of two free parameters: the WIMP mass $m_{\chi}$ and the ratio between the WIMP-neutron and the WIMP-proton couplings $c^n/c^p$. We include the modified signal spectral shape due to non-standard interactions when it is needed in the determination of the bound, such as in the case of background subtraction or of the application of the optimal-interval method. For each coupling, in the $m_{\chi}$-$c^n/c^p$ plane we provide contour plots of the most stringent 90 % C.L. bound on the WIMP-nucleon cross section and show the experiment providing it. We also introduce NRDD_constraints, a simple interpolating code written in Python that allows to obtain the numerical value of the bound as a function of the WIMP mass $m_{\chi}$ and of the coupling ratio $c^n/c^p$ for each NR coupling. We find that 9 experiments out of the 14 present Dark Matter searches considered in our analysis provide the most stringent bound on some of the effective couplings for a given choice of $(m_{\chi},c^n/c^p)$: this is evidence of the complementarity of different target nuclei and/or different combinations of count-rates and energy thresholds when the search of DM is extended to a wide range of possible interactions."
"Artificial intelligence-based process for metal scrap sorting Machine learning offers remarkable benefits for improving workplaces and working conditions amongst others in the recycling industry. Here e.g. hand-sorting of medium value scrap is labor intensive and requires experienced and skilled workers. On the one hand, they have to be highly concentrated for making proper readings and analyses of the material, but on the other hand, this work is monotonous. Therefore, a machine learning approach is proposed for a quick and reliable automated identification of alloys in the recycling industry, while the mere scrap handling is regarded to be left in the hands of the workers. To this end, a set of twelve tool and high-speed steels from the field were selected to be identified by their spectrum induced by electric arcs. For data acquisition, the optical emission spectrometer Thorlabs CCS 100 was used. Spectra have been post-processed to be fed into the supervised machine learning algorithm. The development of the machine learning software is conducted according to the steps of the VDI 2221 standard method. For programming Python 3 as well as the python-library sklearn were used. By systematic parameter variation, the appropriate machine learning algorithm was selected and validated. Subsequent validation steps showed that the automated identification process using a machine learning approach and the optical emission spectrometry is applicable, reaching a maximum F1 score of 96.9 %. This performance is as good as the performance of a highly trained worker using visual grinding spark identification. The tests were based on a self-generated set of 600 spectra per single alloy (7,200 spectra in total) which were produced using an industry workshop device."
"Strong isospin violation and chiral logarithms in the baryon spectrum We present a precise lattice QCD calculation of the contribution to the neutron-proton mass splitting arising from strong isospin breaking, $m_n-m_p|_{QCD}=2.32\pm0.17$ MeV. We also determine $m_{\Xi^-} - m_{\Xi^0}|_{QCD} = 5.44\pm0.31$ MeV. The calculation is performed at three values of the pion mass, with several values of the quark mass splitting and multiple lattice volumes, but only a single lattice spacing and an estimate of discretization errors. The calculations are performed on the anisotropic clover-Wilson ensembles generated by the Hadron Spectrum Collaboration. The omega-baryon mass is used to set the scale $a_t^{-1}=6111\pm127$ MeV, while the kaon masses are used to determine the value of the light-quark mass spitting. The nucleon mass splitting is then determined as a function of the pion mass. We observe, for the first time, conclusive evidence for non-analytic light quark mass dependence in lattice QCD calculations of the baryon spectrum. When left as a free parameter, the fits prefer a nucleon axial coupling of $g_A=1.24(56)$. To highlight the presence of this chiral logarithm in the nucleon mass splitting, we also compute the isospin splitting in the cascade-baryon system which is less sensitive to chiral dynamics. Finally, we update the best lattice QCD determination of the CP-odd pion-nucleon coupling that would arise from a non-zero QCD theta-term, $\bar{g}_0 / (\sqrt{2}f_\pi) = (14.7\pm1.8\pm1.4) \cdot 10^{-3} \bar{\theta}$.   The original lattice QCD correlation functions, analysis results and extrapolated quantities are packaged in HDF5 files made publicly available including a simple Python script to access the numerical results, construct effective mass plots along with our analysis results, and perform the extrapolations of various quantities determined in this work."
"VIP: Vortex Image Processing package for high-contrast direct imaging We present the Vortex Image Processing (VIP) library, a python package dedicated to astronomical high-contrast imaging. Our package relies on the extensive python stack of scientific libraries and aims to provide a flexible framework for high-contrast data and image processing. In this paper, we describe the capabilities of VIP related to processing image sequences acquired using the angular differential imaging (ADI) observing technique. VIP implements functionalities for building high-contrast data processing pipelines, encompass- ing pre- and post-processing algorithms, potential sources position and flux estimation, and sensitivity curves generation. Among the reference point-spread function subtraction techniques for ADI post-processing, VIP includes several flavors of principal component analysis (PCA) based algorithms, such as annular PCA and incremental PCA algorithm capable of processing big datacubes (of several gigabytes) on a computer with limited memory. Also, we present a novel ADI algorithm based on non-negative matrix factorization (NMF), which comes from the same family of low-rank matrix approximations as PCA and provides fairly similar results. We showcase the ADI capabilities of the VIP library using a deep sequence on HR8799 taken with the LBTI/LMIRCam and its recently commissioned L-band vortex coronagraph. Using VIP we investigated the presence of additional companions around HR8799 and did not find any significant additional point source beyond the four known planets. VIP is available at http://github.com/vortex-exoplanet/VIP and is accompanied with Jupyter notebook tutorials illustrating the main functionalities of the library."
"Vaex: Big Data exploration in the era of Gaia We present a new Python library called vaex, to handle extremely large tabular datasets, such as astronomical catalogues like the Gaia catalogue, N-body simulations or any other regular datasets which can be structured in rows and columns. Fast computations of statistics on regular N-dimensional grids allows analysis and visualization in the order of a billion rows per second. We use streaming algorithms, memory mapped files and a zero memory copy policy to allow exploration of datasets larger than memory, e.g. out-of-core algorithms. Vaex allows arbitrary (mathematical) transformations using normal Python expressions and (a subset of) numpy functions which are lazily evaluated and computed when needed in small chunks, which avoids wasting of RAM. Boolean expressions (which are also lazily evaluated) can be used to explore subsets of the data, which we call selections. Vaex uses a similar DataFrame API as Pandas, a very popular library, which helps migration from Pandas. Visualization is one of the key points of vaex, and is done using binned statistics in 1d (e.g. histogram), in 2d (e.g. 2d histograms with colormapping) and 3d (using volume rendering). Vaex is split in in several packages: vaex-core for the computational part, vaex-viz for visualization mostly based on matplotlib, vaex-jupyter for visualization in the Jupyter notebook/lab based in IPyWidgets, vaex-server for the (optional) client-server communication, vaex-ui for the Qt based interface, vaex-hdf5 for hdf5 based memory mapped storage, vaex-astro for astronomy related selections, transformations and memory mapped (column based) fits storage. Vaex is open source and available under MIT license on github, documentation and other information can be found on the main website: https://vaex.io, https://docs.vaex.io or https://github.com/maartenbreddels/vaex"
"In-RDBMS Hardware Acceleration of Advanced Analytics The data revolution is fueled by advances in machine learning, databases, and hardware design. Programmable accelerators are making their way into each of these areas independently. As such, there is a void of solutions that enables hardware acceleration at the intersection of these disjoint fields. This paper sets out to be the initial step towards a unifying solution for in-Database Acceleration of Advanced Analytics (DAnA). Deploying specialized hardware, such as FPGAs, for in-database analytics currently requires hand-designing the hardware and manually routing the data. Instead, DAnA automatically maps a high-level specification of advanced analytics queries to an FPGA accelerator. The accelerator implementation is generated for a User Defined Function (UDF), expressed as a part of an SQL query using a Python-embedded Domain-Specific Language (DSL). To realize an efficient in-database integration, DAnA accelerators contain a novel hardware structure, Striders, that directly interface with the buffer pool of the database. Striders extract, cleanse, and process the training data tuples that are consumed by a multi-threaded FPGA engine that executes the analytics algorithm. We integrate DAnA with PostgreSQL to generate hardware accelerators for a range of real-world and synthetic datasets running diverse ML algorithms. Results show that DAnA-enhanced PostgreSQL provides, on average, 8.3x end-to-end speedup for real datasets, with a maximum of 28.2x. Moreover, DAnA-enhanced PostgreSQL is, on average, 4.0x faster than the multi-threaded Apache MADLib running on Greenplum. DAnA provides these benefits while hiding the complexity of hardware design from data scientists and allowing them to express the algorithm in =30-60 lines of Python."
"CIGALE: a python Code Investigating GALaxy Emission Context. Measuring how the physical properties of galaxies change across cosmic times is essential to understand galaxy formation and evolution. With the advent of numerous ground-based and space-borne instruments launched over the past few decades we now have exquisite multi-wavelength observations of galaxies from the FUV to the radio domain. To tap into this mine of data and obtain new insight into the formation and evolution of galaxies, it is essential that we are able to extract information from their SED. Aims. We present a completely new implementation of CIGALE. Written in python, its main aims are to easily and efficiently model the FUV to radio spectrum of galaxies and estimate their physical properties such as star formation rate, attenuation, dust luminosity, stellar mass, and many other physical quantities. Methods. To compute the spectral models, CIGALE builds composite stellar populations from simple stellar populations combined with highly flexible star formation histories, calculates the emission from gas ionised by massive stars, and attenuates both the stars and the ionised gas with a highly flexible attenuation curve. Based on an energy balance principle, the absorbed energy is then re-emitted by the dust in the mid- and far-infrared domains while thermal and non-thermal components are also included, extending the spectrum far into the radio range. A large grid of models is then fitted to the data and the physical properties are estimated through the analysis of the likelihood distribution. Results. CIGALE is a versatile and easy-to-use tool that makes full use of the architecture of multi-core computers, building grids of millions of models and analysing samples of thousands of galaxies, both at high speed. Beyond fitting the SEDs of galaxies and parameter estimations, it can also be used as a model-generation tool or serve as a library to build new applications."
"PynPoint: a modular pipeline architecture for processing and analysis of high-contrast imaging data The direct detection and characterization of planetary and substellar companions at small angular separations is a rapidly advancing field. Dedicated high-contrast imaging instruments deliver unprecedented sensitivity, enabling detailed insights into the atmospheres of young low-mass companions. In addition, improvements in data reduction and PSF subtraction algorithms are equally relevant for maximizing the scientific yield, both from new and archival data sets. We aim at developing a generic and modular data reduction pipeline for processing and analysis of high-contrast imaging data obtained with pupil-stabilized observations. The package should be scalable and robust for future implementations and in particular well suitable for the 3-5 micron wavelength range where typically (ten) thousands of frames have to be processed and an accurate subtraction of the thermal background emission is critical. PynPoint is written in Python 2.7 and applies various image processing techniques, as well as statistical tools for analyzing the data, building on open-source Python packages. The current version of PynPoint has evolved from an earlier version that was developed as a PSF subtraction tool based on PCA. The architecture of PynPoint has been redesigned with the core functionalities decoupled from the pipeline modules. Modules have been implemented for dedicated processing and analysis steps, including background subtraction, frame registration, PSF subtraction, photometric and astrometric measurements, and estimation of detection limits. The pipeline package enables end-to-end data reduction of pupil-stabilized data and supports classical dithering and coronagraphic data sets. As an example, we processed archival VLT/NACO L' and M' data of beta Pic b and reassessed the planet's brightness and position with an MCMC analysis, and we provide a derivation of the photometric error budget."
"petitRADTRANS: a Python radiative transfer package for exoplanet characterization and retrieval We present the easy-to-use, publicly available, Python package petitRADTRANS, built for the spectral characterization of exoplanet atmospheres. The code is fast, accurate, and versatile; it can calculate both transmission and emission spectra within a few seconds at low resolution ($\lambda/\Delta\lambda$ = 1000; correlated-k method) and high resolution ($\lambda/\Delta\lambda = 10^6$; line-by-line method), using only a few lines of input instruction. The somewhat slower correlated-k method is used at low resolution because it is more accurate than methods such as opacity sampling. Clouds can be included and treated using wavelength-dependent power law opacities, or by using optical constants of real condensates, specifying either the cloud particle size, or the atmospheric mixing and particle settling strength. Opacities of amorphous or crystalline, spherical or irregularly-shaped cloud particles are available. The line opacity database spans temperatures between 80 and 3000 K, allowing to model fluxes of objects such as terrestrial planets, super-Earths, Neptunes, or hot Jupiters, if their atmospheres are hydrogen-dominated. Higher temperature points and species will be added in the future, allowing to also model the class of ultra hot-Jupiters, with equilibrium temperatures $T_{\rm eq} \gtrsim 2000$ K. Radiative transfer results were tested by cross-verifying the low- and high-resolution implementation of petitRADTRANS, and benchmarked with the petitCODE, which itself is also benchmarked to the ATMO and Exo-REM codes. We successfully carried out test retrievals of synthetic JWST emission and transmission spectra (for the hot Jupiter TrES-4b, which has a $T_{\rm eq}$ of $\sim$ 1800 K). The code is publicly available at http://gitlab.com/mauricemolli/petitRADTRANS, and its documentation can be found at https://petitradtrans.readthedocs.io."
"The GIST Pipeline: A Multi-Purpose Tool for the Analysis and Visualisation of (Integral-field) Spectroscopic Data We present a convenient, all-in-one framework for the scientific analysis of fully reduced, (integral-field) spectroscopic data. The GIST pipeline (Galaxy IFU Spectroscopy Tool) is entirely written in Python3 and conducts all steps from the preparation of input data, over the scientific analysis to the production of publication-quality plots. In its basic setup, it extracts stellar kinematics, performs an emission-line analysis and derives stellar population properties from full spectral fitting as well as via the measurement of absorption line-strength indices by exploiting the well-known pPXF and GandALF routines, where the latter has now been implemented in Python. The pipeline is not specific to any instrument or analysis technique and provides easy means of modification and further development, as of its modular code architecture. An elaborate, Python-native parallelisation is implemented and tested on various machines. The software further features a dedicated visualization routine with a sophisticated graphical user interface. This allows an easy, fully-interactive plotting of all measurements, spectra, fits, and residuals, as well as star formation histories and the weight distribution of the models. The pipeline has successfully been applied to both low and high-redshift data from MUSE, PPAK (CALIFA), and SINFONI, as well as to simulated data for HARMONI@ELT and WEAVE and is currently being used by the TIMER, Fornax3D, and PHANGS collaborations. We demonstrate its capabilities by applying it to MUSE TIMER observations of NGC 1433."
"Automatic microscopic image analysis by moving window local Fourier Transform and Machine Learning Analysis of microscope images is a tedious work which requires patience and time, usually done manually by the microscopist after data collection. Here we introduce an approach of automatic image analysis, which is based on locally applied Fourier Transform and Machine Learning methods. In this approach, a whole image is scanned by a local moving window with defined size and the 2D Fourier Transform is calculated for each window. Then, all the Local Fourier Transforms are fed into Machine Learning processing. Firstly, a number of components in the data is estimated from Principal Component Analysis (PCA) Scree Plot performed on the data. Secondly, the data are decomposed blindly by Non-Negative Matrix Factorization (NMF) into interpretable spatial maps (loadings) and corresponding Fourier Transforms (factors). The microscopic image is analyzed and the features on the image are automatically discovered, based on the local changes in Fourier Transform. The user selects only a size and movement of the scanning local window which defines the final analysis resolution. This automatic approach was successfully applied to analysis of various microscopic images with and without local periodicity i.e. atomically resolved High Angle Annular Dark Field (HAADF) Scanning Transmission Electron Microscopy (STEM) image of Au nanoisland of fcc and Au hcp phases, Scanning Tunneling Microscopy (STM) image of Au-induced reconstruction on Ge(001) surface, Scanning Electron Microscopy (SEM) image of metallic nanoclusters grown on GaSb surface, and Fluorescence microscopy image of HeLa cell line of cervical cancer. The proposed approach could be used to automatically analyze the local structure of microscopic images within a time of about a minute for a single image on a modern desktop/notebook computer and it is freely available as a Python analysis notebook and Python program for batch processing."
"Fitting the nonlinear matter bispectrum by the Halofit approach We provide a new fitting formula of the matter bispectrum in the nonlinear regime calibrated by high-resolution cosmological $N$-body simulations of $41$ cold dark matter ($w$CDM, $w=$ constant) models around the Planck 2015 best-fit parameters. As the parameterization in our fitting function is similar to that in Halofit, our fitting is named BiHalofit. The simulation volume is sufficiently large ($> 10 \, {\rm Gpc}^3$) to cover almost all measurable triangle bispectrum configurations in the universe. The function is also calibrated using one-loop perturbation theory at large scales ($k<0.3 \, h \, {\rm Mpc}^{-1}$). Our formula reproduced the matter bispectrum to within $10 \, (15) \, \%$ accuracy in the Planck 2015 model at wavenumber $k< 3 \, (10) \, h \, {\rm Mpc}^{-1}$ and redshifts $z=0-3$. The other $40$ $w$CDM models obtained poorer fits, with accuracy approximating $20 \, \%$ at $k<3 \, h \, {\rm Mpc}^{-1}$ and $z=0-1.5$ (the deviation includes the $10 \, \%$-level sample variance of the simulations). We also provide a fitting formula that corrects the baryonic effects such as radiative cooling and active galactic nucleus feedback, using the latest hydrodynamical simulation IllustrisTNG. We demonstrate that our new formula more accurately predicts the weak-lensing bispectrum than the existing fitting formulas. This formula will assist current and future weak-lensing surveys and cosmic microwave background lensing experiments. Numerical codes of the formula are available, written in Python, C and Fortran."
"Ask2Me VarHarmonizer: A Python-Based Tool to Harmonize Variants from Cancer Genetic Testing Reports and Map them to the ClinVar Database PURPOSE: The popularity of germline genetic panel testing has led to a vast accumulation of variant-level data. Variant names are not always consistent across laboratories and not easily mappable to public variant databases such as ClinVar. A tool that can automate the process of variants harmonization and mapping is needed to help clinicians ensure their variant interpretations are accurate. METHODS: We present a Python-based tool, Ask2Me VarHarmonizer, that incorporates data cleaning, name harmonization, and a four-attempt mapping to ClinVar procedure. We applied this tool to map variants from a pilot dataset collected from 11 clinical practices. Mapping results were evaluated with and without the transcript information. RESULTS: Using Ask2Me VarHarmonizer, 4728 out of 6027 variant entries (78%) were successfully mapped to ClinVar, corresponding to 3699 mappable unique variants. With the addition of 1099 unique unmappable variants, a total of 4798 unique variants were eventually identified. 427 (9%) of these had multiple names, of which 343 (7%) had multiple names within-practice. 99% mapping consistency was observed with and without transcript information. CONCLUSION: Ask2Me VarHarmonizer aggregates and structures variant data, harmonizes names, and maps variants to ClinVar. Performing harmonization removes the ambiguity and redundancy of variants from different sources."
"The Baghdad Atlas: A relational database of inelastic neutron-scattering $(n,n')$ data A relational database has been developed based on the original ($n,n'\gamma$) work carried out by A. M. Demidov $et$ $al$., at the Nuclear Research Institute in Baghdad, Iraq [$""Atlas$ $of$ $Gamma$-$Ray$ $Spectra$ $from$ $the$ $Inelastic$ $Scattering$ $of$ $Reactor$ $Fast$ $Neutrons""$, Nuclear Research Institute, Baghdad, Iraq (Moscow, Atomizdat 1978)] for 105 independent measurements comprising 76 elemental samples of natural composition and 29 isotopically-enriched samples. The information from this Atlas includes: $\gamma$-ray energies and relative intensities; nuclide and level data corresponding to the residual nucleus and meta data associated with the target sample that allows for the extraction of the flux-weighted ($n,n'\gamma$) cross sections for a given transition relative to a defined value. The optimized angular-distribution-corrected fast-neutron flux-weighted partial $\gamma$-ray cross section for the production of the 846.8-keV $2^{+}_{1} \rightarrow 0^{+}_{\rm gs}$ $\gamma$-ray transition in $^{56}$Fe, determined to be $\langle \sigma_{\gamma} \rangle = 143(29)$ mb, is used for this purpose. However, different values for the adopted cross section can be readily implemented to accommodate user preference based on revised determinations of this quantity. The Atlas ($n,n'\gamma$) data has been compiled into a series of CSV-style ASCII data sets and a suite of Python scripts have been developed to build and install the database locally. The database can then be accessed directly through the SQLite engine, or using alternative methods such as the Jupyter Notebook Python-browser interface. Several examples exploiting different interaction methodologies are distributed with the complete software package."
"Lightweight Lexical Test Prioritization for Immediate Feedback The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes.   The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight.   In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \emph{TF-IDF} retrieval model tailored to retrieving failing tests.   To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages.   We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle."
"PEtab -- interoperable specification of parameter estimation problems in systems biology Reproducibility and reusability of the results of data-based modeling studies are essential. Yet, there has been -- so far -- no broadly supported format for the specification of parameter estimation problems in systems biology. Here, we introduce PEtab, a format which facilitates the specification of parameter estimation problems using Systems Biology Markup Language (SBML) models and a set of tab-separated value files describing the observation model and experimental data as well as parameters to be estimated. We already implemented PEtab support into eight well-established model simulation and parameter estimation toolboxes with hundreds of users in total. We provide a Python library for validation and modification of a PEtab problem and currently 20 example parameter estimation problems based on recent studies. Specifications of PEtab, the PEtab Python library, as well as links to examples, and all supporting software tools are available at https://github.com/PEtab-dev/PEtab, a snapshot is available at https://doi.org/10.5281/zenodo.3732958. All original content is available under permissive licenses."
"The Effective Halo Model: Creating a Physical and Accurate Model of the Matter Power Spectrum and Cluster Counts We introduce a physically-motivated model of the matter power spectrum, based on the halo model and perturbation theory. This model achieves 1\% accuracy on all $k-$scales between $k=0.02h\,\mathrm{Mpc}^{-1}$ to $k=1h\,\mathrm{Mpc}^{-1}$. Our key ansatz is that the number density of halos depends on the non-linear density contrast filtered on some unknown scale $R$. Using the Effective Field Theory of Large Scale Structure to evaluate the two-halo term, we obtain a model for the power spectrum with only two fitting parameters: $R$ and the effective `sound speed', which encapsulates small-scale physics. This is tested with two suites of cosmological simulations across a broad range of cosmologies and found to be highly accurate. Due to its physical motivation, the statistics can be easily extended beyond the power spectrum; we additionally derive the one-loop covariance matrices of cluster counts and their combination with the matter power spectrum. This yields a significantly better fit to simulations than previous models, and includes a new model for super-sample effects, which is rigorously tested with separate universe simulations. At low redshift, we find a significant ($\sim 10\%$) exclusion covariance from accounting for the finite size of halos which has not previously been modeled. Such power spectrum and covariance models will enable joint analysis of upcoming large-scale structure surveys, gravitational lensing surveys and cosmic microwave background maps on scales down to the non-linear scale. We provide a publicly released Python code."
"A Fortran-Keras Deep Learning Bridge for Scientific Computing Implementing artificial neural networks is commonly achieved via high-level programming languages like Python and easy-to-use deep learning libraries like Keras. These software libraries come pre-loaded with a variety of network architectures, provide autodifferentiation, and support GPUs for fast and efficient computation. As a result, a deep learning practitioner will favor training a neural network model in Python, where these tools are readily available. However, many large-scale scientific computation projects are written in Fortran, making it difficult to integrate with modern deep learning methods. To alleviate this problem, we introduce a software library, the Fortran-Keras Bridge (FKB). This two-way bridge connects environments where deep learning resources are plentiful, with those where they are scarce. The paper describes several unique features offered by FKB, such as customizable layers, loss functions, and network ensembles.   The paper concludes with a case study that applies FKB to address open questions about the robustness of an experimental approach to global climate simulation, in which subgrid physics are outsourced to deep neural network emulators. In this context, FKB enables a hyperparameter search of one hundred plus candidate models of subgrid cloud and radiation physics, initially implemented in Keras, to be transferred and used in Fortran. Such a process allows the model's emergent behavior to be assessed, i.e. when fit imperfections are coupled to explicit planetary-scale fluid dynamics. The results reveal a previously unrecognized strong relationship between offline validation error and online performance, in which the choice of optimizer proves unexpectedly critical. This reveals many neural network architectures that produce considerable improvements in stability including some with reduced error, for an especially challenging training dataset."
"Completeness of the Gaia-verse II: what are the odds that a star is missing from Gaia DR2? The second data release of the Gaia mission contained astrometry and photometry for an incredible 1,692,919,135 sources, but how many sources did Gaia miss and where do they lie on the sky? The answer to this question will be crucial for any astronomer attempting to map the Milky Way with Gaia DR2. We infer the completeness of Gaia DR2 by exploiting the fact that it only contains sources with at least five astrometric detections. The odds that a source achieves those five detections depends on both the number of observations and the probability that an observation of that source results in a detection. We predict the number of times that each source was observed by Gaia and assume that the probability of detection is either a function of magnitude or a distribution as a function of magnitude. We fit both these models to the 1.7 billion stars of Gaia DR2, and thus are able to robustly predict the completeness of Gaia across the sky as a function of magnitude. We extend our selection function to account for crowding in dense regions of the sky, and show that this is vitally important, particularly in the Galactic bulge and the Large and Small Magellanic Clouds. We find that the magnitude limit at which Gaia is still 99% complete varies over the sky from $G=18.9$ to $21.3$. We have created a new Python package selectionfunctions (https://github.com/gaiaverse/selectionfunctions) which provides easy access to our selection functions."
"Environmental effects with Frozen Density Embedding in Real-Time Time-Dependent Density Functional Theory using localized basis functions Frozen Density Embedding (FDE) represents a versatile embedding scheme to describe the environmental effect on the electron dynamics in molecular systems. The extension of the general theory of FDE to the real-time time-dependent Kohn-Sham method has previously been presented and implemented in plane-waves and periodic boundary conditions (Pavanello et al. J. Chem. Phys. 142, 154116, 2015). In the current paper, we extend our recent formulation of real-time time-dependent Kohn-Sham method based on localized basis set functions and developed within the Psi4NumPy framework (De Santis et al. J. Chem. Theory Comput. 2020, 16, 2410) to the FDE scheme. The latter has been implemented in its ""uncoupled"" flavor (in which the time evolution is only carried out for the active subsystem, while the environment subsystems remain at their ground state), using and adapting the FDE implementation already available in the PyEmbed module of the scripting framework PyADF. The implementation was facilitated by the fact that both Psi4NumPy and PyADF, being native Python API, provided an ideal framework of development using the Python advantages in terms of code readability and reusability. We demonstrate that the inclusion of the FDE potential does not introduce any numerical instability in time propagation of the density matrix of the active subsystem and in the limit of weak external field, the numerical results for low-lying transition energies are consistent with those obtained using the reference FDE calculations based on the linear response TDDFT. The method is found to give stable numerical results also in the presence of strong external field inducing non-linear effects."
"Array Programming with NumPy Array programming provides a powerful, compact, expressive syntax for accessing, manipulating, and operating on data in vectors, matrices, and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It plays an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, material science, engineering, finance, and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves and the first imaging of a black hole. Here we show how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring, and analyzing scientific data. NumPy is the foundation upon which the entire scientific Python universe is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Because of its central position in the ecosystem, NumPy increasingly plays the role of an interoperability layer between these new array computation libraries."
"Jupyter Notebooks on GitHub: Characteristics and Code Clones Jupyter notebooks has emerged as a standard tool for data science programming. Programs in Jupyter notebooks are different from typical programs as they are constructed by a collection of code snippets interleaved with text and visualisation. This allows interactive exploration and snippets may be executed in different order which may give rise to different results due to side-effects between snippets. Previous studies have shown the presence of considerable code duplication -- code clones -- in sources of traditional programs, in both so-called systems programming languages and so-called scripting languages. In this paper we present the first large-scale study of code cloning in Jupyter notebooks. We analyse a corpus of 2.7 million Jupyter notebooks hosted on GitHJub, representing 37 million individual snippets and 227 million lines of code. We study clones at the level of individual snippets, and study the extent to which snippets are recurring across multiple notebooks. We study both identical clones and approximate clones and conduct a small-scale ocular inspection of the most common clones. We find that code cloning is common in Jupyter notebooks -- more than 70% of all code snippets are exact copies of other snippets (with possible differences in white spaces), and around 50% of all notebooks do not have any unique snippet, but consists solely of snippets that are also found elsewhere. In notebooks written in Python, at least 80% of all snippets are approximate clones and the prevalence of code cloning is higher in Python than in other languages. We further find that clones between different repositories are far more common than clones within the same repository. However, the most common individual repository from which a Jupyter notebook contains clones is the repository in which itself resides."
"SPInS, a pipeline for massive stellar parameter inference: A public Python tool to age-date, weigh, size up stars, and more Stellar parameters are required in a variety of contexts, ranging from the characterisation of exoplanets to Galactic archaeology. Among them, the age of stars cannot be directly measured, while the mass and radius can be measured in some particular cases (binary systems, interferometry). Stellar ages, masses, and radii have to be inferred from stellar evolution models by appropriate techniques. We have designed a Python tool named SPInS. It takes a set of photometric, spectroscopic, interferometric, and/or asteroseismic observational constraints and, relying on a stellar model grid, provides the age, mass, and radius of a star, among others, as well as error bars and correlations. We make the tool available to the community via a dedicated website. SPInS uses a Bayesian approach to find the PDF of stellar parameters from a set of classical constraints. At the heart of the code is a MCMC solver coupled with interpolation within a pre-computed stellar model grid. Priors can be considered, such as the IMF or SFR. SPInS can characterise single stars or coeval stars, such as members of binary systems or of stellar clusters. We illustrate the capabilities of SPInS by studying stars that are spread over the Hertzsprung-Russell diagram. We then validate the tool by inferring the ages and masses of stars in several catalogues and by comparing them with literature results. We show that in addition to the age and mass, SPInS can efficiently provide derived quantities, such as the radius, surface gravity, and seismic indices. We demonstrate that SPInS can age-date and characterise coeval stars that share a common age and chemical composition. The SPInS tool will be very helpful in preparing and interpreting the results of large-scale surveys, such as the wealth of data expected or already provided by space missions, such as Gaia, Kepler, TESS, and PLATO."
"kMap.py: A Python program for simulation and data analysis in photoemission tomography For organic molecules adsorbed as well-oriented ultra-thin films on metallic surfaces, angle-resolved photoemission spectroscopy has evolved into a technique called photoemission tomography (PT). By approximating the final state of the photoemitted electron as a free electron, PT uses the angular dependence of the photocurrent, a so-called momentum map or k-map, and interprets it as the Fourier transform of the initial state's molecular orbital, thereby gains insights into the geometric and electronic structure of organic/metal interfaces.   In this contribution, we present kMap.py which is a Python program that enables the user, via a PyQt-based graphical user interface, to simulate photoemission momentum maps of molecular orbitals and to perform a one-to-one comparison between simulation and experiment. Based on the plane wave approximation for the final state, simulated momentum maps are computed numerically from a fast Fourier transform of real space molecular orbital distributions, which are used as program input and taken from density functional calculations. The program allows the user to vary a number of simulation parameters such as the final state kinetic energy, the molecular orientation or the polarization state of the incident light field. Moreover, also experimental photoemission data can be loaded into the program enabling a direct visual comparison as well as an automatic optimization procedure to determine structural parameters of the molecules or weights of molecular orbitals contributions. With an increasing number of experimental groups employing photoemission tomography to study adsorbate layers, we expect kMap.py to serve as an ideal analysis software to further extend the applicability of PT."
"A Framework for Multiphase Galactic Wind Launching using TIGRESS Galactic outflows have density, temperature, and velocity variations at least as large as that of the multiphase, turbulent interstellar medium (ISM) from which they originate. We have conducted a suite of parsec-resolution numerical simulations using the TIGRESS framework, in which outflows emerge as a consequence of interaction between supernovae (SNe) and the star-forming ISM. The outflowing gas is characterized by two distinct thermal phases, cool (T<10^4 K) and hot (T>10^6 K), with most mass carried by the cool phase and most energy and newly-injected metals carried by the hot phase. Both components have a broad distribution of outflow velocity, and especially for cool gas this implies a varying fraction of escaping material depending on the halo potential. Informed by the TIGRESS results, we develop straightforward analytic formulae for the joint probability density functions (PDFs) of mass, momentum, energy, and metal loading as distributions in outflow velocity and sound speed. The model PDFs have only two parameters, SFR surface density \Sigma_SFR and the metallicity of the ISM, and fully capture the behavior of the original TIGRESS simulation PDFs over \Sigma_SFR~(10^{-4},1)M_sun/kpc^2/yr. Employing PDFs from resolved simulations will enable galaxy formation subgrid model implementations with wind velocity and temperature (as well as total loading factors) that are based on theoretical predictions rather than empirical tuning. This is a critical step to incorporate advances from TIGRESS and other high-resolution simulations in future cosmological hydrodynamics and semi-analytic galaxy formation models. We release a python package to prototype our model and to ease its implementation."
"MIRISim: A Simulator for the Mid-Infrared Instrument on JWST The Mid-Infrared Instrument (MIRI) on the James Webb Space Telescope (JWST), has imaging, four coronagraphs and both low and medium resolution spectroscopic modes . Being able to simulate MIRI observations will help commissioning of the instrument, as well as get users familiar with representative data. We designed the MIRI instrument simulator (MIRISim) to mimic the on-orbit performance of the MIRI imager and spectrometers using the Calibration Data Products (CDPs) developed by the MIRI instrument team. The software encorporates accurate representations of the detectors, slicers, distortions, and noise sources along the light path including the telescope's radiative background and cosmic rays. The software also includes a module which enables users to create astronomical scenes to simulate. MIRISim is a publicly available Python package that can be run at the command line, or from within Python. The outputs of MIRISim are detector images in the same uncalibrated data format that will be delivered to MIRI users. These contain the necessary metadata for ingestion by the JWST calibration pipeline."
"netrd: A library for network reconstruction and graph distances Over the last two decades, alongside the increased availability of large network datasets, we have witnessed the rapid rise of network science. For many systems, however, the data we have access to is not a direct description of the underlying network. More and more, we see the drive to study networks that have been inferred or reconstructed from non-network data---in particular, using time series data from the nodes in a system to infer likely connections between them. Selecting the most appropriate technique for this task is a challenging problem in network science. Different reconstruction techniques usually have different assumptions, and their performance varies from system to system in the real world. One way around this problem could be to use several different reconstruction techniques and compare the resulting networks. However, network comparison is also not an easy problem, as it is not obvious how best to quantify the differences between two networks, in part because of the diversity of tools for doing so. The netrd Python package seeks to address these two parallel problems in network science by providing, to our knowledge, the most extensive collection of both network reconstruction techniques and network comparison techniques (often referred to as graph distances) in a single library (https://github.com/netsiphd/netrd). In this article, we detail the two main functionalities of the netrd package. Along the way, we describe some of its other useful features. This package builds on commonly used Python packages and is already a widely used resource for network scientists and other multidisciplinary researchers. With ongoing open-source development, we see this as a tool that will continue to be used by all sorts of researchers to come."
"Transparent Compiler and Runtime Specializations for Accelerating Managed Languages on FPGAs In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging from low-power embedded systems to high-end data centers and cloud infrastructures.   However, these hardware characteristics come at the cost of programmability. Developers who create their applications using high-level programming languages (e.g., Java, Python, etc.) are required to familiarize with a hardware description language (e.g., VHDL, Verilog) or recently heterogeneous programming models (e.g., OpenCL, HLS) in order to exploit the co-processors? capacity and tune the performance of their applications. Currently, the above-mentioned heterogeneous programming models support exclusively the compilation from compiled languages, such as C and C++. Thus, the transparent integration of heterogeneous co-processors to the software ecosystem of managed programming languages (e.g. Java, Python) is not seamless.   In this paper we rethink the engineering trade-offs that we encountered, in terms of transparency and compilation overheads, while integrating FPGAs into high-level managed programming languages. We present a novel approach that enables runtime code specialization techniques for seamless and high-performance execution of Java programs on FPGAs. The proposed solution is prototyped in the context of the Java programming language and TornadoVM; an open-source programming framework for Java execution on heterogeneous hardware. Finally, we evaluate the proposed solution for FPGA execution against both sequential and multi-threaded Java implementations showcasing up to 224x and 19.8x performance speedups, respectively, and up to 13.82x compared to TornadoVM running on an Intel integrated GPU. We also provide a break-down analysis of the proposed compiler optimizations for FPGA execution, as a means to project their impact on the applications? characteristics."
"Lake symbols for island parsing Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG).   Inquiry: In practice, however, the grammar rules are often complicated since the developer must define the water inside the island; otherwise, the island parsing will not reduce the total number of grammar rules. When describing the grammar rules for such water, the developer must consider other rules and enumerate a set of symbols, which we call alternative symbols. Due to this difficulty, island parsing seems to be not widely used today despite its usefulness in many applications.   Approach: This paper proposes the lake symbols for addressing this difficulty in developing an island parser. It also presents an extension to PEG for supporting the lake symbols. The lake symbols automate the enumeration of the alternative symbols for the water inside an island. The paper proposes an algorithm for translating the extended PEG to the normal PEG, which can be given to an existing parser generator based on PEG.   Knowledge: The user can use lake symbols to define water without specifying each alternative symbol. Our algorithms can calculate all alternative symbols for a lake symbol, based on where the lake symbol is used in the grammar.   Grounding: We implemented a parser generator accepting our extended PEG and implemented 36 island parsers for Java and 20 island parsers for Python. Our experiments show that the lake symbols reduce 42 % of grammar rules for Java and 89 % of rules for Python on average, excluding the case where islands are expressions.   Importance: This work eases the use of island parsing. Lake symbols enable the user to define the water inside the island simpler than before. Defining water inside the island is essential to apply island parsing for practical programming languages."
"PBjam: A Python package for automating asteroseismology of solar-like oscillators Asteroseismology is an exceptional tool for studying stars by using the properties of observed modes of oscillation. So far the process of performing an asteroseismic analysis of a star has remained somewhat esoteric and inaccessible to non-experts. In this software paper we describe PBjam, an open-source Python package for analyzing the frequency spectra of solar-like oscillators in a simple but principled and automated way. The aim of PBjam is to provide a set of easy-to-use tools to extract information about the radial and quadrupole oscillations in stars that oscillate like the Sun, which may then be used to infer bulk properties such as stellar mass, radius and age or even structure. Asteroseismology and its data analysis methods are becoming increasingly important as space-based photometric observatories are producing a wealth of new data, allowing asteroseismology to be applied in a wide range of contexts such as exoplanet, stellar structure and evolution, and Galactic population studies."
"Unifying of Inflation with Early and Late Dark Energy Epochs in Axion $F(R)$ Gravity We provide a theoretical model of $F(R)$ gravity in which it is possible to describe in a unified way inflation, an early and a late dark energy era, in the presence of a light axion particle which plays the role of the dark matter component of the Universe. Particularly, the early-time phenomenology is dominated by an $R^2$ term, while the presence of the other terms $f(R)$ ensure the occurrence of the early and late-time dark energy eras. The inflationary phenomenology is compatible with the Planck 2018 data for inflation, while the late-time dark energy era is compatible with the Planck 2018 constraints on the cosmological parameters. Also, the model exhibits an early dark energy era, at $z\sim 2.5$ approximately, followed by a deceleration era, which starts at approximately $z\sim 1.5$, which in turn is followed by a late-time dark energy era for redshifts $z\sim 0.5$, which lasts for approximately 5 billion years up to present time. A notable feature of our model is that the dark energy era is free from dark energy oscillations, at least in the redshift interval $z=[0,10]$. In addition, we also discuss several features related to observational data at $z\sim 2.34$, at which redshift intricate observational data exist in the literature. Moreover, the numerical code for the dark energy phenomenology, written in Python 3, is presented in the end of the article. Finally, the model has another interesting characteristic, a sudden jump of the value of the Hubble rate in the redshift interval $z\sim [2,2.6]$ where its value suddenly increases and then decreases until $z\sim 0$."
"MechElastic: A Python Library for Analysis of Mechanical and Elastic Properties of Bulk and 2D Materials The MechElastic Python package evaluates the mechanical and elastic properties of bulk and 2D materials using the elastic coefficient matrix ($C_{ij}$) obtained from any ab-initio density-functional theory (DFT) code. The current version of this package reads the output of VASP, ABINIT, and Quantum Espresso codes (but it can be easily generalized to any other DFT code) and performs the appropriate post-processing of elastic constants as per the requirement of the user. This program can also detect the input structure's crystal symmetry and test the mechanical stability of all crystal classes using the Born-Huang criteria. Various useful material-specific properties such as elastic moduli, longitudinal and transverse elastic wave velocities, Debye temperature, elastic anisotropy, 2D layer modulus, hardness, Pugh's ratio, Cauchy's pressure, Kleinman parameter, and Lame's coefficients, can be estimated using this program. Another existing feature of this program is to employ the ELATE package [J. Phys.: Condens. Matter 28, 275201 (2016)] and plot the spatial variation of several elastic properties such as Poisson's ratio, linear compressibility, shear modulus, and Young's modulus in three dimensions. Further, the MechElastic package can plot the equation of state (EOS) curves for energy and pressure for a variety of EOS models such as Murnaghan, Birch, Birch-Murnaghan, and Vinet, by reading the inputted energy/pressure versus volume data obtained via numerical calculations or experiments. This package is particularly useful for the high-throughput analysis of elastic and mechanical properties of materials."
"DarpanX: A Python Package for Modeling X-ray Reflectivity of Multilayer Mirrors Multilayer X-ray mirrors consist of a coating of a large number of alternate layers of high Z and low Z materials with a typical thickness of 10-100 Angstrom, on a suitable substrate. Such coatings play an important role in enhancing the reflectivity of X-ray mirrors by allowing reflections at angles much larger than the critical angle of X-ray reflection for the given materials. Coating with an equal thickness of each bilayer enhances the reflectivity at discrete energies, satisfying Bragg condition. However, by systematically varying the bilayer thickness in the multilayer stack, it is possible to design X-ray mirrors having enhanced reflectivity over a broad energy range. One of the most important applications of such a depth graded multilayer mirror is to realize hard X-ray telescopes for astronomical purposes. Design of such multilayer X-ray mirrors and their characterization with X-ray reflectivity measurements require appropriate software tools. We have initiated the development of hard X-ray optics for future Indian X-ray astronomical missions, and in this context, we have developed a program, DarpanX, to calculate X-ray reflectivity for single and multilayer mirrors. It can be used as a stand-alone tool for designing multilayer mirrors with required characteristics. But more importantly, it has been implemented as a local model for the popular X-ray spectral fitting program, XSPEC, and thus can be used for accurate fitting of the experimentally measured X-ray reflectivity data. DarpanX is implemented as a Python 3 module, and an API is provided to access the underlying algorithms. Here we present details of DarpanX implementation and its validation for different type multilayer structures. We also demonstrate the model fitting capability of DarpanX for experimental X-ray reflectivity measurements of single and multilayer samples."
"Fast and scalable likelihood maximization for Exponential Random Graph Models with local constraints Exponential Random Graph Models (ERGMs) have gained increasing popularity over the years. Rooted into statistical physics, the ERGMs framework has been successfully employed for reconstructing networks, detecting statistically significant patterns in graphs, counting networked configurations with given properties. From a technical point of view, the ERGMs workflow is defined by two subsequent optimization steps: the first one concerns the maximization of Shannon entropy and leads to identify the functional form of the ensemble probability distribution that is maximally non-committal with respect to the missing information; the second one concerns the maximization of the likelihood function induced by this probability distribution and leads to its numerical determination. This second step translates into the resolution of a system of $O(N)$ non-linear, coupled equations (with $N$ being the total number of nodes of the network under analysis), a problem that is affected by three main issues, i.e. accuracy, speed and scalability. The present paper aims at addressing these problems by comparing the performance of three algorithms (i.e. Newton's method, a quasi-Newton method and a recently-proposed fixed-point recipe) in solving several ERGMs, defined by binary and weighted constraints in both a directed and an undirected fashion. While Newton's method performs best for relatively little networks, the fixed-point recipe is to be preferred when large configurations are considered, as it ensures convergence to the solution within seconds for networks with hundreds of thousands of nodes (e.g. the Internet, Bitcoin). We attach to the paper a Python code implementing the three aforementioned algorithms on all the ERGMs considered in the present work."
"The Stochastic Self-Consistent Harmonic Approximation: Calculating Vibrational Properties of Materials with Full Quantum and Anharmonic Effects The efficient and accurate calculation of how ionic quantum and thermal fluctuations impact the free energy of a crystal, its atomic structure, and phonon spectrum is one of the main challenges of solid state physics, especially when strong anharmonicy invalidates any perturbative approach. To tackle this problem, we present the implementation on a modular Python code of the stochastic self-consistent harmonic approximation method. This technique rigorously describes the full thermodyamics of crystals accounting for nuclear quantum and thermal anharmonic fluctuations. The approach requires the evaluation of the Born-Oppenheimer energy, as well as its derivatives with respect to ionic positions (forces) and cell parameters (stress tensor) in supercells, which can be provided, for instance, by first principles density-functional-theory codes. The method performs crystal geometry relaxation on the quantum free energy landscape, optimizing the free energy with respect to all degrees of freedom of the crystal structure. It can be used to determine the phase diagram of any crystal at finite temperature. It enables the calculation of phase boundaries for both first-order and second-order phase transitions from the Hessian of the free energy. Finally, the code can also compute the anharmonic phonon spectra, including the phonon linewidths, as well as phonon spectral functions. We review the theoretical framework of the stochastic self-consistent harmonic approximation and its dynamical extension, making particular emphasis on the physical interpretation of the variables present in the theory that can enlighten the comparison with any other anharmonic theory. A modular and flexible Python environment is used for the implementation, which allows for a clean interaction with other packages. We briefly present a toy-model calculation to illustrate the potential of the code."
"Accurate 3D fireball trajectory and orbit calculation using the 3D-FireTOC automatic Python code The disruption of asteroids and comets produces cm-sized meteoroids that end up impacting the Earth's atmosphere and producing bright fireballs that might have associated shock waves or, in geometrically-favorable occasions excavate craters that put them into unexpected hazardous scenarios. The astrometric reduction of meteors and fireballs to infer their atmospheric trajectories and heliocentric orbits involves a complex and tedious process that generally requires many manual tasks. To streamline the process, we present a software package called SPMN 3D Fireball Trajectory and Orbit Calculator (3D-FireTOC), an automatic Python code for detection, trajectory reconstruction of meteors, and heliocentric orbit computation from video recordings. The automatic 3D-FireTOC package comprises of a user interface and a graphic engine that generates a realistic 3D representation model, which allows users to easily check the geometric consistency of the results and facilitates scientific content production for dissemination. The software automatically detects meteors from digital systems, completes the astrometric measurements, performs photometry, computes the meteor atmospheric trajectory, calculates the velocity curve, and obtains the radiant and the heliocentric orbit, all in all quantifying the error measurements in each step. The software applies corrections such as light aberration, refraction, zenith attraction, diurnal aberration and atmospheric extinction. It also characterizes the atmospheric flight and consequently determines fireball fates by using the $\alpha - \beta$ criterion that analyses the ability of a fireball to penetrate deep into the atmosphere and produce meteorites. We demonstrate the performance of the software by analyzing two bright fireballs recorded by the Spanish Fireball and Meteorite Network (SPMN)."
"$\texttt{RidgeSketch}$: A Fast sketching based solver for large scale ridge regression We propose new variants of the sketch-and-project method for solving large scale ridge regression problems. Firstly, we propose a new momentum alternative and provide a theorem showing it can speed up the convergence of sketch-and-project, through a fast $\textit{sublinear}$ convergence rate. We carefully delimit under what settings this new sublinear rate is faster than the previously known linear rate of convergence of sketch-and-project without momentum. Secondly, we consider combining the sketch-and-project method with new modern sketching methods such as the count sketch, subcount sketch (a new method we propose), and subsampled Hadamard transforms. We show experimentally that when combined with the sketch-and-project method, the (sub)count sketch is very effective on sparse data and the standard subsample sketch is effective on dense data. Indeed, we show that these sketching methods, combined with our new momentum scheme, result in methods that are competitive even when compared to the Conjugate Gradient method on real large scale data. On the contrary, we show the subsampled Hadamard transform does not perform well in this setting, despite the use of fast Hadamard transforms, and nor do recently proposed acceleration schemes work well in practice. To support all of our experimental findings, and invite the community to validate and extend our results, with this paper we are also releasing an open source software package: $\texttt{RidgeSketch}$. We designed this object-oriented package in Python for testing sketch-and-project methods and benchmarking ridge regression solvers. $\texttt{RidgeSketch}$ is highly modular, and new sketching methods can easily be added as subclasses. We provide code snippets of our package in the appendix."
"rta-dq-lib: a software library to perform online data quality analysis of scientific data The Cherenkov Telescope Array (CTA) is an initiative that is currently building the largest gamma-ray ground Observatory that ever existed. A Science Alert Generation (SAG) system, part of the Array Control and Data Acquisition (ACADA) system of the CTA Observatory, analyses online the telescope data - arriving at an event rate of tens of kHz - to detect transient gamma-ray events. The SAG system also performs an online data quality analysis to assess the instruments' health during the data acquisition: this analysis is crucial to confirm good detections. A Python and a C++ software library to perform the online data quality analysis of CTA data, called rta-dq-lib, has been proposed for CTA. The Python version is dedicated to the rapid prototyping of data quality use cases. The C++ version is optimized for maximum performance. The library allows the user to define, through XML configuration files, the format of the input data and, for each data field, which quality checks must be performed and which types of aggregations and transformations must be applied. It internally translates the XML configuration into a direct acyclic computational graph that encodes the dependencies of the computational tasks to be performed. This model allows the library to easily take advantage of parallelization at the thread level and the overall flexibility allow us to develop generic data quality analysis pipelines that could also be reused in other applications."
"orvara: An Efficient Code to Fit Orbits using Radial Velocity, Absolute, and/or Relative Astrometry We present an open-source Python package, Orbits from Radial Velocity, Absolute, and/or Relative Astrometry (orvara), to fit Keplerian orbits to any combination of radial velocity, relative astrometry, and absolute astrometry data from the Hipparcos-Gaia Catalog of Accelerations. By combining these three data types, one can measure precise masses and sometimes orbital parameters even when the observations cover a small fraction of an orbit. orvara achieves its computational performance with an eccentric anomaly solver five to ten times faster than commonly used approaches, low-level memory management to avoid python overheads, and by analytically marginalizing out parallax, barycenter proper motion, and the instrument-specific radial velocity zero points. Through its integration with the Hipparcos and Gaia intermediate astrometry package htof, orvara can properly account for the epoch astrometry measurements of Hipparcos and the measurement times and scan angles of individual Gaia epochs. We configure orvara with modifiable .ini configuration files tailored to any specific stellar or planetary system. We demonstrate orvara with a case study application to a recently discovered white dwarf/main sequence (WD/MS) system, HD 159062. By adding absolute astrometry to literature RV and relative astrometry data, our comprehensive MCMC analysis improves the precision of HD~159062B's mass by more than an order of magnitude to $0.6083^{+0.0083}_{-0.0073}\,M_\odot$. We also derive a low eccentricity and large semimajor axis, establishing HD 159062AB as a system that did not experience Roche lobe overflow."
"Detecting mesoscale structures by surprise The importance of identifying the presence of mesoscale structures in complex networks can be hardly overestimated. So far, much attention has been devoted to the detection of communities, bipartite and core-periphery structures on binary networks; such an effort has led to the definition of a unified framework based upon the score function called surprise, i.e. a p-value that can be assigned to any given partition of nodes, on both undirected and directed networks. Here, we aim at making a step further, by extending the entire framework to the weighted case: after reviewing the application of the surprise-based formalism to the detection of binary mesoscale structures, we present a suitable generalization of it for detecting weighted mesoscale structures, a topic that is still largely under-explored. To this aim, we analyze four variants of the surprise; from a technical point of view, this amounts at employing four variants of the hypergeometric distribution: the binomial one for the detection of binary communities, the multinomial one for the detection of binary `bimodular' structures and their negative counterparts for the detection of communities and `bimodular' structures on weighted networks. On top of that, we define two `enhanced' variants of surprise, able to encode both binary and weighted constraints and whose definition rests upon two suitable generalizations of the hypergeometric probability mass function. As a result, we present a general, statistically grounded approach to detect mesoscale structures on networks via a unified, suprise-based framework. To illustrate the performance of our methods, we report the results of their application to several real-world networks, including social, economic, financial and ecological ones. Moreover, we attach to the paper a Python code implementing all versions of the surprise considered in the present work."
"CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences -- Version 1 Neural networks are increasingly used in environmental science applications. Furthermore, neural network models are trained by minimizing a loss function, and it is crucial to choose the loss function very carefully for environmental science applications, as it determines what exactly is being optimized. Standard loss functions do not cover all the needs of the environmental sciences, which makes it important for scientists to be able to develop their own custom loss functions so that they can implement many of the classic performance measures already developed in environmental science, including measures developed for spatial model verification. However, there are very few resources available that cover the basics of custom loss function development comprehensively, and to the best of our knowledge none that focus on the needs of environmental scientists. This document seeks to fill this gap by providing a guide on how to write custom loss functions targeted toward environmental science applications. Topics include the basics of writing custom loss functions, common pitfalls, functions to use in loss functions, examples such as fractions skill score as loss function, how to incorporate physical constraints, discrete and soft discretization, and concepts such as focal, robust, and adaptive loss. While examples are currently provided in this guide for Python with Keras and the TensorFlow backend, the basic concepts also apply to other environments, such as Python with PyTorch. Similarly, while the sample loss functions provided here are from meteorology, these are just examples of how to create custom loss functions. Other fields in the environmental sciences have very similar needs for custom loss functions, e.g., for evaluating spatial forecasts effectively, and the concepts discussed here can be applied there as well. All code samples are provided in a GitHub repository."
"DBSP_DRP: A Python package for automated spectroscopic data reduction of DBSP data DBSP_DRP is a python package that provides fully automated data reduction of data taken by the Double Spectrograph (DBSP) at the 200-inch Hale Telescope at Palomar Observatory (Oke & Gunn, 1982). The underlying data reduction functionality to extract 1D spectra, perform flux calibration and correction for atmospheric absorption, and coadd spectra together is provided by PypeIt (Prochaska et al., 2020). The new functionality that DBSP_DRP brings is in orchestrating the complex data reduction process by making smart decisions so that no user input is required after verifying the correctness of the metadata in the raw FITS files in a table-like GUI. Though the primary function of DBSP_DRP is to automatically reduce an entire night of data without user input, it has the flexibility for astronomers to fine-tune the data reduction with GUIs for manually identifying the faintest objects, as well as exposing the full set of PypeIt parameters to be tweaked for users with particular science needs. DBSP_DRP also handles some of the occasional quirks specific to DBSP, such as swapping FITS header cards, adding (an) extra null byte/s to FITS files making them not conform to the FITS specification, and not writing the coordinates of the observation to file. Additionally, DBSP_DRP contains a quicklook script for making real-time decisions during an observing run, and can open a GUI displaying a minimally reduced exposure in under 15 seconds. Docker containers are available for ease of deploying DBSP_DRP in its quicklook configuration (without some large atmospheric model files) or in its full configuration."
"Review of Ground-Based CMB Experiments We present in this paper a brief review of Ground-Based Cosmic Microwave Background (CMB) experiments. We first recall the main experimental problems and adopted solutions. We then review the Tenerife experiments, giving an update together with some new results. Then results and problems from other experiments are highlighted including IAC-Bartol, Python, Saskatoon, Mobile Anisotropy Telecope (MAT) and the Owen Valley Radio Observatory (OVRO) experiments. We next move on to the future ground experiments, in particular new interferometers such as the Very Small Array (VSA), the Cosmic Background Imager (CBI) and the Degree Angular Scale Interferometer (DASI/VCA). To finish, very recent work is presented on joint likelihood analysis for estimation of cosmological parameters where both CMB results and Large Scale Structure (LSS) surveys are considered."
"Bayesian `Hyper-Parameters' Approach to Joint Estimation: The Hubble Constant from CMB Measurements Recently several studies have jointly analysed data from different cosmological probes with the motivation of estimating cosmological parameters. Here we generalise this procedure to take into account the relative weights of various probes. This is done by including in the joint \chi^2 function a set of `Hyper-Parameters', which are dealt with using Bayesian considerations. The resulting algorithm (in the case of uniform priors on the log of the Hyper-Parameters) is very simple: instead of minimising \sum \chi_j^2 (where \chi_j^2 is per data set j) we propose to minimise \sum N_j \ln (\chi_j^2) (where N_j is the number of data points per data set j). We illustrate the method by estimating the Hubble constant H_0 from different sets of recent CMB experiments (including Saskatoon, Python V, MSAM1, TOCO and Boomerang)."
"OVRO CMB Anisotropy Measurement Constraints on Flat-Lambda and Open CDM Cosmogonies We use Owens Valley Radio Observatory (OVRO) cosmic microwave background (CMB) anisotropy data to constrain cosmological parameters. We account for the OVRO beamwidth and calibration uncertainties, as well as the uncertainty induced by the removal of non-CMB foreground contamination. We consider open and spatially-flat-Lambda cold dark matter cosmogonies, with nonrelativistic-mass density parameter Omega_0 in the range 0.1--1, baryonic-mass density parameter Omega_B in the range (0.005--0.029)h^{-2}, and age of the universe t_0 in the range (10--20) Gyr. Marginalizing over all parameters but Omega_0, the OVRO data favors an open (spatially-flat-Lambda) model with Omega_0 ~= 0.33 (0.1). At the 2 sigma confidence level model normalizations deduced from the OVRO data are mostly consistent with those deduced from the DMR, UCSB South Pole 1994, Python I-III, ARGO, MAX 4 and 5, White Dish, and SuZIE data sets."
"An Automatic Image Reduction Pipeline for the Advanced Camera for Surveys We have written an automatic image processing pipeline for the Advanced Camera for Surveys (ACS) Guaranteed Time Observation (GTO) program. The pipeline, known as Apsis, supports the different cameras available on the ACS instrument and is written in Python with a flexible object-oriented design that simplifies the incorporation of new pipeline modules. The processing steps include empirical determination of image offsets and rotation, cosmic ray rejection, image combination using the drizzle routine called via the STScI Pyraf package, object detection and photometry using SExtractor, and photometric redshift estimation in the event of multiple bandpasses. The products are encapsulated in XML markup for automated ingestion into the ACS Team archive."
"XAssist: A System for the Automation of X-ray Astrophysics Analysis XAssist is a NASA AISR-funded project for the automation of X-ray astrophysics, with emphasis on galaxies. It is nearing completion of its initially funded effort, and is working well for Chandra and ROSAT HRI data. Initial support for XMM-Newton data is present as well. It is capable of data reprocessing, source detection, and preliminary spatial, temporal and spectral analysis for each source with sufficient counts. The bulk of the system is written in Python, which in turn drives underlying software (CIAO for Chandra data, etc.). Future work will include a GUI (mainly for beginners and status monitoring) and the exposure of at least some functionality as web services. The latter will help XAssist to eventually become part of the VO, making advanced queries possible, such as determining the X-ray fluxes of counterparts to HST or SDSS sources (including the use of unpublished X-ray data), and add the ability of ``on-the-fly'' X-ray processing. Pipelines are running on ROSAT, Chandra and now XMM-Newton observations of galaxies to demonstrate XAssist's capabilities, and the results are available online (in real time) at http://www.xassist.org . XAssist itself as well as various associated projects are available for download."
"Automated Editing of Radio Interferometer Data with Pieflag Editing of radio interferometer data, a process commonly known as ``flagging'', can be laborious and time-consuming. One quickly tends to flag more data than actually required, sacrificing sensitivity and image fidelity in the process. I describe a program, Pieflag, which can analyse radio interferometer data to filter out measurements which are likely to be affected by interference. Pieflag uses two algorithms to allow for data sets which are either dominated by receiver noise or by source structure. Together, the algorithms detect essentially all affected data whilst the amount of data which is not affected by interference but falsely marked as such is kept to a minimum. The sections marked by Pieflag are very similar to what would be deemed affected by the observer in a visual inspection of the data. Pieflag displays its results concisely and allows the user to add and remove flags interactively. It is written in Python, is easy to install and use, and has a variety of options to adjust its algorithms to a particular observing situation. I describe how Pieflag works and illustrate its effect using data from typical observations."
"APECS - The Atacama Pathfinder Experiment Control System APECS is the distributed control system of the new Atacama Pathfinder EXperiment (APEX) telescope located on the Llano de Chajnantor at an altitude of 5107 m in the Atacama desert in northern Chile. APECS is based on Atacama Large Millimeter Array (ALMA) software and employs a modern, object-oriented design using the Common Object Request Broker Architecture (CORBA) as the middleware. New generic device interfaces simplify adding instruments to the control system. The Python based observer command scripting language allows using many existing software libraries and facilitates creating more complex observing modes. A new self-descriptive raw data format (Multi-Beam FITS or MBFITS) has been defined to store the multi-beam, multi-frequency data. APECS provides an online pipeline for initial calibration, observer feedback and a quick-look display. APECS is being used for regular science observations in local and remote mode since August 2005."
"Implementation of the control and data acquisition system for a small angle neutron scattering spectrometer according to the ""Juelich-Munich standard"" In Forschungszentrum Juelich the control and data acquisition systems for several neutron spectrometers are being built. Because some of these spectrometers will be commissioned to the new research reactor FRM-II at the technical university of Munich, there was a joint effort with the instrumentation group of the FRM-II to establish the ""Juelich-Munich standard"", which is basically a collection of tools and devices which are used for the implementation of the spectrometers. This includes: Siemens S7 PLCs for all axis movement issues, PROFIBUS DP for the connection of slow control equipment in the front end, TACO Middleware running on PC-Systems with Linux, python for scripting and Qt for the implementation of GUIs. The paper describes the implementation the control and data acquisition system of the KWS-1, the first experiment built according to the above standard"
"Proliferation of SDDS Support for Various Platforms and Languages Since Self-Describing Data Sets (SDDS) were first introduced, the source code has been ported to many different operating systems and various languages. SDDS is now available in C, Tcl, Java, Fortran, and Python. All of these versions are supported on Solaris, Linux, and Windows. The C version of SDDS is also supported on VxWorks. With the recent addition of the Java port, SDDS can now be deployed on virtually any operating system. Due to this proliferation, SDDS files serve to link not only a collection of C programs, but programs and scripts in many languages on different operating systems. The platform independent binary feature of SDDS also facilitates portability among operating systems. This paper presents an overview of various benefits of SDDS platform interoperability."
"GANGA: a user-Grid interface for Atlas and LHCb The Gaudi/Athena and Grid Alliance (GANGA) is a front-end for the configuration, submission, monitoring, bookkeeping, output collection, and reporting of computing jobs run on a local batch system or on the grid. In particular, GANGA handles jobs that use applications written for the Gaudi software framework shared by the Atlas and LHCb experiments. GANGA exploits the commonality of Gaudi-based computing jobs, while insulating against grid-, batch- and framework-specific technicalities, to maximize end-user productivity in defining, configuring, and executing jobs. Designed for a python-based component architecture, GANGA has a modular underpinning and is therefore well placed for contributing to, and benefiting from, work in related projects. Its functionality is accessible both from a scriptable command-line interface, for expert users and automated tasks, and through a graphical interface, which simplifies the interaction with GANGA for beginning and c1asual users.   This paper presents the GANGA design and implementation, the development of the underlying software bus architecture, and the functionality of the first public GANGA release."
"Electronic Laboratory Notebook Assisting Reflectance Spectrometry in Legal Medicine Reflectance spectrometry is a fast and reliable method for the characterisation of human skin if the spectra are analysed with respect to a physical model describing the optical properties of human skin. For a field study performed at the Institute of Legal Medicine and the Freiburg Materials Research Center of the University of Freiburg an electronic laboratory notebook has been developed, which assists in the recording, management, and analysis of reflectance spectra. The core of the electronic laboratory notebook is a MySQL database. It is filled with primary data via a web interface programmed in Java, which also enables the user to browse the database and access the results of data analysis. These are carried out by Matlab, Tcl and   Python scripts, which retrieve the primary data from the electronic laboratory notebook, perform the analysis, and store the results in the database for further usage."
"Leptonic widths of heavy quarkonia: QCD/NRQCD matching for the electromagnetic current at O(_s v^2) We construct the S-wave part of the electromagnetic vector annihilation current to $O(\alpha_s v^2)$, where $v$ is the non-relativistic quark velocity, for heavy quarks whose dynamics are described by the NRQCD action on the lattice. The NRQCD vector current for $Q\bar{Q}$ annihilation is expressed as a linear combination of lattice operators with quantum numbers L=0, $J^P=1^-$, and the coefficients are determined by matching to the corresponding continuum current in QCD to$O(v^2)$ at one-loop. The annihilation channel gives a complex amplitude with Coulomb-exchange and infrared singularities, making a careful choice for the contours of integration and infrared subtraction functions in the numerical integration necessary. An automated vertex generation program written in Python is employed, allowing us to use a realistic NRQCD action and an improved gluon lattice action; a change in the definition of either action is easily accommodated in this procedure. The final result is applicable to simulations of electromagnetic decays of heavy quarkonia, notably the $\Upsilon$ meson."
"Resurrecting the KH78/80 partial wave analysis Most of the data from the meson factories were available only after the $\pi N$ partial wave analysis of Koch and Pietarinen was published over 20 years ago. Since then, both the experimental precision and the theoretical framework have evolved a lot as well as the computing technology. Both the new and the earlier data are to be analysed by a highly modernised version of the earlier approach. Especially the propagation of the measurement errors in the analysis will be considered in detail, visualisation tools will be developed using the Python/Tkinter combination, and the huge data base of experiments will be handled by MySQL."
"A Prototype of the UAL 2.0 Application Toolkit The paper presents a prototype of the accelerator commissioning and simulation application toolkit based on the Unified Accelerator Libraries (UAL) framework. The existing UAL 1.x environment has been implemented as an open collection of C++ and Perl packages that address various tasks of accelerator physics. The UAL 2.0 application toolkit has been developing on the top of the Java infrastructure for integrating it with distributed accelerator control systems. The core part of the toolkit is composed of the Common Accelerator Objects (such as Accelerator, Algorithms, etc.) that form the framework for developing project-specific applications. The toolkit environment is not only limited to Java applications, but also supports the development and integration of high-level scripting codes (e.g. Java Python scripts) and existing C/C++ libraries. The configuration and navigation of the project-specific application system is provided by the XML-based Application Manager."
"Diagnostic tools for 3D unstructured oceanographic data Most ocean models in current use are built upon structured meshes. It follows that most existing tools for extracting diagnostic quantities (volume and surface integrals, for example) from ocean model output are constructed using techniques and software tools which assume structured meshes. The greater complexity inherent in unstructured meshes (especially fully unstructured grids which are unstructured in the vertical as well as the horizontal direction) has left some oceanographers, accustomed to traditional methods, unclear on how to calculate diagnostics on these meshes. In this paper we show that tools for extracting diagnostic data from the new generation of unstructured ocean models can be constructed with relative ease using open source software. Higher level languages such as Python, in conjunction with packages such as NumPy, SciPy, VTK and MayaVi, provide many of the high-level primitives needed to perform 3D visualisation and evaluate diagnostic quantities, e.g. density fluxes. We demonstrate this in the particular case of calculating flux of vector fields through isosurfaces, using flow data obtained from the unstructured mesh finite element ocean code ICOM, however this tool can be applied to model output from any unstructured grid ocean code."
"Baryon oscillations in galaxy and matter power-spectrum covariance matrices We investigate large-amplitude baryon acoustic oscillations (BAO's) in off-diagonal entries of cosmological power-spectrum covariance matrices. These covariance-matrix BAO's describe the increased attenuation of power-spectrum BAO's caused by upward fluctuations in large-scale power. We derive an analytic approximation to covariance-matrix entries in the BAO regime, and check the analytical predictions using N-body simulations. These BAO's look much stronger than the BAO's in the power spectrum, but seem detectable only at about a one-sigma level in gigaparsec-scale galaxy surveys. In estimating cosmological parameters using matter or galaxy power spectra, including the covariance-matrix BAO's can have a several-percent effect on error-bar widths for some parameters directly related to the BAO's, such as the baryon fraction. Also, we find that including the numerous galaxies in small haloes in a survey can reduce error bars in these cosmological parameters more than the simple reduction in shot noise might suggest."
"Logic Engines as Interactors We introduce a new programming language construct, Interactors, supporting the agent-oriented view that programming is a dialog between simple, self-contained, autonomous building blocks.   We define Interactors as an abstraction of answer generation and refinement in Logic Engines resulting in expressive language extension and metaprogramming patterns, including emulation of Prolog's dynamic database.   A mapping between backtracking based answer generation in the callee and ""forward"" recursion in the caller enables interaction between different branches of the callee's search process and provides simplified design patterns for algorithms involving combinatorial generation and infinite answer streams.   Interactors extend language constructs like Ruby, Python and C#'s multiple coroutining block returns through yield statements and they can emulate the action of monadic constructs and catamorphisms in functional languages.   Keywords: generalized iterators, logic engines, agent oriented programming language constructs, interoperation with stateful objects, metaprogramming"
"How to turn a scripting language into a domain specific language for computer algebra We have developed two computer algebra systems, meditor [Jolly:2007] and JAS [Kredel:2006]. These CAS systems are available as Java libraries. For the use-case of interactively entering and manipulating mathematical expressions, there is a need of a scripting front-end for our libraries. Most other CAS invent and implement their own scripting interface for this purpose. We, however, do not want to reinvent the wheel and propose to use a contemporary scripting language with access to Java code. In this paper we discuss the requirements for a scripting language in computer algebra and check whether the languages Python, Ruby, Groovy and Scala meet these requirements. We conclude that, with minor problems, any of these languages is suitable for our purpose."
"Non-linear Least Squares Fitting in IDL with MPFIT MPFIT is a port to IDL of the non-linear least squares fitting program MINPACK-1. MPFIT inherits the robustness of the original FORTRAN version of MINPACK-1, but is optimized for performance and convenience in IDL. In addition to the main fitting engine, MPFIT, several specialized functions are provided to fit 1-D curves and 2-D images; 1-D and 2-D peaks; and interactive fitting from the IDL command line. Several constraints can be applied to model parameters, including fixed constraints, simple bounding constraints, and ""tying"" the value to another parameter. Several data weighting methods are allowed, and the parameter covariance matrix is computed. Extensive diagnostic capabilities are available during the fit, via a call-back subroutine, and after the fit is complete. Several different forms of documentation are provided, including a tutorial, reference pages, and frequently asked questions. The package has been translated to C and Python as well. The full IDL and C packages can be found at http://purl.com/net/mpfit"
"Convergence and coupling for spin glasses and hard spheres We discuss convergence and coupling of Markov chains, and present general relations between the transfer matrices describing these two processes. We then analyze a recently developed local-patch algorithm, which computes rigorous upper bound for the coupling time of a Markov chain for non-trivial statistical-mechanics models. Using the coupling from the past protocol, this allows one to exactly sample the underlying equilibrium distribution. For spin glasses in two and three spatial dimensions, the local-patch algorithm works at lower temperatures than previous exact-sampling methods. We discuss variants of the algorithm which might allow one to reach, in three dimensions, the spin-glass transition temperature. The algorithm can be adapted to hard-sphere models. For two-dimensional hard disks, the algorithm allows us to draw exact samples at higher densities than previously possible."
"Mapping the SKA Simulated Skies with the S3-Tools The S3-Tools are a set of Python-based routines and interfaces whose purpose is to provide user-friendly access to the SKA Simulated Skies (S3) set of simulations, an effort led by the University of Oxford in the framework of the European Union's SKADS program (http://www.skads-eu.org). The databases built from the S3 simulations are hosted by the Oxford e-Research Center (OeRC), and can be accessed through a web portal at http://s-cubed.physics.ox.ac.uk. This paper focuses on the practical steps involved to make radio images from the S3-SEX and S3-SAX simulations using the S3-Map tool and should be taken as a broad overview. For a more complete description, the interested reader should look up the user's guide. The output images can then be used as input to instrument simulators, e.g. to assess technical designs and observational strategies for the SKA and SKA pathfinders."
"Symbolic computation of the Hartree-Fock energy from a chiral EFT three-nucleon interaction at N$^2$LO We present the first of a two-part Mathematica notebook collection that implements a symbolic approach for the application of the density matrix expansion (DME) to the Hartree-Fock (HF) energy from a chiral effective field theory (EFT) three-nucleon interaction at N$^2$LO. The final output from the notebooks is a Skyrme-like energy density functional that provides a quasi-local approximation to the nonlocal HF energy. In this paper, we discuss the derivation of the HF energy and its simplification in terms of the scalar/vector-isoscalar/isovector parts of the one-body density matrix. Furthermore, a set of steps is described and illustrated on how to extend the approach to other three-nucleon interactions."
"SrRietveld: A program for automating Rietveld refinements for high throughput powder diffraction studies SrRietveld is a highly automated software toolkit for Rietveld refinement. Compared to traditional refinement programs, it is more efficient to use and easier to learn. It is designed for modern high throughput diffractometers and capable of processing large numbers of data-sets with minimal effort. The new software currently uses conventional Rietveld refinement engines, automating GSAS and FullProf refinements. However, as well as automating and extending many tasks associated with these programs, it is designed in a flexible and extensible way so that in the future these engines can be replaced with new refinement engines as they become available. SrRietveld is an open source software package developed in Python."
"Youpi, a Web-based Astronomical Image Processing Pipeline Youpi stands for ""YOUpi is your processing PIpeline"". It is a portable, easy to use web application providing high level functionalities to perform data reduction on scientific FITS images. It is built on top of open source processing tools that are released to the community by Terapix, in order to organize your data on a computer cluster, to manage your processing jobs in real time and to facilitate teamwork by allowing fine-grain sharing of results and data. On the server side, Youpi is written in the Python programming language and uses the Django web framework. On the client side, Ajax techniques are used along with the Prototype and script.aculo.us Javascript librairies."
"PyMorph: Automated Galaxy Structural Parameter Estimation using Python We present a new software pipeline -- PyMorph -- for automated estimation of structural parameters of galaxies. Both parametric fits through a two dimensional bulge disk decomposition as well as structural parameter measurements like concentration, asymmetry etc. are supported. The pipeline is designed to be easy to use yet flexible; individual software modules can be replaced with ease. A find-and-fit mode is available so that all galaxies in a image can be measured with a simple command. A parallel version of the Pymorph pipeline runs on computer clusters and a Virtual Observatory compatible web enabled interface is under development."
"The GeoClaw software for depth-averaged flows with adaptive refinement Many geophysical flow or wave propagation problems can be modeled with two-dimensional depth-averaged equations, of which the shallow water equations are the simplest example. We describe the GeoClaw software that has been designed to solve problems of this nature, consisting of open source Fortran programs together with Python tools for the user interface and flow visualization. This software uses high-resolution shock-capturing finite volume methods on logically rectangular grids, including latitude--longitude grids on the sphere. Dry states are handled automatically to model inundation. The code incorporates adaptive mesh refinement to allow the efficient solution of large-scale geophysical problems. Examples are given illustrating its use for modeling tsunamis, dam break problems, and storm surge. Documentation and download information is available at www.clawpack.org/geoclaw"
"Libpsht - algorithms for efficient spherical harmonic transforms Libpsht (or ""library for Performant Spherical Harmonic Transforms"") is a collection of algorithms for efficient conversion between spatial-domain and spectral-domain representations of data defined on the sphere. The package supports transforms of scalars as well as spin-1 and spin-2 quantities, and can be used for a wide range of pixelisations (including HEALPix, GLESP and ECP). It will take advantage of hardware features like multiple processor cores and floating-point vector operations, if available. Even without this additional acceleration, the employed algorithms are among the most efficient (in terms of CPU time as well as memory consumption) currently being used in the astronomical community.   The library is written in strictly standard-conforming C90, ensuring portability to many different hard- and software platforms, and allowing straightforward integration with codes written in various programming languages like C, C++, Fortran, Python etc.   Libpsht is distributed under the terms of the GNU General Public License (GPL) version 2 and can be downloaded from http://sourceforge.net/projects/libpsht."
"Fast computing of scattering maps of nanostructures using graphical processing units Scattering maps from strained or disordered nano-structures around a Bragg reflection can either be computed quickly using approximations and a (Fast) Fourier transform, or using individual atomic positions. In this article we show that it is possible to compute up to 4.10^10 $reflections.atoms/s using a single graphic card, and we evaluate how this speed depends on number of atoms and points in reciprocal space. An open-source software library (PyNX) allowing easy scattering computations (including grazing incidence conditions) in the Python language is described, with examples of scattering from non-ideal nanostructures."
"Mayavi: a package for 3D visualization of scientific data Mayavi is an open-source, general-purpose, 3D scientific visualization package. It seeks to provide easy and interactive tools for data visualization that fit with the scientific user's workflow. For this purpose, Mayavi provides several entry points: a full-blown interactive application; a Python library with both a MATLAB-like interface focused on easy scripting and a feature-rich object hierarchy; widgets associated with these objects for assembling in a domain-specific application, and plugins that work with a general purpose application-building framework. In this article, we present an overview of the various features of Mayavi, we then provide insight on the design and engineering decisions made in implementing Mayavi, and finally discuss a few novel applications."
"Analyzing and Visualizing Cosmological Simulations with ParaView The advent of large cosmological sky surveys - ushering in the era of precision cosmology - has been accompanied by ever larger cosmological simulations. The analysis of these simulations, which currently encompass tens of billions of particles and up to trillion particles in the near future, is often as daunting as carrying out the simulations in the first place. Therefore, the development of very efficient analysis tools combining qualitative and quantitative capabilities is a matter of some urgency. In this paper we introduce new analysis features implemented within ParaView, a parallel, open-source visualization toolkit, to analyze large N-body simulations. The new features include particle readers and a very efficient halo finder which identifies friends-of-friends halos and determines common halo properties. In combination with many other functionalities already existing within ParaView, such as histogram routines or interfaces to Python, this enhanced version enables fast, interactive, and convenient analyses of large cosmological simulations. In addition, development paths are available for future extensions."
"A FEniCS-Based Programming Framework for Modeling Turbulent Flow by the Reynolds-Averaged Navier-Stokes Equations Finding an appropriate turbulence model for a given flow case usually calls for extensive experimentation with both models and numerical solution methods. This work presents the design and implementation of a flexible, programmable software framework for assisting with numerical experiments in computational turbulence. The framework targets Reynolds-averaged Navier-Stokes models, discretized by finite element methods. The novel implementation makes use of Python and the FEniCS package, the combination of which leads to compact and reusable code, where model- and solver-specific code resemble closely the mathematical formulation of equations and algorithms. The presented ideas and programming techniques are also applicable to other fields that involve systems of nonlinear partial differential equations. We demonstrate the framework in two applications and investigate the impact of various linearizations on the convergence properties of nonlinear solvers for a Reynolds-averaged Navier-Stokes model."
"Efficient numerical computation of the Pfaffian for dense and banded skew-symmetric matrices Computing the Pfaffian of a skew-symmetric matrix is a problem that arises in various fields of physics. Both computing the Pfaffian and a related problem, computing the canonical form of a skew-symmetric matrix under unitary congruence, can be solved easily once the skew-symmetric matrix has been reduced to skew-symmetric tridiagonal form. We develop efficient numerical methods for computing this tridiagonal form based on Gauss transformations, using a skew-symmetric, blocked form of the Parlett-Reid algorithm, or based on unitary transformations, using block Householder transformations and Givens rotations, that are applicable to dense and banded matrices, respectively. We also give a complete and fully optimized implementation of these algorithms in Fortran, and also provide Python, Matlab and Mathematica implementations for convenience. Finally, we apply these methods to compute the topological charge of a class D nanowire, and show numerically the equivalence of definitions based on the Hamiltonian and the scattering matrix."
"Numerical Experiments for Darcy Flow on a Surface Using Mixed Exterior Calculus Methods There are very few results on mixed finite element methods on surfaces. A theory for the study of such methods was given recently by Holst and Stern, using a variational crimes framework in the context of finite element exterior calculus. However, we are not aware of any numerical experiments where mixed finite elements derived from discretizations of exterior calculus are used for a surface domain. This short note shows results of our preliminary experiments using mixed methods for Darcy flow (hence scalar Poisson's equation in mixed form) on surfaces. We demonstrate two numerical methods. One is derived from the primal-dual Discrete Exterior Calculus and the other from lowest order finite element exterior calculus. The programming was done in the language Python, using the PyDEC package which makes the code very short and easy to read. The qualitative convergence studies seem to be promising."
"DOLFIN: Automated Finite Element Computing We describe here a library aimed at automating the solution of partial differential equations using the finite element method. By employing novel techniques for automated code generation, the library combines a high level of expressiveness with efficient computation. Finite element variational forms may be expressed in near mathematical notation, from which low-level code is automatically generated, compiled and seamlessly integrated with efficient implementations of computational meshes and high-performance linear algebra. Easy-to-use object-oriented interfaces to the library are provided in the form of a C++ library and a Python module. This paper discusses the mathematical abstractions and methods used in the design of the library and its implementation. A number of examples are presented to demonstrate the use of the library in application code."
"UFO - The Universal FeynRules Output We present a new model format for automatized matrix-element generators, the so- called Universal FeynRules Output (UFO). The format is universal in the sense that it features compatibility with more than one single generator and is designed to be flexible, modular and agnostic of any assumption such as the number of particles or the color and Lorentz structures appearing in the interaction vertices. Unlike other model formats where text files need to be parsed, the information on the model is encoded into a Python module that can easily be linked to other computer codes. We then describe an interface for the Mathematica package FeynRules that allows for an automatic output of models in the UFO format."
"Parity and reality properties of the EPRL spinfoam We study the parity behavior of the Lorentzian EPRL spinfoam model. We demonstrate that the vertex amplitude does not depend on the sign of the Immirzi parameter. We present numerical results for the transition amplitude and the graviton propagator in the large-spin 4-simplex approximation. The results suggest a simple relation between the contributions of the two parity-related critical points. Finally, we observe that the graviton propagator is not invariant under parity-odd permutations of equivalent nodes. Thus, the Lorentzian model has the same chirality problem as the Euclidean."
"Quantifying uniformity of mapped reads Summary: We describe a tool for quantifying the uniformity of mapped reads in high-throughput sequencing experiments. Our statistic directly measures the uniformity of both read position and fragment length, and we explain how to compute a p-value that can be used to quantify biases arising from experimental protocols and mapping procedures. Our method is useful for comparing different protocols in experiments such as RNA-Seq.   Availability and Implementation: We provide a freely available and open source python script that can be used to analyze raw read data or reads mapped to transcripts in BAM format at http://www.math.miami.edu/~vhower/ReadSpy.html .   Contact: lpachter@math.berkeley.edu"
"Enumerating maximal definite quadratic forms of bounded class number over Z in n >= 3 variables In this paper we give an algorithm for enumerating all primitive (positive) definite maximal Z-valued quadratic forms Q in n >= 3 variables with bounded class number h(Q) <= B. We do this by analyzing the exact mass formula [GHY], and bounding all relevant local invariants to give only finitely many possibilities. We also briefly describe an open-source implementation of this algorithm we have written in Python/Sage which explicitly enumerates all such quadratic forms of bounded class number in n >= 3 variables. Using this we determine that there are exactly 115 primitive positive definite maximal Z-valued quadratic forms in n >= 3 variables of class number one, and produce a list of them.   In a future paper we will complete this chain of ideas by extending these algorithms to allow the enumeration of all primitive maximal totally definite O_F-valued quadratic lattices of rank n >= 3, where O_F is the ring of integers of any totally real number field F."
"Mapping of Affymetrix probe sets to groups of transcripts using transcriptional networks Motivation: Usefulness of analysis derived from Affymetrix microarrays depends largely upon the reliability of files describing the correspondence between probe sets, genes and transcripts. In particular, in case a gene is targeted by two probe sets, one must be able to assess if the corresponding signals measure a group of common transcripts or two groups of transcripts with little or no overlap.   Results: Probe sets that effectively target the same group of transcripts have specific properties in the trancriptional networks we constructed. We found indeed that such probe sets had a very low negative correlation, a high positive correlation and a similar neighbourhood. Taking advantage of these properties, we devised a test allowing to group probe sets which target the same group of transcripts in a particular network. By considering several networks, additional information concerning the frequency of these associations was obtained.   Availability and Implementation: The programs developed in Python (PSAWNpy) and in Matlab (PSAWNml) are freely available, and can be downloaded at http://code.google.com/p/arraymatic/.   Tutorials and reference manuals are available at http://bns.crbm.cnrs.fr/softwares.html.   Contact: mbellis@crbm.cnrs.fr.   Supplementary information: Supplementary data are available at http://bns.crbm.cnrs.fr/download.html."
"Optimizing the Performance of Streaming Numerical Kernels on the IBM Blue Gene/P PowerPC 450 Processor Several emerging petascale architectures use energy-efficient processors with vectorized computational units and in-order thread processing. On these architectures the sustained performance of streaming numerical kernels, ubiquitous in the solution of partial differential equations, represents a challenge despite the regularity of memory access. Sophisticated optimization techniques are required to fully utilize the Central Processing Unit (CPU).   We propose a new method for constructing streaming numerical kernels using a high-level assembly synthesis and optimization framework. We describe an implementation of this method in Python targeting the IBM Blue Gene/P supercomputer's PowerPC 450 core. This paper details the high-level design, construction, simulation, verification, and analysis of these kernels utilizing a subset of the CPU's instruction set.   We demonstrate the effectiveness of our approach by implementing several three-dimensional stencil kernels over a variety of cached memory scenarios and analyzing the mechanically scheduled variants, including a 27-point stencil achieving a 1.7x speedup over the best previously published results."
"SpartyJet 4.0 User's Manual SpartyJet is a set of software tools for jet finding and analysis, built around the FastJet library of jet algorithms. SpartyJet provides four key extensions to FastJet: a simple Python interface to most FastJet features, a powerful framework for building up modular analyses, extensive input file handling capabilities, and a graphical browser for viewing analysis output and creating new on-the-fly analyses. Many of these capabilities rely on a ROOT-based backend. Beyond finding jets, many jet tools in SpartyJet perform measurement of jet or event variables, available to subsequent tools and stored in the final output. SpartyJet can be downloaded from HepForge at http://projects.hepforge.org/spartyjet."
"Managing Communication Latency-Hiding at Runtime for Parallel Programming Languages and Libraries This work introduces a runtime model for managing communication with support for latency-hiding. The model enables non-computer science researchers to exploit communication latency-hiding techniques seamlessly. For compiled languages, it is often possible to create efficient schedules for communication, but this is not the case for interpreted languages. By maintaining data dependencies between scheduled operations, it is possible to aggressively initiate communication and lazily evaluate tasks to allow maximal time for the communication to finish before entering a wait state. We implement a heuristic of this model in DistNumPy, an auto-parallelizing version of numerical Python that allows sequential NumPy programs to run on distributed memory architectures. Furthermore, we present performance comparisons for eight benchmarks with and without automatic latency-hiding. The results shows that our model reduces the time spent on waiting for communication as much as 27 times, from a maximum of 54% to only 2% of the total execution time, in a stencil application."
"emcee: The MCMC Hammer We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman & Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to $\sim N^2$ for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License."
"A general theory of DNA-mediated and other valence-limited interactions We present a general theory for predicting the interaction potentials between DNA-coated colloids, and more broadly, any particles that interact via valence-limited ligand-receptor binding. Our theory correctly incorporates the configurational and combinatorial entropic factors that play a key role in valence-limited interactions. By rigorously enforcing self-consistency, it achieves near-quantitative accuracy with respect to detailed Monte Carlo calculations. With suitable approximations and in particular geometries, our theory reduces to previous successful treatments, which are now united in a common and extensible framework. We expect our tools to be useful to other researchers investigating ligand-mediated interactions. A complete and well-documented Python implementation is freely available at http://github.com/patvarilly/DNACC ."
"Preliminary Investigation of the Frictional Response of Reptilian Shed Skin Developing deterministic surfaces relies on controlling the structure of the rubbing interface so that not only the surface is of optimized topography, but also is able to self-adjust its tribological behaviour according to the evolution of sliding conditions. In seeking inspirations for such designs, many engineers are turning toward the biological world to correlate surface structure to functional behavior of bio-analogues. From a tribological point of view, squamate reptiles offer diverse examples where surface texturing, submicron and nano-scale features, achieve frictional regulation. In this paper, we study the frictional response of shed skin obtained from a snake (Python regius). The study employed a specially designed tribo-acoustic probe capable of measuring the coefficient of friction and detecting the acoustical behavior of the skin in vivo. The results confirm the anisotropy of the frictional response of snakes. The coefficient of friction depends on the direction of sliding: the value in forward motion is lower than that in the backward direction. In addition it is shown that the anisotropy of the frictional response may stem from profile asymmetry of the individual fibril structures present within the ventral scales of the reptile"
"MadAnalysis 5, a user-friendly framework for collider phenomenology We present MadAnalysis 5, a new framework for phenomenological investigations at particle colliders. Based on a C++ kernel, this program allows to efficiently perform, in a straightforward and user-friendly fashion, sophisticated physics analyses of event files such as those generated by a large class of Monte Carlo event generators. MadAnalysis 5 comes with two modes of running. The first one, easier to handle, uses the strengths of a powerful Python interface in order to implement physics analyses by means of a set of intuitive commands. The second one requires to implement the analyses in the C++ programming language, directly within the core of the analysis framework. This opens unlimited possibilities concerning the level of complexity which can be reached, being only limited by the programming skills and the originality of the user."
"CyberChair: A Web-Based Groupware Application to Facilitate the Paper Reviewing Process In this paper we describe CyberChair, a web-based groupware application that supports the review process for technical contributions to conferences. CyberChair deals with most administrative tasks that are involved in the review process, such as storing author information, abstracts, (camera-ready) papers and reviews. It generates several overviews based on the reviews which support the Program Committee (PC) in selecting the best papers. CyberChair points out conflicting reviews and offers the reviewers means to easily communicate to solve these conflicts. In his paper Identify the Champion, O. Nierstrasz describes this review process in terms of a pattern language. CyberChair supports PCs by using these patterns in its implementation."
"Pippi - painless parsing, post-processing and plotting of posterior and likelihood samples Interpreting samples from likelihood or posterior probability density functions is rarely as straightforward as it seems it should be. Producing publication-quality graphics of these distributions is often similarly painful. In this short note I describe pippi, a simple, publicly-available package for parsing and post-processing such samples, as well as generating high-quality PDF graphics of the results. Pippi is easily and extensively configurable and customisable, both in its options for parsing and post-processing samples, and in the visual aspects of the figures it produces. I illustrate some of these using an existing supersymmetric global fit, performed in the context of a gamma-ray search for dark matter. Pippi can be downloaded and followed at http://github.com/patscott/pippi ."
"FFPopSim: An efficient forward simulation package for the evolution of large populations The analysis of the evolutionary dynamics of a population with many polymorphic loci is challenging since a large number of possible genotypes needs to be tracked. In the absence of analytical solutions, forward computer simulations are an important tool in multi-locus population genetics. The run time of standard algorithms to simulate sexual populations increases as 8^L with the number L of loci, or with the square of the population size N. We have developed algorithms that allow to simulate large populations with a run-time that scales as 3^L. The algorithm is based on an analog of the Fast-Fourier Transform (FFT) and allows for arbitrary fitness functions (i.e. any epistasis) and genetic maps. The algorithm is implemented as a collection of C++ classes and a Python interface."
"High-Order Discontinuous Galerkin Methods by GPU Metaprogramming Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. In a recent publication, we have shown that DG methods also adapt readily to execution on modern, massively parallel graphics processors (GPUs). A number of qualities of the method contribute to this suitability, reaching from locality of reference, through regularity of access patterns, to high arithmetic intensity. In this article, we illuminate a few of the more practical aspects of bringing DG onto a GPU, including the use of a Python-based metaprogramming infrastructure that was created specifically to support DG, but has found many uses across all disciplines of computational science."
"A Bernstein Polynomial Collocation Method for the Solution of Elliptic Boundary Value Problems In this article, a formulation of a point-collocation method in which the unknown function is approximated using global expansion in tensor product Bernstein polynomial basis is presented. Bernstein polynomials used in this study are defined over general interval [a,b]. Method incorporates several ideas that enable higher numerical efficiency compared to Bernstein polynomial methods that have been previously presented. The approach is illustrated by a solution of Poisson, Helmholtz and Biharmonic equations with Dirichlet and Neumann type boundary conditions. Comparisons with analytical solutions are given to demonstrate the accuracy and convergence properties of the current procedure. The method is implemented in an open-source code, and a library for manipulation of Bernstein polynomials bernstein-poly, developed by the authors."
"WorkingWiki: a MediaWiki-based platform for collaborative research WorkingWiki is a software extension for the popular MediaWiki platform that makes a wiki into a powerful environment for collaborating on publication-quality manuscripts and software projects. Developed in Jonathan Dushoff's theoretical biology lab at McMaster University and available as free software, it allows wiki users to work together on anything that can be done by using UNIX commands to transform textual ""source code"" into output. Researchers can use it to collaborate on programs written in R, python, C, or any other language, and there are special features to support easy work on LaTeX documents. It develops the potential of the wiki medium to serve as a combination collaborative text editor, development environment, revision control system, and publishing platform. Its potential uses are open-ended - its processing is controlled by makefiles that are straightforward to customize - and its modular design is intended to allow parts of it to be adapted to other purposes."
"Locality Optimization for Data Parallel Programs Productivity languages such as NumPy and Matlab make it much easier to implement data-intensive numerical algorithms. However, these languages can be intolerably slow for programs that don't map well to their built-in primitives. In this paper, we discuss locality optimizations for our system Parakeet, a just-in-time compiler and runtime system for an array-oriented subset of Python. Parakeet dynamically compiles whole user functions to high performance multi-threaded native code. Parakeet makes extensive use of the classic data parallel operators Map, Reduce, and Scan. We introduce a new set of data parallel operators,TiledMap, TiledReduce, and TiledScan, that break up their computations into local pieces of bounded size so as better to make use of small fast memories. We introduce a novel tiling transformation to generate tiled operators automatically. Applying this transformation once tiles the program for cache, and applying it again enables tiling for registers. The sizes for cache tiles are left unspecified until runtime, when an autotuning search is performed. Finally, we evaluate our optimizations on benchmarks and show significant speedups on programs that exhibit data locality."
"GPU Scripting and Code Generation with PyCUDA High-level scripting languages are in many ways polar opposites to GPUs. GPUs are highly parallel, subject to hardware subtleties, and designed for maximum throughput, and they offer a tremendous advance in the performance achievable for a significant number of computational problems. On the other hand, scripting languages such as Python favor ease of use over computational speed and do not generally emphasize parallelism. PyCUDA is a package that attempts to join the two together. This chapter argues that in doing so, a programming environment is created that is greater than just the sum of its two parts.   We would like to note that nearly all of this chapter applies in unmodified form to PyOpenCL, a sister project of PyCUDA, whose goal it is to realize the same concepts as PyCUDA for OpenCL."
"Somoclu: An Efficient Parallel Library for Self-Organizing Maps Somoclu is a massively parallel tool for training self-organizing maps on large data sets written in C++. It builds on OpenMP for multicore execution, and on MPI for distributing the workload across the nodes in a cluster. It is also able to boost training by using CUDA if graphics processing units are available. A sparse kernel is included, which is useful for high-dimensional but sparse data, such as the vector spaces common in text mining workflows. Python, R and MATLAB interfaces facilitate interactive use. Apart from fast execution, memory use is highly optimized, enabling training large emergent maps even on a single computer."
"B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language Discourse analysis may seek to characterize not only the overall composition of a given text but also the dynamic patterns within the data. This technical report introduces a data format intended to facilitate multi-level investigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by the long-form data format required for mixed-effects modeling, B(eo)W(u)LF structures linguistic data into an expanded matrix encoding any number of researchers-specified markers, making it ideal for recurrence-based analyses. While we do not necessarily claim to be the first to use methods along these lines, we have created a series of tools utilizing Python and MATLAB to enable such discourse analyses and demonstrate them using 319 lines of the Old English epic poem, Beowulf, translated into modern English."
"Improving the Testability of Object-oriented Software during Testing and Debugging Processes Testability is the probability whether tests will detect a fault, given that a fault in the program exists. How efficiently the faults will be uncovered depends upon the testability of the software. Various researchers have proposed qualitative and quantitative techniques to improve and measure the testability of software. In literature, a plethora of reliability growth models have been used to assess and measure the quantitative quality assessment of software during testing and operational phase. The knowledge about failure distribution and their complexity can improve the testability of software. Testing effort allocation can be made easy by knowing the failure distribution and complexity of faults, and this will ease the process of revealing faults from the software. As a result, the testability of the software will be improved. The parameters of the model along with the proportion of faults of different complexity to be removed from the software have been presented in the paper .We have used failure data of two object oriented software developed under open source environment namely MySQL for python and Squirrel SQL Client for estimation purpose"
"MadDM v.1.0: Computation of Dark Matter Relic Abundance Using MadGraph5 We present MadDM v.1.0, a numerical tool to compute dark matter relic abundance in a generic model. The code is based on the existing MadGraph 5 architecture and as such is easily integrable into any MadGraph collider study. A simple Python interface offers a level of user-friendliness characteristic of MadGraph 5 without sacrificing functionality. MadDM is able to calculate the dark matter relic abundance in models which include a multi-component dark sector, resonance annihilation channels and co-annihilations. We validate the code in a wide range of dark matter models by comparing the relic density results from MadDM to the existing tools and literature."
"Algorithm 950: Ncpol2sdpa---Sparse Semidefinite Programming Relaxations for Polynomial Optimization Problems of Noncommuting Variables A hierarchy of semidefinite programming (SDP) relaxations approximates the global optimum of polynomial optimization problems of noncommuting variables. Generating the relaxation, however, is a computationally demanding task, and only problems of commuting variables have efficient generators. We develop an implementation for problems of noncommuting problems that creates the relaxation to be solved by SDPA -- a high-performance solver that runs in a distributed environment. We further exploit the inherent sparsity of optimization problems in quantum physics to reduce the complexity of the resulting relaxations. Constrained problems with a relaxation of order two may contain up to a hundred variables. The implementation is available in Python. The tool helps solve problems such as finding the ground state energy or testing quantum correlations."
"Dynamic connectivity algorithms for Monte Carlo simulations of the random-cluster model We review Sweeny's algorithm for Monte Carlo simulations of the random cluster model. Straightforward implementations suffer from the problem of computational critical slowing down, where the computational effort per edge operation scales with a power of the system size. By using a tailored dynamic connectivity algorithm we are able to perform all operations with a poly-logarithmic computational effort. This approach is shown to be efficient in keeping online connectivity information and is of use for a number of applications also beyond cluster-update simulations, for instance in monitoring droplet shape transitions. As the handling of the relevant data structures is non-trivial, we provide a Python module with a full implementation for future reference."
"Computing in Operations Research using Julia The state of numerical computing is currently characterized by a divide between highly efficient yet typically cumbersome low-level languages such as C, C++, and Fortran and highly expressive yet typically slow high-level languages such as Python and MATLAB. This paper explores how Julia, a modern programming language for numerical computing which claims to bridge this divide by incorporating recent advances in language and compiler design (such as just-in-time compilation), can be used for implementing software and algorithms fundamental to the field of operations research, with a focus on mathematical optimization. In particular, we demonstrate algebraic modeling for linear and nonlinear optimization and a partial implementation of a practical simplex code. Extensive cross-language benchmarks suggest that Julia is capable of obtaining state-of-the-art performance."
"xPDFsuite: an end-to-end software solution for high throughput pair distribution function transformation, visualization and analysis The xPDFsuite software program is described. It is for processing and analyzing atomic pair distribution functions (PDF) from X-ray powder diffraction data. It provides a convenient GUI for SrXplanr and PDFgetX3, allowing the users to easily obtain 1D diffraction pattern from raw 2D diffraction images and then transform them to PDFs. It also bundles PDFgui which allows the users to create structure models and fit to the experiment data. It is specially useful for working with large numbers of datasets such as from high throughout measurements. Some of the key features are: real time PDF transformation and plotting; 2D waterfall, false color heatmap, and 3D contour plotting for multiple datasets; static and dynamic mask editing; geometric calibration of powder diffraction image; configurations and project saving and loading; Pearson correlation analysis on selected datasets; written in Python and support multiple platforms."
"Multi-scale and multi-domain computational astrophysics Astronomical phenomena are governed by processes on all spatial and temporal scales, ranging from days to the age of the Universe (13.8,Gyr) as well as from km size up to the size of the Universe. This enormous range in scales is contrived, but as long as there is a physical connection between the smallest and largest scales it is important to be able to resolve them all, and for the study of many astronomical phenomena this governance is present. Although covering all these scales is a challenge for numerical modelers, the most challenging aspect is the equally broad and complex range in physics, and the way in which these processes propagate through all scales. In our recent effort to cover all scales and all relevant physical processes on these scales we have designed the Astrophysics Multipurpose Software Environment (AMUSE). AMUSE is a Python-based framework with production quality community codes and provides a specialized environment to connect this plethora of solvers to a homogeneous problem solving environment."
"Data-flow Analysis of Programs with Associative Arrays Dynamic programming languages, such as PHP, JavaScript, and Python, provide built-in data structures including associative arrays and objects with similar semantics-object properties can be created at run-time and accessed via arbitrary expressions. While a high level of security and safety of applications written in these languages can be of a particular importance (consider a web application storing sensitive data and providing its functionality worldwide), dynamic data structures pose significant challenges for data-flow analysis making traditional static verification methods both unsound and imprecise. In this paper, we propose a sound and precise approach for value and points-to analysis of programs with associative arrays-like data structures, upon which data-flow analyses can be built. We implemented our approach in a web-application domain-in an analyzer of PHP code."
"COCOpf: An Algorithm Portfolio Framework Algorithm portfolios represent a strategy of composing multiple heuristic algorithms, each suited to a different class of problems, within a single general solver that will choose the best suited algorithm for each input. This approach recently gained popularity especially for solving combinatoric problems, but optimization applications are still emerging. The COCO platform of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms.   As an extension to the COCO platform, we present the Python-based COCOpf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies. In our framework, we focus on black-box algorithm portfolio and online adaptive selection. As a demonstration, we measure the performance of stock SciPy optimization algorithms and the popular CMA algorithm alone and in a portfolio with two simple selection strategies. We confirm that even a naive selection strategy can provide improved performance across problem classes."
"Optimized shooting method for finding periodic orbits of nonlinear dynamical systems An alternative numerical method is developed to find stable and unstable periodic orbits of nonlinear dynamical systems. The method exploits the high-efficiency of the Levenberg-Marquardt algorithm for medium-sized problems and has the additional advantage of being relatively simple to implement. It is also applicable to both autonomous and non-autonomous systems. As an example of its use, it is employed to find periodic orbits in the R\""ossler system, a coupled R\""ossler system, as well as an eight-dimensional model of a flexible rotor-bearing; problems which have been treated previously via two related methods. The results agree with the previous methods and are seen to be more accurate in some cases. A simple implementation of the method, written in the Python programming language, is provided as an Appendix."
"Fast and Robust Archetypal Analysis for Representation Learning We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization."
"Loo.py: transformation-based code generation for GPUs and CPUs Today's highly heterogeneous computing landscape places a burden on programmers wanting to achieve high performance on a reasonably broad cross-section of machines. To do so, computations need to be expressed in many different but mathematically equivalent ways, with, in the worst case, one variant per target machine.   Loo.py, a programming system embedded in Python, meets this challenge by defining a data model for array-style computations and a library of transformations that operate on this model. Offering transformations such as loop tiling, vectorization, storage management, unrolling, instruction-level parallelism, change of data layout, and many more, it provides a convenient way to capture, parametrize, and re-unify the growth among code variants. Optional, deep integration with numpy and PyOpenCL provides a convenient computing environment where the transition from prototype to high-performance implementation can occur in a gradual, machine-assisted form."
"Caffe: Convolutional Architecture for Fast Feature Embedding Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
"From Software Architecture Structure and Behavior Modeling to Implementations of Cyber-Physical Systems Software development for Cyber-Physical Systems (CPS) is a sophisticated activity as these systems are inherently complex. The engineering of CPS requires composition and interaction of diverse distributed software modules. Describing both, a systems architecture and behavior in integrated models, yields many advantages to cope with this complexity: the models are platform independent, can be decomposed to be developed independently by experts of the respective fields, are highly reusable and may be subjected to formal analysis. In this paper, we introduce a code generation framework for the MontiArcAutomaton modeling language. CPS are modeled as Component & Connector architectures with embedded I/O! automata. During development, these models can be analyzed using formal methods, graphically edited, and deployed to various platforms. For this, we present four code generators based on the MontiCore code generation framework, that implement the transformation from MontiArcAutomaton models to Mona (formal analysis), EMF Ecore (graphical editing), and Java and Python (deployment. Based on these prototypes, we discuss their commonalities and differences as well as language and application specific challenges focusing on code generator development."
"Timed Runtime Monitoring for Multiparty Conversations We propose a dynamic verification framework for protocols in real-time distributed systems. The framework is based on Scribble, a tool-chain for design and verification of choreographies based on multiparty session types, developed with our industrial partners. Drawing from recent work on multiparty session types for real-time interactions, we extend Scribble with clocks, resets, and clock predicates constraining the times in which interactions should occur. We present a timed API for Python to program distributed implementations of Scribble specifications. A dynamic verification framework ensures the safe execution of applications written with our timed API: we have implemented dedicated runtime monitors that check that each interaction occurs at a correct timing with respect to the corresponding Scribble specification. The performance of our implementation and its practicability are analysed via benchmarking."
"Machine Learning for Neuroimaging with Scikit-Learn Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g. multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g. resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain."
"SVEN: Informative Visual Representation of Complex Dynamic Structure Graphs change over time, and typically variations on the small multiples or animation pattern is used to convey this dynamism visually. However, both of these classical techniques have significant drawbacks, so a new approach, Storyline Visualization of Events on a Network (SVEN) is proposed. SVEN builds on storyline techniques, conveying nodes as contiguous lines over time. SVEN encodes time in a natural manner, along the horizontal axis, and optimizes the vertical placement of storylines to decrease clutter (line crossings, straightness, and bends) in the drawing. This paper demonstrates SVEN on several different flavors of real-world dynamic data, and outlines the remaining near-term future work."
"Simulating the photometric study of pulsating white dwarf stars in the physics laboratory We have designed a realistic simulation of astronomical observing using a relatively low-cost commercial CCD camera and a microcontroller-based circuit that drives LEDs inside a light-tight box with time-varying intensities. As part of a laboratory experiment, students can acquire sequences of images using the camera, and then perform data analysis using a language such as MATLAB or Python to: (a) extract the intensity of the imaged LEDs, (b) perform basic calibrations on the time-series data, and (c) convert their data into the frequency domain where they can then identify the frequency structure. The primary focus is on studying light curves produced by the pulsating white dwarf stars. The exercise provides an introduction to CCD observing, a framework for teaching concepts in numerical data analysis and Fourier techniques, and connections with the physics of white dwarf stars."
"Gated Feedback Recurrent Neural Networks In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions."
"Lilith: a tool for constraining new physics from Higgs measurements The properties of the observed Higgs boson with mass around 125 GeV can be affected in a variety of ways by new physics beyond the Standard Model (SM). The wealth of experimental results, targeting the different combinations for the production and decay of a Higgs boson, makes it a non-trivial task to assess the compatibility of a non-SM-like Higgs boson with all available results. In this paper we present Lilith, a new public tool for constraining new physics from signal strength measurements performed at the LHC and the Tevatron. Lilith is a Python library that can also be used in C and C++/ROOT programs. The Higgs likelihood is based on experimental results stored in an easily extensible XML database, and is evaluated from the user input, given in XML format in terms of reduced couplings or signal strengths. The results of Lilith can be used to constrain a wide class of new physics scenarios."
"The e-MERLIN Data Reduction Pipeline Written in Python and utilising ParselTongue to interface with the Astronomical Image Processing System (AIPS), the e-MERLIN data reduction pipeline is intended to automate the procedures required in processing and calibrating radio astronomy data from the e-MERLIN correlator. Driven by a plain text file of input parameters, the pipeline is modular and can be run in stages by the user, depending on requirements. The software includes options to load raw data, average in time and/or frequency, flag known sources of interference, flag more comprehensively with SERPent, carry out some or all of the calibration procedures including self-calibration), and image in either normal or wide-field mode. It also optionally produces a number of useful diagnostic plots at various stages so that the quality of the data can be assessed. The software is available for download from the e-MERLIN website or via Github."
"Context Tricks for Cheap Semantic Segmentation Accurate semantic labeling of image pixels is difficult because intra-class variability is often greater than inter-class variability. In turn, fast semantic segmentation is hard because accurate models are usually too complicated to also run quickly at test-time. Our experience with building and running semantic segmentation systems has also shown a reasonably obvious bottleneck on model complexity, imposed by small training datasets. We therefore propose two simple complementary strategies that leverage context to give better semantic segmentation, while scaling up or down to train on different-sized datasets.   As easy modifications for existing semantic segmentation algorithms, we introduce Decorrelated Semantic Texton Forests, and the Context Sensitive Image Level Prior. The proposed modifications are tested using a Semantic Texton Forest (STF) system, and the modifications are validated on two standard benchmark datasets, MSRC-21 and PascalVOC-2010. In Python based comparisons, our system is insignificantly slower than STF at test-time, yet produces superior semantic segmentations overall, with just push-button training."
"The chopthin algorithm for resampling Resampling is a standard step in particle filters and more generally sequential Monte Carlo methods. We present an algorithm, called chopthin, for resampling weighted particles. In contrast to standard resampling methods the algorithm does not produce a set of equally weighted particles; instead it merely enforces an upper bound on the ratio between the weights. Simulation studies show that the chopthin algorithm consistently outperforms standard resampling methods. The algorithms chops up particles with large weight and thins out particles with low weight, hence its name. It implicitly guarantees a lower bound on the effective sample size. The algorithm can be implemented efficiently, making it practically useful. We show that the expected computational effort is linear in the number of particles. Implementations for C++, R (on CRAN), Python and Matlab are available."
"NebulOS: A Big Data Framework for Astrophysics We introduce NebulOS, a Big Data platform that allows a cluster of Linux machines to be treated as a single computer. With NebulOS, the process of writing a massively parallel program for a datacenter is no more complicated than writing a Python script for a desktop computer. The platform enables most pre-existing data analysis software to be used, as scale, in a datacenter without modification. The shallow learning curve and compatibility with existing software greatly reduces the time required to develop distributed data analysis pipelines. The platform is built upon industry-standard, open-source Big Data technologies, from which it inherits several fault tolerance features. NebulOS enhances these technologies by adding an intuitive user interface, automated task monitoring, and other usability features. We present a summary of the architecture, provide usage examples, and discuss the system's performance scaling."
"A Flexible System for Automatic Quality Control of Oceanographic Data Sampling errors are inevitable when measuring the ocean; thus, to achieve a trustable set of observations requires a quality control (QC) procedure capable to detect spurious data. While manual QC by human experts minimizes errors, it is inefficient to handle large datasets and vulnerable to inconsistencies between different experts. Although automatic QC circumvents those issues, the traditional methods results in high rates of false positives. Here, I propose a novel approach to automatically QC oceanographic data based on the anomaly detection technique. Multiple tests are combined into a single, multidimensional criterion that learns the behavior of the good measurements, and identifies bad samples as outliers. When applied to 13 years of hydrographic profiles, the anomaly detection resulted in the best classification performance, reducing the error by at least 50%. An open source Python package, CoTeDe, was developed to provide state of the art tools to quality control oceanographic data."
"Infrared length scale and extrapolations for the no-core shell model We precisely determine the infrared (IR) length scale of the no-core shell model (NCSM). In the NCSM, the $A$-body Hilbert space is truncated by the total energy, and the IR length can be determined by equating the intrinsic kinetic energy of $A$ nucleons in the NCSM space to that of $A$ nucleons in a $3(A-1)$-dimensional hyper-radial well with a Dirichlet boundary condition for the hyper radius. We demonstrate that this procedure indeed yields a very precise IR length by performing large-scale NCSM calculations for $^{6}$Li. We apply our result and perform accurate IR extrapolations for bound states of $^{4}$He, $^{6}$He, $^{6}$Li, $^{7}$Li. We also attempt to extrapolate NCSM results for $^{10}$B and $^{16}$O with bare interactions from chiral effective field theory over tens of MeV."
"Loo.py: From Fortran to performance via transformation and substitution rules A large amount of numerically-oriented code is written and is being written in legacy languages. Much of this code could, in principle, make good use of data-parallel throughput-oriented computer architectures. Loo.py, a transformation-based programming system targeted at GPUs and general data-parallel architectures, provides a mechanism for user-controlled transformation of array programs. This transformation capability is designed to not just apply to programs written specifically for Loo.py, but also those imported from other languages such as Fortran. It eases the trade-off between achieving high performance, portability, and programmability by allowing the user to apply a large and growing family of transformations to an input program. These transformations are expressed in and used from Python and may be applied from a variety of settings, including a pragma-like manner from other languages."
"Fine-grained Language Composition: A Case Study Although run-time language composition is common, it normally takes the form of a crude Foreign Function Interface (FFI). While useful, such compositions tend to be coarse-grained and slow. In this paper we introduce a novel fine-grained syntactic composition of PHP and Python which allows users to embed each language inside the other, including referencing variables across languages. This composition raises novel design and implementation challenges. We show that good solutions can be found to the design challenges; and that the resulting implementation imposes an acceptable performance overhead of, at most, 2.6x."
"Does Gaming Help Improve Cognitive Skills? A nationally representative study of video game play among adolescents in the United States showed that 97% of adolescents aged 12 to 17 years play computer, web, and portable (or console) video games (Lenhart et al., 2008). We hypothesized that if people play games as a regular exercise regime, gaming will correlate with an improvement in their cognitive skills. For this experiment, a few games that tested the logical reasoning and critical analysis skills under a given time constraint were coded in Python using Pygame and were played by a group of 7th grade students. In order to test whether there is a relationship between gaming and test performance, we divided the students into two groups and gave them tests before and after the experimentation period in order to measure their improvement. One group played the games while the other did not. In the group of students that played the games, an average improvement of 62.19% was seen (p < 0.0001). The group that did not play the games only improved their performance by an average of 18.51% (p = 0.0882)."
"TRIQS: A Toolbox for Research on Interacting Quantum Systems We present the TRIQS library, a Toolbox for Research on Interacting Quantum Systems. It is an open-source, computational physics library providing a framework for the quick development of applications in the field of many-body quantum physics, and in particular, strongly-correlated electronic systems. It supplies components to develop codes in a modern, concise and efficient way: e.g. Green's function containers, a generic Monte Carlo class, and simple interfaces to HDF5. TRIQS is a C++/Python library that can be used from either language. It is distributed under the GNU General Public License (GPLv3). State-of-the-art applications based on the library, such as modern quantum many-body solvers and interfaces between density-functional-theory codes and dynamical mean-field theory (DMFT) codes are distributed along with it."
"The Hierarchy Solution to the LHC Inverse Problem Supersymmetric (SUSY) models, even those described by relatively few parameters, generically allow many possible SUSY particle (sparticle) mass hierarchies. As the sparticle mass hierarchy determines, to a great extent, the collider phenomenology of a model, the enumeration of these hierarchies is of the utmost importance. We therefore provide a readily generalizable procedure for determining the number of sparticle mass hierarchies in a given SUSY model. As an application, we analyze the gravity-mediated SUSY breaking scenario with various combinations of GUT-scale boundary conditions involving different levels of universality among the gaugino and scalar masses. For each of the eight considered models, we provide the complete list of forbidden hierarchies in a compact form. Our main result is that the complete (typically rather large) set of forbidden hierarchies among the eight sparticles considered in this analysis can be fully specified by just a few forbidden relations involving much smaller subsets of sparticles."
"Symbolic Manipulation of Code Properties The FAdo system is a symbolic manipulator of formal languages objects, implemented in Python. In this work, we extend its capabilities by implementing methods to manipulate transducers and we go one level higher than existing formal language systems and implement methods to manipulate objects representing classes of independent languages (widely known as code properties). Our methods allow users to define their own code properties and combine them between themselves or with fixed properties such as prefix codes, suffix codes, error detecting codes, etc. The satisfaction and maximality decision questions are solvable for any of the definable properties. The new online system LaSer allows to query about code properties and obtain the answer in a batch mode. Our work is founded on independence theory as well as the theory of rational relations and transducers and contributes with improveded algorithms on these objects."
"A Visualization of Null Geodesics for the Bonnor Massive Dipole In this work we simulate null geodesics for the Bonnor massive dipole metric by implementing a symbolic-numerical algorithm in Sage and Python. This program is also capable of visualizing in 3D, in principle, the geodesics for any given metric. Geodesics are launched from a common point, collectively forming a cone of light beams, simulating a solid-angle section of a point source in front of a massive object with a magnetic field. Parallel light beams also were considered, and their bending due to the curvature of the space-time was simulated."
"The ELAPS Framework: Experimental Linear Algebra Performance Studies Optimal use of computing resources requires extensive coding, tuning and benchmarking. To boost developer productivity in these time consuming tasks, we introduce the Experimental Linear Algebra Performance Studies framework (ELAPS), a multi-platform open source environment for fast yet powerful performance experimentation with dense linear algebra kernels, algorithms, and libraries. ELAPS allows users to construct experiments to investigate how performance and efficiency vary depending on factors such as caching, algorithmic parameters, problem size, and parallelism. Experiments are designed either through Python scripts or a specialized GUI, and run on the whole spectrum of architectures, ranging from laptops to clusters, accelerators, and supercomputers. The resulting experiment reports provide various metrics and statistics that can be analyzed both numerically and visually. We demonstrate the use of ELAPS in four concrete application scenarios and in as many computing environments, illustrating its practical value in supporting critical performance decisions."
"Blocks and Fuel: Frameworks for deep learning We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly."
"Verification of railway interlocking systems In the railway domain, an interlocking is a computerised system that controls the railway signalling objects in order to allow a safe operation of the train traffic. Each interlocking makes use of particular data, called application data, that reflects the track layout of the station under control. The verification and validation of the application data are performed manually and is thus error-prone and costly. In this paper, we explain how we built an executable model in NuSMV of a railway interlocking based on the application data. We also detail the tool that we have developed in order to translate the application data into our model automatically. Finally we show how we could verify a realistic set of safety properties on a real-size station model by customizing the existing model-checking algorithm with PyNuSMV a Python library based on NuSMV."
"pyMOR - Generic Algorithms and Interfaces for Model Order Reduction Reduced basis methods are projection-based model order reduction techniques for reducing the computational complexity of solving parametrized partial differential equation problems. In this work we discuss the design of pyMOR, a freely available software library of model order reduction algorithms, in particular reduced basis methods, implemented with the Python programming language. As its main design feature, all reduction algorithms in pyMOR are implemented generically via operations on well-defined vector array, operator and discretization interface classes. This allows for an easy integration with existing open-source high-performance partial differential equation solvers without adding any model reduction specific code to these solvers. Besides an in-depth discussion of pyMOR's design philosophy and architecture, we present several benchmark results and numerical examples showing the feasibility of our approach."
"Toyz: A Framework for Scientific Analysis of Large Datasets and Astronomical Images As the size of images and data products derived from astronomical data continues to increase, new tools are needed to visualize and interact with that data in a meaningful way. Motivated by our own astronomical images taken with the Dark Energy Camera (DECam) we present Toyz, an open source Python package for viewing and analyzing images and data stored on a remote server or cluster. Users connect to the Toyz web application via a web browser, making it an convenient tool for students to visualize and interact with astronomical data without having to install any software on their local machines. In addition it provides researchers with an easy-to-use tool that allows them to browse the files on a server and quickly view very large images ($>$ 2 Gb) taken with DECam and other cameras with a large FOV and create their own visualization tools that can be added on as extensions to the default Toyz framework."
"Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait Association Mapping Genome-wide association studies have proven to be essential for understanding the genetic basis of disease. However, many complex traits---personality traits, facial features, disease subtyping---are inherently high-dimensional, impeding simple approaches to association mapping. We developed a nonparametric Bayesian reduced rank regression model for multi-SNP, multi-trait association mapping that does not require the rank of the linear subspace to be specified. We show in simulations and real data that our model shares strength over SNPs and over correlated traits, improving statistical power to identify genetic associations with an interpretable, SNP-supervised low-dimensional linear projection of the high-dimensional phenotype. On the HapMap phase 3 gene expression QTL study data, we identify pleiotropic expression QTLs that classical univariate tests are underpowered to find and that two step approaches cannot recover. Our Python software, BERRRI, is publicly available at GitHub: https://github.com/ashlee1031/BERRRI."
"MeFiT: Merging and Filtering Tool for Illumina Paired-End Reads for 16S rRNA Amplicon Sequencing Recent advances in next-generation sequencing have revolutionized genomic research. 16S rRNA amplicon sequencing using paired-end sequencing on the MiSeq platform from Illumina, Inc., is being used to characterize the composition and dynamics of extremely complex/diverse microbial communities. For this analysis on the Illumina platform, merging and quality filtering of paired-end reads are essential first steps in data analysis to ensure the accuracy and reliability of downstream analysis. We have developed the Merging and Filtering Tool (MeFiT) to combine these pre-processing steps into one simple, intuitive pipeline. MeFiT provides an open-source solution that permits users to merge and filter paired end illumina reads based on user-selected quality parameters. The tool has been implemented in python and the source-code is freely available at https://github.com/nisheth/MeFiT."
"Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection This paper introduces a fast algorithm for randomized computation of a low-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this matrix to represent the development of a spatial grid through time e.g. data from a static video source. DMD was originally introduced in the fluid mechanics community, but is also suitable for motion detection in video streams and its use for background subtraction has received little previous investigation. In this study we present a comprehensive evaluation of background subtraction, using the randomized DMD and compare the results with leading robust principal component analysis algorithms. The results are convincing and show the random DMD is an efficient and powerful approach for background modeling, allowing processing of high resolution videos in real-time. Supplementary materials include implementations of the algorithms in Python."
"Using Pilot Systems to Execute Many Task Workloads on Supercomputers High performance computing systems have historically been designed to support applications comprised of mostly monolithic, single-job workloads. Pilot systems decouple workload specification, resource selection, and task execution via job placeholders and late-binding. Pilot systems help to satisfy the resource requirements of workloads comprised of multiple tasks. RADICAL-Pilot (RP) is a modular and extensible Python-based pilot system. In this paper we describe RP's design, architecture and implementation, and characterize its performance. RP is capable of spawning more than 100 tasks/second and supports the steady-state execution of up to 16K concurrent tasks. RP can be used stand-alone, as well as integrated with other application-level tools as a runtime system."
"Integrated Detector Control and Calibration Processing at the European XFEL The European X-ray Free Electron Laser is a high-intensity X-ray light source currently being constructed in the area of Hamburg, that will provide spatially coherent X-rays in the energy range between $0.25\,\mathrm{keV}$ and $25\,\mathrm{keV}$. The machine will deliver $10\,\mathrm{trains/s}$, consisting of up to $2700\,\mathrm{pulses}$, with a $4.5\,\mathrm{MHz}$ repetition rate. The LPD, DSSC and AGIPD detectors are being developed to provide high dynamic-range Mpixel imaging capabilities at the mentioned repetition rates. A consequence of these detector characteristics is that they generate raw data volumes of up to $15\,\mathrm{Gbyte/s}$. In addition the detector's on-sensor memory-cell and multi-/non-linear gain architectures pose unique challenges in data correction and calibration, requiring online access to operating conditions and control settings. We present how these challenges are addressed within XFEL's control and analysis framework Karabo, which integrates access to hardware conditions, acquisition settings (also using macros) and distributed computing. Implementation of control and calibration software is mainly in Python, using self-optimizing (py) CUDA code, numpy and iPython parallels to achieve near-real time performance for calibration application."
"Everyday Radio Telescope We have developed an affordable, portable college level radio telescope for amateur radio astronomy which can be used to provide hands-on experience with the fundamentals of a radio telescope and an insight into the realm of radio astronomy. With our set-up one can measure brightness temperature and flux of the Sun at 11.2 GHz and calculate the beam width of the antenna. The set-up uses commercially available satellite television receiving system and parabolic dish antenna. We report the detection of point sources like Saturn and extended sources like the galactic arm of the Milky way. We have also developed python pipeline, which are available for free download, for data acquisition and visualization."
"A Theorem Prover for Quantum Hoare Logic and Its Applications Quantum Hoare Logic (QHL) was introduced in Ying's work to specify and reason about quantum programs. In this paper, we implement a theorem prover for QHL based on Isabelle/HOL. By applying the theorem prover, verifying a quantum program against a specification is transformed equivalently into an order relation between matrices. Due to the limitation of Isabelle/HOL, the calculation of the order relation is solved by calling an outside oracle written in Python. To the best of our knowledge, this is the first theorem prover for quantum programs. To demonstrate its power, the correctness of two well-known quantum algorithms, i.e., Grover Quantum Search and Quantum Phase Estimation (the key step in Shor's quantum algorithm of factoring in polynomial time) are proved using the theorem prover. These are the first mechanized proofs for both of them."
"Fluid dynamics in porous media with Sailfish In this work we show the application of Sailfish to the study of fluid dynamics in porous media. Sailfish is an open-source software based on the lattice-Boltzmann method. This application of computational fluid dynamics is of particular interest to the oil and gas industry and the subject could be a starting point for an undergraduate or graduate student in physics or engineering. We built artificial samples of porous media with different porosities and used Sailfish to simulate the fluid flow through in order to calculate permeability and tortuosity. We also present a simple way to obtain the specific superficial area of porous media using Python libraries. To contextualize these concepts, we test the Kozeny--Carman equation, discuss its validity and calculate the Kozeny's constant for our artificial samples."
"FreeSASA: An open source C library for solvent accessible surface area calculations Calculating solvent accessible surface areas (SASA) is a run-of-the-mill calculation in structural biology. Although there are many programs available for this calculation, there are no free-standing, open-source tools designed for easy tool-chain integration. FreeSASA is an open source C library for SASA calculations that provides both command-line and Python interfaces in addition to its C API. The library implements both Lee and Richards' and Shrake and Rupley's approximations, and is highly configurable to allow the user to control molecular parameters, accuracy and output granularity. It only depends on standard C libraries and should therefore be easy to compile and install on any platform. The source code is freely available from http://freesasa.github.io/. The library is well-documented, stable and efficient. The command-line interface can easily replace closed source legacy programs, with comparable or better accuracy and speed, and with some added functionality."
"A General System for Heuristic Solution of Convex Problems over Nonconvex Sets We describe general heuristics to approximately solve a wide variety of problems with convex objective and decision variables from a nonconvex set. The heuristics, which employ convex relaxations, convex restrictions, local neighbor search methods, and the alternating direction method of multipliers (ADMM), require the solution of a modest number of convex problems, and are meant to apply to general problems, without much tuning. We describe an implementation of these methods in a package called NCVX, as an extension of CVXPY, a Python package for formulating and solving convex optimization problems. We study several examples of well known nonconvex problems, and show that our general purpose heuristics are effective in finding approximate solutions to a wide variety of problems."
"Toward a System Building Agenda for Data Integration In this paper we argue that the data management community should devote far more effort to building data integration (DI) systems, in order to truly advance the field. Toward this goal, we make three contributions. First, we draw on our recent industrial experience to discuss the limitations of current DI systems. Second, we propose an agenda to build a new kind of DI systems to address these limitations. These systems guide users through the DI workflow, step by step. They provide tools to address the ""pain points"" of the steps, and tools are built on top of the Python data science and Big Data ecosystem (PyData). We discuss how to foster an ecosystem of such tools within PyData, then use it to build DI systems for collaborative/cloud/crowd/lay user settings. Finally, we discuss ongoing work at Wisconsin, which suggests that these DI systems are highly promising and building them raises many interesting research challenges."
"An Algorithm to Compress Line-transition Data for Radiative-transfer Calculations Molecular line-transition lists are an essential ingredient for radiative-transfer calculations. With recent databases now surpassing the billion-lines mark, handling them has become computationally prohibitive, due to both the required processing power and memory. Here I present a temperature-dependent algorithm to separate strong from weak line transitions, reformatting the large majority of the weaker lines into a cross-section data file, and retaining the detailed line-by-line information of the fewer strong lines. For any given molecule over the 0.3--30 {\micron} range, this algorithm reduces the number of lines to a few million, enabling faster radiative-transfer computations without a significant loss of information. The final compression rate depends on how densely populated is the spectrum. I validate this algorithm by comparing Exomol's HCN extinction-coefficient spectra between the complete (65 million line transitions) and compressed (7.7 million) line lists. Over the 0.6--33 {\micron} range, the average difference between extinction-coefficient values is less than 1\%. A Python/C implementation of this algorithm is open-source and available at https://github.com/pcubillos/repack . So far, this code handles the Exomol and HITRAN line-transition format."
"The FluxCompensator: Making Radiative Transfer Models of hydrodynamical Simulations Directly Comparable to Real Observations When modeling astronomical objects throughout the universe, it is important to correctly treat the limitations of the data, for instance finite resolution and sensitivity. In order to simulate these effects, and to make radiative transfer models directly comparable to real observations, we have developed an open-source Python package called the FluxCompensator that enables the post-processing of the output of 3-d Monte-Carlo radiative transfer codes, such as HYPERION. With the FluxCompensator, realistic synthetic observations can be generated by modelling the effects of convolution with arbitrary point-spread functions (PSFs), transmission curves, finite pixel resolution, noise and reddening. Pipelines can be applied to compute synthetic observations that simulate observatories, such as the Spitzer Space Telescope or the Herschel Space Observatory. Additionally, this tool can read in existing observations (e.g. FITS format) and use the same settings for the synthetic observations. In this paper, we describe the package as well as present examples of such synthetic observations."
"Programmable and scalable radio-frequency pulse sequence generator for multi-qubit quantum information experiments We present a versatile rf pulse control system that has been designed for multi-qubit quantum experiments. One instrument can be scaled to provide 32 channels of rf between 10 - 450 MHz. Synchronization can be achieved across multiple instruments. By using direct digital synthesis and custom control circuitry contained within a field-programmable gate array, sequences of transform-limited pulses can be produced. These have been used to carry out quantum gates that are able to meet fault-tolerant thresholds for single- and two-qubit gate fidelities, as published elsewhere. We have also extended the frequency to the gigahertz regime using additional mixers to address hyperfine transitions in atomic systems. The system uses an efficient memory management scheme and a low-latency communications protocol that allows pulse sequences to be updated in real-time. Together these can enable outcome-based algorithms such as quantum error correction to be executed. The system is fully programmable in C++, and other languages such as Python can be supported by the on-board CPU, offering a highly flexible platform for a wide variety of experimental systems, and has been proven in trapped-ion quantum information experiments."
"MARXS: A modular software to ray-trace X-ray instrumention To obtain the best possible scientific result, astronomers must understand the properties of the available instrumentation well. This is important both when designing new instruments and when using existing instruments close to the limits of their specified capabilities or beyond. Ray-tracing is a technique for numerical simulations where the path of many light rays is followed through the system to understand how individual system components influence the observed properties, such as the shape of the point-spread-function (PSF). In instrument design, such simulations can be used to optimize the performance. For observations with existing instruments this helps to discern instrumental artifacts from a true signal. Here, we describe MARXS, a new python package designed to simulate X-ray instruments on satellites and sounding rockets. MARXS uses probability tracking of photons and has polarimetric capabilities."
"Retrieving the Aerosol Complex Refractive Index using PyMieScatt: A Mie Computational Package with Visualization Capabilities The complex refractive index m=n+ik of a particle is an intrinsic property which cannot be directly measured, it must be inferred from its extrinsic properties such as the scattering and absorption cross-sections. Bohren and Huffman called this approach describing the dragon from its tracks, since the inversion of Lorenz-Mie theory equations is intractable without the use of computers. This article describes PyMieScatt, an open-source module for Python that contains functionality for solving the inverse problem for complex m using extensive optical and physical properties as input, and calculating regions where valid solutions may exist within the error bounds of laboratory measurements. Additionally, the module has comprehensive capabilities for studying homogeneous and coated single spheres, as well as ensembles of homogeneous spheres with user-defined size distributions, making it a complete tool for studying the optical behavior of spherical particles."
"BayesVP: a Bayesian Voigt profile fitting package We introduce a Bayesian approach for modeling Voigt profiles in absorption spectroscopy and its implementation in the python package, BayesVP, publicly available at https://github.com/cameronliang/BayesVP. The code fits the absorption line profiles within specified wavelength ranges and generates posterior distributions for the column density, Doppler parameter, and redshifts of the corresponding absorbers. The code uses publicly available efficient parallel sampling packages to sample posterior and thus can be run on parallel platforms. BayesVP supports simultaneous fitting for multiple absorption components in high-dimensional parameter space. We provide other useful utilities in the package, such as explicit specification of priors of model parameters, continuum model, Bayesian model comparison criteria, and posterior sampling convergence check."
"A Multiple-Choice Test Recognition System based on the Gamera Framework This article describes JECT-OMR, a system that analyzes digital images representing scans of multiple-choice tests compiled by students. The system performs a structural analysis of the document in order to get the chosen answer for each question, and it also contains a bar-code decoder, used for the identification of additional information encoded in the document. JECT-OMR was implemented using the Python programming language, and leverages the power of the Gamera framework in order to accomplish its task. The system exhibits an accuracy of over 99% in the recognition of marked and non-marked squares representing answers, thus making it suitable for real world applications"
"MadGraph 5 : Going Beyond MadGraph 5 is the new version of the MadGraph matrix element generator, written in the Python programming language. It implements a number of new, efficient algorithms that provide improved performance and functionality in all aspects of the program. It features a new user interface, several new output formats including C++ process libraries for Pythia 8, and full compatibility with FeynRules for new physics models implementation, allowing for event generation for any model that can be written in the form of a Lagrangian. MadGraph 5 builds on the same philosophy as the previous versions, and its design allows it to be used as a collaborative platform where theoretical, phenomenological and simulation projects can be developed and then distributed to the high-energy community. We describe the ideas and the most important developments of the code and illustrate its capabilities through a few simple phenomenological examples."
"Distributed Transactions for Google App Engine: Optimistic Distributed Transactions built upon Local Multi-Version Concurrency Control Massively scalable web applications encounter a fundamental tension in computing between ""performance"" and ""correctness"": performance is often addressed by using a large and therefore distributed machine where programs are multi-threaded and interruptible, whereas correctness requires data invariants to be maintained with certainty. A solution to this problem is ""transactions"" [Gray-Reuter].   Some distributed systems such as Google App Engine [http://code.google.com/appengine/docs/] provide transaction semantics but only for functions that access one of a set of predefined local regions of the database: a ""Local Transaction"" (LT) [http://code.google.com/appengine/docs/python/datastore/transactions.html]. To address this problem we give a ""Distributed Transaction"" (DT) algorithm which provides transaction semantics for functions that operate on any set of objects distributed across the machine. Our algorithm is in an ""optimistic"" [http://en.wikipedia.org/wiki/Optimistic_concurrency_control] style. We assume Sequential [Time-]Consistency [http://en.wikipedia.org/wiki/Sequential_consistency] for Local Transactions."
"API Blender: A Uniform Interface to Social Platform APIs With the growing success of the social Web, most Web developers have to interact with at least one social Web platform, which implies studying the related API specifications. These are often only informally described, may contain errors, lack harmonization, and generally speaking make the developer's work difficult. Most attempts to solve this problem, proposing formal description languages for Web service APIs, have had limited success outside of B2B applications; we believe it is due to their top-down nature. In addition, a programmer dealing with one or several of these APIs has to deal with a number of related tasks such as data integration, requests chaining, or policy management, that are cumbersome to implement. Inspired by the SPORE project, we present API Blender, an open-source solution to describe, interact with, and integrate the most common social Web APIs. In this perspective, we first introduce two new lightweight description formats for requests and services and demonstrate their relevance with respect to current platform APIs. We present our Python implementation of API Blender and its features regarding authentication, policy management and multi-platform data integration."
"Video Tester -- A multiple-metric framework for video quality assessment over IP networks This paper presents an extensible and reusable framework which addresses the problem of video quality assessment over IP networks. The proposed tool (referred to as Video-Tester) supports raw uncompressed video encoding and decoding. It also includes different video over IP transmission methods (i.e.: RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it is furnished with a rich set of offline analysis capabilities. Video-Tester analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth, packet inter-arrival time, jitter and loss rate, as well as GOP size and I-frame loss rate). Our design facilitates the integration of virtually any existing video quality metric thanks to the adopted Python-based modular approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video quality metric, DIV and PSNR-based MOS estimations. In order to promote its use and extension, Video-Tester is open and publicly available."
"PyPLN: a Distributed Platform for Natural Language Processing This paper presents a distributed platform for Natural Language Processing called PyPLN. PyPLN leverages a vast array of NLP and text processing open source tools, managing the distribution of the workload on a variety of configurations: from a single server to a cluster of linux servers. PyPLN is developed using Python 2.7.3 but makes it very easy to incorporate other softwares for specific tasks as long as a linux version is available. PyPLN facilitates analyses both at document and corpus level, simplifying management and publication of corpora and analytical results through an easy to use web interface. In the current (beta) release, it supports English and Portuguese languages with support to other languages planned for future releases. To support the Portuguese language PyPLN uses the PALAVRAS parser\citep{Bick2000}. Currently PyPLN offers the following features: Text extraction with encoding normalization (to UTF-8), part-of-speech tagging, token frequency, semantic annotation, n-gram extraction, word and sentence repertoire, and full-text search across corpora. The platform is licensed as GPL-v3."
"Next generation input-output data format for HEP using Google's protocol buffers We propose a data format for Monte Carlo (MC) events, or any structural data, including experimental data, in a compact binary form using variable-size integer encoding as implemented in the Google's Protocol Buffers package. This approach is implemented in the so-called ProMC library which produces smaller file sizes for MC records compared to the existing input-output libraries used in high-energy physics (HEP). Other important features are a separation of abstract data layouts from concrete programming implementations, self-description and random access. Data stored in ProMC files can be written, read and manipulated in a number of programming languages, such C++, Java and Python."
"VCF2Networks: applying Genotype Networks to Single Nucleotide Variants data Summary: Genotype networks are a method used in systems biology to study the innovability of a given phenotype, determining whether the phenotype is robust to mutations, and how do the genotypes associated to it are distributed in the genotype space. Here we developed VCF2Networks, a tool to apply this method to population genetics data, and in particular to single Nucleotide Variants data encoded in the Variant Call file Format (VCF). A complete summary of the properties of the genotype network that can be calculated by VCF2Networks is given in the Supplementary Materials 1.   Availability and Implementation: The home page of the project is https://bitbucket.org/dalloliogm/vcf2networks . VCF2Networks is also available directly from the Python Package Index (PyPI), under the name vcf2networks."
"Linear Density Perturbations in Multifield Coupled Quintessence We study the behaviour of linear perturbations in multifield coupled quintessence models. Using gauge invariant linear cosmological perturbation theory we provide the full set of governing equations for this class of models, and solve the system numerically. We apply the numerical code to generate growth functions for various examples, and compare these both to the standard $\Lambda$CDM model and to current and future observational bounds. Finally, we examine the applicability of the ""small scale approximation"", often used to calculate growth functions in quintessence models, in light of upcoming experiments such as SKA and Euclid. We find the deviation of the full equation results for large k modes from the approximation exceeds the experimental uncertainty for these future surveys. The numerical code, PYESSENCE, written in Python will be publicly available."
"An assessment of orthographic similarity measures for several African languages Natural Language Interfaces and tools such as spellcheckers and Web search in one's own language are known to be useful in ICT-mediated communication. Most languages in Southern Africa are under-resourced, however. Therefore, it would be very useful if both the generic and the few language-specific NLP tools could be reused or easily adapted across languages. This depends on the notion, and extent, of similarity between the languages. We assess this from the angle of orthography and corpora. Twelve versions of the Universal Declaration of Human Rights (UDHR) are examined, showing clusters of languages, and which are thus more or less amenable to cross-language adaptation of NLP tools, which do not match with Guthrie zones. To examine the generalisability of these results, we zoom in on isiZulu both quantitatively and qualitatively with four other corpora and texts in different genres. The results show that the UDHR is a typical text document orthographically. The results also provide insight into usability of typical measures such as lexical diversity and genre, and that the same statistic may mean different things in different documents. While NLTK for Python could be used for basic analyses of text, it, and similar NLP tools, will need considerable customization."
"Prototyping the graphical user interface for the operator of the Cherenkov Telescope Array The Cherenkov Telescope Array (CTA) is a planned gamma-ray observatory. CTA will incorporate about 100 imaging atmospheric Cherenkov telescopes (IACTs) at a Southern site, and about 20 in the North. Previous IACT experiments have used up to five telescopes. Subsequently, the design of a graphical user interface (GUI) for the operator of CTA involves new challenges. We present a GUI prototype, the concept for which is being developed in collaboration with experts from the field of Human-Computer Interaction. The prototype is based on Web technology; it incorporates a Python web server, Web Sockets and graphics generated with the d3.js Javascript library."
"Visual Analysis of Nonlinear Dynamical Systems: Chaos, Fractals, Self-Similarity and the Limits of Prediction Nearly all nontrivial real-world systems are nonlinear dynamical systems. Chaos describes certain nonlinear dynamical systems that have a very sensitive dependence on initial conditions. Chaotic systems are always deterministic and may be very simple, yet they produce completely unpredictable and divergent behavior. Systems of nonlinear equations are difficult to solve analytically, and scientists have relied heavily on visual and qualitative approaches to discover and analyze the dynamics of nonlinearity. Indeed, few fields have drawn as heavily from visualization methods for their seminal innovations: from strange attractors, to bifurcation diagrams, to cobweb plots, to phase diagrams and embedding. Although the social sciences are increasingly studying these types of systems, seminal concepts remain murky or loosely adopted. This article has three aims. First, it argues for several visualization methods to critically analyze and understand the behavior of nonlinear dynamical systems. Second, it uses these visualizations to introduce the foundations of nonlinear dynamics, chaos, fractals, self-similarity and the limits of prediction. Finally, it presents Pynamical, an open-source Python package to easily visualize and explore nonlinear dynamical systems' behavior."
"Probabilistic Data Analysis with Probabilistic Programming Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries."
"SPIKE a Processing Software dedicated to Fourier Spectroscopies We present SPIKE (Spectrometry Processing Innovative KErnel), an open-source Python package dedicated to Fourier spectroscopies. It provides basic functionalities such as apodisation, a complete set of Fourier transforms, phasing for NMR), peak-picking, baseline correction and also tools such as Linear Prediction. Beside its versatility, the most prominent novelty of this package is to incorporate new tools for Big Data processing. This is exemplified by its ability to handle the processing and visualization of very large data-sets, with multiprocessor capabilities and a low memory footprint. The software contains also all the tools necessary for the specific fast processing and visualization of 2D-FTICR-MS data-sets."
"Containers for portable, productive and performant scientific computing Containers are an emerging technology that hold promise for improving productivity and code portability in scientific computing. We examine Linux container technology for the distribution of a non-trivial scientific computing software stack and its execution on a spectrum of platforms from laptop computers through to high performance computing (HPC) systems. We show on a workstation and a leadership-class HPC system that when deployed appropriately there are no performance penalties running scientific programs inside containers. For Python code run on large parallel computers, the run time is reduced inside a container due to faster library imports. The software distribution approach and data that we present will help developers and users decide on whether container technology is appropriate for them. We also provide guidance for the vendors of HPC systems that rely on proprietary libraries for performance on what they can do to make containers work seamlessly and without performance penalty."
"Taming Context-Sensitive Languages with Principled Stateful Parsing Historically, true context-sensitive parsing has seldom been applied to programming languages, due to its inherent complexity. However, many mainstream programming and markup languages (C, Haskell, Python, XML, and more) possess context-sensitive features. These features are traditionally handled with ad-hoc code (e.g., custom lexers), outside of the scope of parsing theory.   Current grammar formalisms struggle to express context-sensitive features. Most solutions lack context transparency: they make grammars hard to write, maintain and compose by hardwiring context through the entire grammar. Instead, we approach context-sensitive parsing through the idea that parsers may recall previously matched input (or data derived therefrom) in order to make parsing decisions. We make use of mutable parse state to enable this form of recall.   We introduce principled stateful parsing as a new transactional discipline that makes state changes transparent to parsing mechanisms such as backtracking and memoization. To enforce this discipline, users specify parsers using formally specified primitive state manipulation operations.   Our solution is available as a parsing library named Autumn. We illustrate our solution by implementing some practical context-sensitive grammar features such as significant whitespace handling and namespace classification."
"Multiparty Session Actors Actor coordination armoured with a suitable protocol description language has been a pressing problem in the actors community. We study the applicability of multiparty session type (MPST) protocols for verification of actor programs. We incorporate sessions to actors by introducing minimum additions to the model such as the notion of actor roles and protocol mailboxes. The framework uses Scribble, which is a protocol description language based on multiparty session types. Our programming model supports actor-like syntax and runtime verification mechanism guaranteeing communication safety of the participating entities. An actor can implement multiple roles in a similar way as an object can implement multiple interfaces. Multiple roles allow for cooperative inter-concurrency in a single actor. We demonstrate our framework by designing and implementing a session actor library in Python and its runtime verification mechanism. Benchmark results demonstrate that the runtime checks induce negligible overhead. We evaluate the applicability of our verification framework to specify actor interactions by implementing twelve examples from an actor benchmark suit."
"FastBDT: A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment."
"UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS In this paper, we present UbuntuWorld 1.0 LTS - a platform for developing automated technical support agents in the Ubuntu operating system. Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu environment for a learning-based agent and demonstrate the usefulness of adopting reinforcement learning (RL) techniques for basic problem solving and troubleshooting in this environment. We provide a plug-and-play interface to the simulator as a python package where different types of agents can be plugged in and evaluated, and provide pathways for integrating data from online support forums like AskUbuntu into an automated agent's learning process. Finally, we show that the use of this data significantly improves the agent's learning efficiency. We believe that this platform can be adopted as a real-world test bed for research on automated technical support."
"PHOTOMETRYPIPELINE: An Automated Pipeline for Calibrated Photometry PHOTOMETRYPIPELINE (PP) is an automated pipeline that produces calibrated photometry from imaging data through image registration, aperture photometry, photometric calibration, and target identification with only minimal human interaction. PP utilizes the widely used Source Extractor software for source identification and aperture photometry; SCAMP is used for image registration. Both image registration and photometric calibration are based on matching field stars with star catalogs, requiring catalog coverage of the respective field. A number of different astrometric and photometric catalogs can be queried online. Relying on a sufficient number of background stars for image registration and photometric calibration, PP is well-suited to analyze data from small to medium-sized telescopes. Calibrated magnitudes obtained by PP are typically accurate within 0.03 mag and astrometric accuracies are of the order of 0.3 arcsec relative to the catalogs used in the registration. The pipeline consists of an open-source software suite written in Python 2.7, can be run on Unix-based systems on a simple desktop machine, and is capable of realtime data analysis. PP has been developed for observations of moving targets, but can be used for analyzing point source observations of any kind."
"Statistical details of the default priors in the Bambi library This is a companion paper to Yarkoni and Westfall (2017), which describes the Python package Bambi for estimating Bayesian generalized linear mixed models using a simple interface. Here I give the statistical details underlying the default, weakly informative priors used in all models when the user does not specify the priors. Our approach is to first deduce what the variances of the slopes would be if we were instead to have defined the priors on the partial correlation scale, and then to set independent Normal priors on the slopes with variances equal to these implied variances. Our approach is similar in spirit to that of Zellner's g-prior (Zellner 1986), in that it involves a multivariate normal prior on the regression slopes, with a tuning parameter to control the width or informativeness of the priors irrespective of the scales of the data and predictors. The primary differences are that here the tuning parameter is directly interpretable as the standard deviation of the distribution of plausible partial correlations, and that this tuning parameter can have different values for different coefficients. The default priors for the intercepts and random effects are ultimately based on the prior slope variances."
"LSSGalPy: Interactive Visualization of the Large-scale Environment Around Galaxies New tools are needed to handle the growth of data in astrophysics delivered by recent and upcoming surveys. We aim to build open-source, light, flexible, and interactive software designed to visualize extensive three-dimensional (3D) tabular data. Entirely written in the Python language, we have developed interactive tools to browse and visualize the positions of galaxies in the universe and their positions with respect to its large-scale structures (LSS). Motivated by a previous study, we created two codes using Mollweide projection and wedge diagram visualizations, where survey galaxies can be overplotted on the LSS of the universe. These are interactive representations where the visualizations can be controlled by widgets. We have released these open-source codes that have been designed to be easily re-used and customized by the scientific community to fulfill their needs. The codes are adaptable to other kinds of 3D tabular data and are robust enough to handle several millions of objects."
"Design Flaws in the Implementation of the Ziggurat and Monty Python methods (and some remarks on Matlab randn) {\em Ziggurat} and {\em Monty Python} are two fast and elegant methods proposed by Marsaglia and Tsang to transform uniform random variables to random variables with normal, exponential and other common probability distributions. While the proposed methods are theoretically correct, we show that there are various design flaws in the uniform pseudo random number generators (PRNG's) of their published implementations for both the normal and Gamma distributions \cite{Ziggurat,{Gamma},Monty}. These flaws lead to non-uniformity of the resulting pseudo-random numbers and consequently to noticeable deviations of their outputs from the required distributions. In addition, we show that the underlying uniform PRNG of the published implementation of Matlab's \texttt{randn}, which is also based on the Ziggurat method, is not uniformly distributed with correlations between consecutive pairs. Also, we show that the simple linear initialization of the registers in matlab's \texttt{randn} may lead to non-trivial correlations between output sequences initialized with different (related or even random unrelated) seeds. These, in turn, may lead to erroneous results for stochastic simulations."
"CplexA: a Mathematica package to study macromolecular-assembly control of gene expression Summary: Macromolecular assembly vertebrates essential cellular processes, such as gene regulation and signal transduction. A major challenge for conventional computational methods to study these processes is tackling the exponential increase of the number of configurational states with the number of components. CplexA is a Mathematica package that uses functional programming to efficiently compute probabilities and average properties over such exponentially large number of states from the energetics of the interactions. The package is particularly suited to study gene expression at complex promoters controlled by multiple, local and distal, DNA binding sites for transcription factors. Availability: CplexA is freely available together with documentation at http://sourceforge.net/projects/cplexa/."
"AMP: A Science-driven Web-based Application for the TeraGrid The Asteroseismic Modeling Portal (AMP) provides a web-based interface for astronomers to run and view simulations that derive the properties of Sun-like stars from observations of their pulsation frequencies. In this paper, we describe the architecture and implementation of AMP, highlighting the lightweight design principles and tools used to produce a functional fully-custom web-based science application in less than a year. Targeted as a TeraGrid science gateway, AMP's architecture and implementation are intended to simplify its orchestration of TeraGrid computational resources. AMP's web-based interface was developed as a traditional standalone database-backed web application using the Python-based Django web development framework, allowing us to leverage the Django framework's capabilities while cleanly separating the user interface development from the grid interface development. We have found this combination of tools flexible and effective for rapid gateway development and deployment."
"Numeric and symbolic evaluation of the pfaffian of general skew-symmetric matrices Evaluation of pfaffians arises in a number of physics applications, and for some of them a direct method is preferable to using the determinantal formula. We discuss two methods for the numerical evaluation of pfaffians. The first is tridiagonalization based on Householder transformations. The main advantage of this method is its numerical stability that makes unnecessary the implementation of a pivoting strategy. The second method considered is based on Aitken's block diagonalization formula. It yields to a kind of LU (similar to Cholesky's factorization) decomposition (under congruence) of arbitrary skew-symmetric matrices that is well suited both for the numeric and symbolic evaluations of the pfaffian. Fortran subroutines (FORTRAN 77 and 90) implementing both methods are given. We also provide simple implementations in Python and Mathematica for purpose of testing, or for exploratory studies of methods that make use of pfaffians."
"LEDDB: LOFAR Epoch of Reionization Diagnostic Database One of the key science projects of the Low-Frequency Array (LOFAR) is the detection of the cosmological signal coming from the Epoch of Reionization (EoR). Here we present the LOFAR EoR Diagnostic Database (LEDDB) that is used in the storage, management, processing and analysis of the LOFAR EoR observations. It stores referencing information of the observations and diagnostic parameters extracted from their calibration. This stored data is used to ease the pipeline processing, monitor the performance of the telescope and visualize the diagnostic parameters which facilitates the analysis of the several contamination effects on the signals. It is implemented with PostgreSQL and accessed through the psycopg2 python module. We have developed a very flexible query engine, which is used by a web user interface to access the database, and a very extensive set of tools for the visualization of the diagnostic parameters through all their multiple dimensions."
"New features of MadAnalysis 5 for analysis design and reinterpretation We present MadAnalysis 5, an analysis package dedicated to phenomenological studies of simulated collisions occurring in high-energy physics experiments. Within this framework, users are invited, through a user-friendly Python interpreter, to implement physics analyses in a very simple manner. A C++ code is then automatically generated, compiled and executed. Very recently, the expert mode of the program has been extended so that analyses with multiple signal/control regions can be handled. Additional observables have also been included, and an interface to several fast detector simulation packages has been developed, one of them being a tune of the Delphes 3 software. As a result, a recasting of existing ATLAS and CMS analyses can be achieved straightforwardly."
"Visual appearance of wireframe objects in special relativity The visual appearance of a moving object in special relativity can be constructed in a straightforward manner when representing the surface of the object, or at least a wire frame model of it, as a point cloud. The apparent position of each individual point is then found by intersecting its worldline with the observer's backward light cone. In this paper, we present a complete derivation of the apparent position of a point and some more complex geometric objects for general parameter settings (configurations). We implemented our results in python and asymptote and used these tools to generate scripts that create the figures in this paper. These scripts are directly applicable in an undergraduate course to special relativity and can also serve as the basis for student projects with the aim to study more complex sceneries."
"VIMAP: an Interactive Program Providing Radio Spectral Index Maps of Active Galactic Nuclei We present a GUI-based interactive Python program, VIMAP, which generates radio spectral index maps of active galactic nuclei (AGN) from Very Long Baseline Interferometry (VLBI) maps obtained at different frequencies. VIMAP is a handy tool for the spectral analysis of synchrotron emission from AGN jets, specifically of spectral index distributions, turn-over frequencies, and core-shifts. In general, the required accurate image alignment is difficult to achieve because of a loss of absolute spatial coordinate information during VLBI data reduction (self-calibration) and/or intrinsic variations of source structure as function of frequency. These issues are overcome by VIMAP which in turn is based on the two-dimensional cross-correlation algorithm of Croke and Gabuzda (2008). In this paper, we briefly review the problem of aligning VLBI AGN maps, describe the workflow of VIMAP, and present an analysis of archival VLBI maps of the active nucleus 3C 120."
"In Defense of Classical Image Processing: Fast Depth Completion on the CPU With the rise of data driven deep neural networks as a realization of universal function approximators, most research on computer vision problems has moved away from hand crafted classical image processing algorithms. This paper shows that with a well designed algorithm, we are capable of outperforming neural network based methods on the task of depth completion. The proposed algorithm is simple and fast, runs on the CPU, and relies only on basic image processing operations to perform depth completion of sparse LIDAR depth data. We evaluate our algorithm on the challenging KITTI depth completion benchmark, and at the time of submission, our method ranks first on the KITTI test server among all published methods. Furthermore, our algorithm is data independent, requiring no training data to perform the task at hand. The code written in Python will be made publicly available at https://github.com/kujason/ip_basic."
"End2You -- The Imperial Toolkit for Multimodal Profiling by End-to-End Learning We introduce End2You -- the Imperial College London toolkit for multimodal profiling by end-to-end deep learning. End2You is an open-source toolkit implemented in Python and is based on Tensorflow. It provides capabilities to train and evaluate models in an end-to-end manner, i.e., using raw input. It supports input from raw audio, visual, physiological or other types of information or combination of those, and the output can be of an arbitrary representation, for either classification or regression tasks. To our knowledge, this is the first toolkit that provides generic end-to-end learning for profiling capabilities in either unimodal or multimodal cases. To test our toolkit, we utilise the RECOLA database as was used in the AVEC 2016 challenge. Experimental results indicate that End2You can provide comparable results to state-of-the-art methods despite no need of expert-alike feature representations, but self-learning these from the data ""end to end""."
Vectorized Calculation of Short Circuit Currents Considering Distributed Generation - An Open Source Implementation of IEC 60909 An important task in grid planning is to ensure that faults in the grid are detected and cut off without damage in any grid element. Calculating short circuit currents is therefore a vital grid analysis functionality for grid planning applications. The standard IEC 60909 provides guidelines for short circuit calculations and is routinely applied in grid planning applications. This paper presents a method for the vectorized calculation of short circuit currents according to IEC 60909. Distributed generation units are considered according to the latest revision of the standard. The method is implemented in the python based open source tool pandapower and validated against commercial software and examples from literature. The implementation presented in this paper is the first comprehensive implementation of the IEC 60909 standard which is available under an open source license. It can be used to evaluate fault currents in grid studies with a high degree of automation and is shown to scale well for large grids. Its practical applicability is shown in a case study with a real MV grid with a high degree of DG penetration.
"Praaline: Integrating Tools for Speech Corpus Research This paper presents Praaline, an open-source software system for managing, annotating, analysing and visualising speech corpora. Researchers working with speech corpora are often faced with multiple tools and formats, and they need to work with ever-increasing amounts of data in a collaborative way. Praaline integrates and extends existing time-proven tools for spoken corpora analysis (Praat, Sonic Visualiser and a bridge to the R statistical package) in a modular system, facilitating automation and reuse. Users are exposed to an integrated, user-friendly interface from which to access multiple tools. Corpus metadata and annotations may be stored in a database, locally or remotely, and users can define the metadata and annotation structure. Users may run a customisable cascade of analysis steps, based on plug-ins and scripts, and update the database with the results. The corpus database may be queried, to produce aggregated data-sets. Praaline is extensible using Python or C++ plug-ins, while Praat and R scripts may be executed against the corpus data. A series of visualisations, editors and plug-ins are provided. Praaline is free software, released under the GPL license."
"Web-Based Implementation of Travelling Salesperson Problem Using Genetic Algorithm The world is connected through the Internet. As the abundance of Internet users connected into the Web and the popularity of cloud computing research, the need of Artificial Intelligence (AI) is demanding. In this research, Genetic Algorithm (GA) as AI optimization method through natural selection and genetic evolution is utilized. There are many applications of GA such as web mining, load balancing, routing, and scheduling or web service selection. Hence, it is a challenging task to discover whether the code mainly server side and web based language technology affects the performance of GA. Travelling Salesperson Problem (TSP) as Non Polynomial-hard (NP-hard) problem is provided to be a problem domain to be solved by GA. While many scientists prefer Python in GA implementation, another popular high-level interpreter programming language such as PHP (PHP Hypertext Preprocessor) and Ruby were benchmarked. Line of codes, file sizes, and performances based on GA implementation and runtime were found varies among these programming languages. Based on the result, the use of Ruby in GA implementation is recommended."
"Communication-Computation Efficient Gradient Coding This paper develops coding techniques to reduce the running time of distributed learning tasks. It characterizes the fundamental tradeoff to compute gradients (and more generally vector summations) in terms of three parameters: computation load, straggler tolerance and communication cost. It further gives an explicit coding scheme that achieves the optimal tradeoff based on recursive polynomial constructions, coding both across data subsets and vector components. As a result, the proposed scheme allows to minimize the running time for gradient computations. Implementations are made on Amazon EC2 clusters using Python with mpi4py package. Results show that the proposed scheme maintains the same generalization error while reducing the running time by $32\%$ compared to uncoded schemes and $23\%$ compared to prior coded schemes focusing only on stragglers (Tandon et al., ICML 2017)."
"PyFml - a Textual Language For Feature Modeling The Feature model is a typical approach to capture variability in a software product line design and implementation. For that, most works automate feature model using a limited graphical notation represented by propositional logic and implemented by Prolog or Java programming languages. These works do not properly combine the extensions of classical feature models and do not provide scalability to implement large size problem issues. In this work, we propose a textual feature modeling language based on Python programming language (PyFML), that generalizes the classical feature models with instance feature cardinalities and attributes which be extended with highlight of replication and complex logical and mathematical cross-tree constraints. textX Meta-language is used for building PyFML to describe and organize feature model dependencies, and PyConstraint Problem Solver is used to implement feature model variability and its constraints validation. The work provides a textual human-readable language to represent feature model and maps the feature model descriptions directly into the object-oriented representation to be used by Constraint Problem Solver for computation. Furthermore, the proposed PyFML makes the notation of feature modeling more expressive to deal with complex software product line representations and using PyConstraint Problem Solver"
"CREPE: A Convolutional Representation for Pitch Estimation The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application."
"Computing the Cumulative Distribution Function and Quantiles of the One-sided Kolmogorov-Smirnov Statistic The cumulative distribution and quantile functions for the one-sided one sample Kolmogorov-Smirnov probability distributions are used for goodness-of-fit testing. While the Smirnov-Birnbaum-Tingey formula for the CDF appears straight forward, its numerical evaluation generates intermediate results spanning many hundreds of orders of magnitude and at times requires very precise accurate representations. Computing the quantile function for any specific probability may require evaluating both the CDF and its derivative, both of which are computationally expensive. To work around avoid these issues, different algorithms can be used across different parts of the domain, and approximations can be used to reduce the computational requirements. We show here that straight forward implementation incurs accuracy loss for sample sizes of well under 1000. Further the approximations in use inside the open source SciPy python software often result in increased computation, not just reduced accuracy, and at times suffer catastrophic loss of accuracy for any sample size. Then we provide alternate algorithms which restore accuracy and efficiency across the whole domain."
"AGAMA: Action-based galaxy modelling architecture Agama is a publicly available software library for a broad range of applications in the field of stellar dynamics. It provides methods for computing the gravitational potential of arbitrary analytic density profiles or N-body models; orbit integration and analysis; transformations between position/velocity and action/angle variables; distribution functions expressed in terms of actions and their moments; iterative construction of self-consistent multicomponent galaxy models. Applications include the inference about the structure of Milky Way or other galaxies from observations of stellar kinematics; preparation of equilibrium initial conditions for N-body simulations; analysis of snapshots from simulations. The library is written in C++, provides a Python interface, and can be coupled to other stellar-dynamical software: Amuse, Galpy and Nemo."
"PySE: Software for Extracting Sources from Radio Images PySE is a Python software package for finding and measuring sources in radio telescope images. The software was designed to detect sources in the LOFAR telescope images, but can be used with images from other radio telescopes as well. We introduce the LOFAR Telescope, the context within which PySE was developed, the design of PySE, and describe how it is used. Detailed experiments on the validation and testing of PySE are then presented, along with results of performance testing. We discuss some of the current issues with the algorithms implemented in PySE and their inter- action with LOFAR images, concluding with the current status of PySE and its future development."
"A scripted control system for autonomous hardware timed experiments We present the labscript suite, an open-source experiment control system for automating shot-based experiments and their analysis. Experiments are composed as Python code, which is used to produce low-level hardware instructions. They are queued up and executed on the hardware in real time, synchronized by a pseudoclock. Experiment parameters are manipulated graphically, and analysis routines are run as new data is acquired. With this system, we can easily automate exploration of parameter spaces, including closed-loop optimization."
"Libsharp - spherical harmonic transforms revisited We present libsharp, a code library for spherical harmonic transforms (SHTs), which evolved from the libpsht library, addressing several of its shortcomings, such as adding MPI support for distributed memory systems and SHTs of fields with arbitrary spin, but also supporting new developments in CPU instruction sets like the Advanced Vector Extensions (AVX) or fused multiply-accumulate (FMA) instructions. The library is implemented in portable C99 and provides an interface that can be easily accessed from other programming languages such as C++, Fortran, Python etc. Generally, libsharp's performance is at least on par with that of its predecessor; however, significant improvements were made to the algorithms for scalar SHTs, which are roughly twice as fast when using the same CPU capabilities. The library is available at http://sourceforge.net/projects/libsharp/ under the terms of the GNU General Public License."
"The Astrophysical Multipurpose Software Environment We present the open source Astrophysical Multi-purpose Software Environment (AMUSE, www.amusecode.org), a component library for performing astrophysical simulations involving different physical domains and scales. It couples existing codes within a Python framework based on a communication layer using MPI. The interfaces are standardized for each domain and their implementation based on MPI guarantees that the whole framework is well-suited for distributed computation. It includes facilities for unit handling and data storage. Currently it includes codes for gravitational dynamics, stellar evolution, hydrodynamics and radiative transfer. Within each domain the interfaces to the codes are as similar as possible. We describe the design and implementation of AMUSE, as well as the main components and community codes currently supported and we discuss the code interactions facilitated by the framework. Additionally, we demonstrate how AMUSE can be used to resolve complex astrophysical problems by presenting example applications."
"Fast polynomial evaluation and composition The library \emph{fast\_polynomial} for Sage compiles multivariate polynomials for subsequent fast evaluation. Several evaluation schemes are handled, such as H\""orner, divide and conquer and new ones can be added easily. Notably, a new scheme is introduced that improves the classical divide and conquer scheme when the number of terms is not a pure power of two. Natively, the library handles polynomials over gmp big integers, boost intervals, python numeric types. And any type that supports addition and multiplication can extend the library thanks to the template design. Finally, the code is parallelized for the divide and conquer schemes, and memory allocation is localized and optimized for the different evaluation schemes. This extended abstract presents the concepts behind the \emph{fast\_polynomial} library. The sage package can be downloaded at \url{http://trac.sagemath.org/sage_trac/ticket/13358}."
"Fitting Afterglows With Multi-Dimensional Simulations We present preliminary data fit results of synthetic light curves computed from numerical afterglow blast wave simulations. Our technique uses Markov chain Monte Carlo (MCMC) in a new data analysis tool, ScaleFit. Scaling relations in both the hydrodynamics and radiation equations allow light curves to be parameterized by a small set of scale-invariant characteristic quantities. These quantities have been calculated and tabulated from high resolution two-dimensional hydrodynamic simulations. Producing a light curve from the characteristics takes only a millisecond, allowing for the use of MCMC data fitting techniques which can require millions of iterations. ScaleFit is a portable, lightweight, python package which performs this analysis on afterglow light curves. Using the set of Swift-XRT light curves from 2011 & 2012 with known redshifts, we find ScaleFit can measure the jet opening angle, observer angle, and spectral index of most afterglows. Globally we find gamma-ray burst afterglows tend to be observed off axis, at a significant fraction of the jet opening angle."
"Geant4 based simulations for novel neutron detector development A Geant4-based Python/C++ simulation and coding framework, which has been developed and used in order to aid the R&D efforts for thermal neutron detectors at neutron scattering facilities, is described. Built upon configurable geometry and generator modules, it integrates a general purpose object oriented output file format with meta-data, developed in order to facilitate a faster turn-around time when setting up and analysing simulations. Also discussed are the extensions to Geant4 which have been implemented in order to include the effects of low-energy phenomena such as Bragg diffraction in the polycrystalline support materials of the detector. Finally, an example application of the framework is briefly shown."
"ProMC: Input-output data format for HEP applications using varint encoding A new data format for Monte Carlo (MC) events, or any structural data, including experimental data, is discussed. The format is designed to store data in a compact binary form using variable-size integer encoding as implemented in the Google's Protocol Buffers package. This approach is implemented in the ProMC library which produces smaller file sizes for MC records compared to the existing input-output libraries used in high-energy physics (HEP). Other important features of the proposed format are a separation of abstract data layouts from concrete programming implementations, self-description and random access. Data stored in ProMC files can be written, read and manipulated in a number of programming languages, such C++, JAVA, FORTRAN and PYTHON."
"Sailfish: a flexible multi-GPU implementation of the lattice Boltzmann method We present Sailfish, an open source fluid simulation package implementing the lattice Boltzmann method (LBM) on modern Graphics Processing Units (GPUs) using CUDA/OpenCL. We take a novel approach to GPU code implementation and use run-time code generation techniques and a high level programming language (Python) to achieve state of the art performance, while allowing easy experimentation with different LBM models and tuning for various types of hardware. We discuss the general design principles of the code, scaling to multiple GPUs in a distributed environment, as well as the GPU implementation and optimization of many different LBM models, both single component (BGK, MRT, ELBM) and multicomponent (Shan-Chen, free energy). The paper also presents results of performance benchmarks spanning the last three NVIDIA GPU generations (Tesla, Fermi, Kepler), which we hope will be useful for researchers working with this type of hardware and similar codes."
"Distributed Multiscale Computing with MUSCLE 2, the Multiscale Coupling Library and Environment We present the Multiscale Coupling Library and Environment: MUSCLE 2. This multiscale component-based execution environment has a simple to use Java, C++, C, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We demonstrate its local and distributed computing capabilities and compare its performance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local throughput of MPI is about two times higher, so very tightly coupled code should use MPI as a single submodel of MUSCLE 2; the distributed performance of GridFTP is lower, especially for small messages. We test the performance of a canal system model with MUSCLE 2, where it introduces an overhead as small as 5% compared to MPI."
Connecting Simplified Models: Constraining Supersymmetry on Triangles We investigate an approach for the presentation of experimental constraints on supersymmetric scenarios. It is a triangle based visualization that extends the status quo wherein LHC results are reported in terms of simplified models under the assumption of 100\% branching ratios. We show that the (re)interpretation of LHC data on triangles allows the extraction of accurate exclusion limits for a multitude of more realistic models with arbitrary branching ratios. We demonstrate the utility of this triangle visualization approach using the example of gluino production and decay in several common supersymmetric scenarios. A Python script that can be adapted to visualize data on triangular plots can be obtained from https://github.com/renuk16/Triangles.
"Link aggregation process for modelling weighted mutualistic networks Mutualism is a biological interaction mutually beneficial for both species involved, such as the interaction between plants and their pollinators. Real mutualistic communities can be understood as weighted bipartite networks and they present a nested structure and truncated power law degree and strength distributions. We present a novel link aggregation model that works on a strength-preferential attachment rule based on the Individual Neutrality hypothesis. The model generates mutualistic networks with emergent nestedness and truncated distributions. We provide some analytical results and compare the simulated and empirical network topology. Upon further improving the shape of the distributions, we have also studied the role of forbidden interactions on the model and found that the inclusion of forbidden links does not prevent for the appearance of super-generalist species. A Python script with the model algorithms is available."
"A modified ziggurat algorithm for generating exponentially- and normally-distributed pseudorandom numbers The Ziggurat Algorithm is a very fast rejection sampling method for generating PseudoRandom Numbers (PRNs) from common statistical distributions. The algorithm divides a distribution into rectangular layers that stack on top of each other (resembling a Ziggurat), subsuming the desired distribution. Random values within these rectangular layers are then sampled by rejection. This implementation splits layers into two types: those constituting the majority that fall completely under the distribution and can be sampled extremely fast without a rejection test, and a few additional layers that encapsulate the fringe of the distribution and require a rejection test. This method offers speedups of 65% for exponentially- and 82% for normally-distributed PRNs when compared to the best available C implementations of these generators. Even greater speedups are obtained when the algorithm is extended to the Python and MATLAB/OCTAVE programming environments."
"Extending the range of real time density matrix renormalization group simulations We discuss a few simple modifications to time-dependent density matrix renormalization group (DMRG) algorithms which allow to access larger time scales. We specifically aim at beginners and present practical aspects of how to implement these modifications almost effortlessly within any standard matrix product state (MPS) based formulation of the method. Most importantly, we show how to 'combine' the Schroedinger and Heisenberg time evolutions of arbitrary pure states |psi> and operators A in the evaluation of <A>_psi(t)=<psi|A(t)|psi>. This includes quantum quenches. The generalization (non-)thermal mixed state dynamics <A>_rho(t)=Tr[rho A(t)] induced by an initial density matrix rho is straightforward. In the context of equilibrium (ground state or finite temperature T>0) correlation functions, one can extend the simulation time by a factor of two by 'exploiting time translation invariance', which is efficiently implementable within MPS DMRG. We present a simple analytic argument for why a recently-introduced disentangler succeeds in reducing the effort of time-dependent simulations at T>0. Finally, we advocate the python programming language as an elegant option for beginners to set up a DMRG code."
"CATOS: Computer Aided Training/Observing System In animal behavioral biology, there are several cases in which an autonomous observing/training system would be useful. 1) Observation of certain species continuously, or for documenting specific events, which happen irregularly; 2) Longterm intensive training of animals in preparation for behavioral experiments; and 3) Training and testing of animals without human interference, to eliminate potential cues and biases induced by humans. The primary goal of this study is to build a system named CATOS (Computer Aided Training/Observing System) that could be used in the above situations. As a proof of concept, the system was built and tested in a pilot experiment, in which cats were trained to press three buttons differently in response to three different sounds (human speech) to receive food rewards. The system was built in use for about 6 months, successfully training two cats. One cat learned to press a particular button, out of three buttons, to obtain the food reward with over 70 percent correctness."
"Computing an Optimal Control Policy for an Energy Storage We introduce StoDynProg, a small library created to solve Optimal Control problems arising in the management of Renewable Power Sources, in particular when coupled with an Energy Storage System. The library implements generic Stochastic Dynamic Programming (SDP) numerical methods which can solve a large class of Dynamic Optimization problems. We demonstrate the library capabilities with a prototype problem: smoothing the power of an Ocean Wave Energy Converter. First we use time series analysis to derive a stochastic Markovian model of this system since it is required by Dynamic Programming. Then, we briefly describe the ""policy iteration"" algorithm we have implemented and the numerical tools being used. We show how the API design of the library is generic enough to address Dynamic Optimization problems outside the field of Energy Management. Finally, we solve the power smoothing problem and compare the optimal control with a simpler heuristic control."
"How to extract data from proprietary software database systems using TCP/IP? This document is an white paper about how to connect reverse engineering and programing skills to extract data from a proprietary implementation of a database system to build EML-Tools for data format conversion into raw data. This article shows how to access data of a source software system without any interface for data conversion. We discuss how raw data can be transfered into structural format by using XML or any other custom designed software solution. For demonstration purposes only, we will use a CRM system called Harmony(r) by Harmony(r) Software AG, the programing language Python and methods of computer security, which are used to get quick access to the raw data.   All trademarks are property of their owners, as Harmony(r) is of Harmony Software AG."
"How to efficiently select an arbitrary Clifford group element We give an algorithm which produces a unique element of the Clifford group C_n on n qubits from an integer 0\le i < |C_n| (the number of elements in the group). The algorithm involves O(n^3) operations. It is a variant of the subgroup algorithm by Diaconis and Shahshahani which is commonly applied to compact Lie groups. We provide an adaption for the symplectic group Sp(2n,F_2) which provides, in addition to a canonical mapping from the integers to group elements g, a factorization of g into a sequence of at most 4n symplectic transvections. The algorithm can be used to efficiently select random elements of C_n which is often useful in quantum information theory and quantum computation. We also give an algorithm for the inverse map, indexing a group element in time O(n^3)."
"Multiparty Session Actors Actor coordination armoured with a suitable protocol description language has been a pressing problem in the actors community. We study the applicability of multiparty session type (MPST) protocols for verification of actor programs. We incorporate sessions to actors by introducing minimum additions to the model such as the notion of actor roles and protocol mailbox. The framework uses Scribble, which is a protocol description language based on multiparty session types. Our programming model supports actor-like syntax and runtime verification mechanism guaranteeing type-safety and progress of the communicating entities. An actor can implement multiple roles in a similar way as an object can implement multiple interfaces. Multiple roles allow for inter-concurrency in a single actor still preserving its progress property. We demonstrate our framework by designing and implementing a session actor library in Python and its runtime verification mechanism."
"The SpeX Prism Library: 1000+ Low-resolution, Near-infrared Spectra of Ultracool M, L, T and Y Dwarfs The SpeX Prism Library (SPL) is a uniform compilation of low-resolution (R ~ 75-120), near-infrared (0.8-2.5 micron) spectra spanning a decade of observations with the IRTF SpeX spectrograph. Primarily containing ultracool M, L, T and Y dwarfs, this spectral library has been used in over 100 publications to date, facilitating a broad range of science on low mass stars, exoplanets, high redshift sources and instrument/survey design. I summarize the contents of the SPL and highlight a few of the key scientific results that have made use of this resource, as well as applications in education, outreach and art. I also outline the future plans of the SPL, which include a reanalysis of early data, better integration and dissemination of source and spectral metadata, conversion to Virtual Observatory formats, development of a Python software package for community analysis, and a design for a node-based visual programming platform that can facilitate citizen science and project-based learning in stellar spectroscopy."
"Correcting for Telluric Absorption: Methods, Case Studies, and Release of the TelFit Code Ground-based astronomical spectra are contaminated by the Earth's atmosphere to varying degrees in all spectral regions. We present a Python code that can accurately fit a model to the telluric absorption spectrum present in astronomical data, with residuals of $\sim 3-5\%$ of the continuum for moderately strong lines. We demonstrate the quality of the correction by fitting the telluric spectrum in a nearly featureless A0V star, HIP 20264, as well as to a series of dwarf M star spectra near the 819 nm sodium doublet. We directly compare the results to an empirical telluric correction of HIP 20264 and find that our model-fitting procedure is at least as good and sometimes more accurate. The telluric correction code, which we make freely available to the astronomical community, can be used as a replacement for telluric standard star observations for many purposes."
"Nuclear fission as resonance-mediated conductance For 75 years the theory of nuclear fission has been based on the existence of a collective coordinate associated with the nuclear shape, an assumption required by the Bohr-Wheeler formula as well as by the R-matrix theory of fission. We show that it is also possible to formulate the theory without the help of collective coordinates. In the new formulation, fission is facilitated by individual states in the barrier region rather than channels over the barrier. In a certain limit the theory reduces to a formula closely related to the formula for electronic conductance through resonant tunneling states. In contrast, conduction through channels gives rise to a staircase excitation function that is well-known in nanoscale electronics but has never been seen in nuclear fission."
"Determining stellar atmospheric parameters and chemical abundances of FGK stars with iSpec Context. An increasing number of high-resolution stellar spectra is available today thanks to many past and ongoing extensive spectroscopic surveys. Consequently, the scientific community needs automatic procedures to derive atmospheric parameters and individual element abundances.   Aims. Based on the widely known SPECTRUM code by R. O. Gray, we developed an integrated spectroscopic software framework suitable for the determination of atmospheric parameters (i.e., effective temperature, surface gravity, metallicity) and individual chemical abundances. The code, named iSpec and freely distributed, is written mainly in Python and can be used on different platforms.   Methods. iSpec can derive atmospheric parameters by using the synthetic spectral fitting technique and the equivalent width method. We validated the performance of both approaches by developing two different pipelines and analyzing the Gaia FGK benchmark stars spectral library. The analysis was complemented with several tests designed to assess other aspects, such as the interpolation of model atmospheres and the performance with lower quality spectra.   Results. We provide a code ready to perform automatic stellar spectral analysis. We successfully assessed the results obtained for FGK stars with high-resolution and high signal-to-noise spectra."
"Modeling structural change in spatial system dynamics: A Daisyworld example System dynamics (SD) is an effective approach for helping reveal the temporal behavior of complex systems. Although there have been recent developments in expanding SD to include systems' spatial dependencies, most applications have been restricted to the simulation of diffusion processes; this is especially true for models on structural change (e.g. LULC modeling). To address this shortcoming, a Python program is proposed to tightly couple SD software to a Geographic Information System (GIS). The approach provides the required capacities for handling bidirectional and synchronized interactions of operations between SD and GIS. In order to illustrate the concept and the techniques proposed for simulating structural changes, a fictitious environment called Daisyworld has been recreated in a spatial system dynamics (SSD) environment. The comparison of spatial and non-spatial simulations emphasizes the importance of considering spatio-temporal feedbacks. Finally, practical applications of structural change models in agriculture and disaster management are proposed."
"spotrod: a semi-analytic model for transits of spotted stars The Hubble Space Telescope (HST) and the Kepler space mission observed a large number of planetary transits showing anomalies due to starspot eclipses, with more such observations expected in the near future by the K2 mission and the Transiting Exoplanet Survey Satellite (TESS). To facilitate analysis of this phenomenon, we present spotrod, a model for planetary transits of stars with an arbitrary limb darkening law and a number of homogeneous, circular spots on their surface. A free, open source implementation written in C, ready to use in Python, is available for download.   We analyze Kepler observations of the planetary host star HAT-P-11, and study the size and contrast of more than two hundred starspots. We find that the flux ratio of spots ranges at least from 0.6 to 0.9, corresponding to an effective temperature approximately 100 to 450 K lower than the stellar surface, although it is possible that some spots are darker than 0.5. The largest detected spots have a radius less than approximately 0.2 stellar radii."
"Automated procedure to derive fundamental parameters of B and A stars: Application to the young cluster NGC 3293 This work describes a procedure to derive several fundamental parameters such as the effective temperature, surface gravity, equatorial rotational velocity and microturbulent velocity. In this work, we have written a numerical procedure in Python which finds the best fit between a grid of synthetic spectra and the observed spectra by minimizing a standard chi-square. LTE model atmospheres were calculated using the ATLAS9 code and were used as inputs to the spectrum synthesis code SYNSPEC48 in order to compute a large grid of synthetic Balmer line profiles. This new procedure has been applied to a large number of new observations (GIRAFFE spectra) of B and A stars members of the young open cluster NGC3293. These observations are part of the GAIA ESO Survey. Takeda's procedure was also used to derive rotational velocities and microturbulent velocities. The results have been compared to previous determinations by other authors and are found to agree with them. As a first result, we concluded that using this procedure, an accuracy of +-200 K could be achieved in effective temperature and +-0.2 dex in surface gravities."
"Abstractions, Algorithms and Data Structures for Structural Bioinformatics in PyCogent To facilitate flexible and efficient structural bioinformatics analyses, new functionality for three-dimensional structure processing and analysis has been introduced into PyCogent -- a popular feature-rich framework for sequence-based bioinformatics, but one which has lacked equally powerful tools for handling stuctural/coordinate-based data. Extensible Python modules have been developed, which provide object-oriented abstractions (based on a hierarchical representation of macromolecules), efficient data structures (e.g. kD-trees), fast implementations of common algorithms (e.g. surface-area calculations), read/write support for Protein Data Bank-related file formats and wrappers for external command-line applications (e.g. Stride). Integration of this code into PyCogent is symbiotic, allowing sequence-based work to benefit from structure-derived data and, reciprocally, enabling structural studies to leverage PyCogent's versatile tools for phylogenetic and evolutionary analyses."
"Process-Oriented Parallel Programming with an Application to Data-Intensive Computing We introduce process-oriented programming as a natural extension of object-oriented programming for parallel computing. It is based on the observation that every class of an object-oriented language can be instantiated as a process, accessible via a remote pointer. The introduction of process pointers requires no syntax extension, identifies processes with programming objects, and enables processes to exchange information simply by executing remote methods. Process-oriented programming is a high-level language alternative to multithreading, MPI and many other languages, environments and tools currently used for parallel computations. It implements natural object-based parallelism using only minimal syntax extension of existing languages, such as C++ and Python, and has therefore the potential to lead to widespread adoption of parallel programming. We implemented a prototype system for running processes using C++ with MPI and used it to compute a large three-dimensional Fourier transform on a computer cluster built of commodity hardware components. Three-dimensional Fourier transform is a prototype of a data-intensive application with a complex data-access pattern. The process-oriented code is only a few hundred lines long, and attains very high data throughput by achieving massive parallelism and maximizing hardware utilization."
"Assembly of repetitive regions using next-generation sequencing data High read depth can be used to assemble short sequence repeats. The existing genome assemblers fail in repetitive regions of longer than average read.   I propose a new algorithm for a DNA assembly which uses the relative frequency of reads to properly reconstruct repetitive sequences. The mathematical model shows the upper limits of accuracy of the results as a function of read coverage. For high coverage, the estimation error depends linearly on repetitive sequence length and inversely proportional to the sequencing coverage. The algorithm requires high read depth, provided by the next-generation sequencers and could use the existing data. The tests on errorless reads, generated in silico from several model genomes, pointed the properly reconstructed repetitive sequences, where existing assemblers fail."
"Introduction to astroML: Machine Learning for Astrophysics Astronomy and astrophysics are witnessing dramatic increases in data volume as detectors, telescopes and computers become ever more powerful. During the last decade, sky surveys across the electromagnetic spectrum have collected hundreds of terabytes of astronomical data for hundreds of millions of sources. Over the next decade, the data volume will enter the petabyte domain, and provide accurate measurements for billions of sources. Astronomy and physics students are not traditionally trained to handle such voluminous and complex data sets. In this paper we describe astroML; an initiative, based on Python and scikit-learn, to develop a compendium of machine learning tools designed to address the statistical needs of the next generation of students and astronomical surveys. We introduce astroML and present a number of example applications that are enabled by this package."
"False discovery rate smoothing We present false discovery rate smoothing, an empirical-Bayes method for exploiting spatial structure in large multiple-testing problems. FDR smoothing automatically finds spatially localized regions of significant test statistics. It then relaxes the threshold of statistical significance within these regions, and tightens it elsewhere, in a manner that controls the overall false-discovery rate at a given level. This results in increased power and cleaner spatial separation of signals from noise. The approach requires solving a non-standard high-dimensional optimization problem, for which an efficient augmented-Lagrangian algorithm is presented. In simulation studies, FDR smoothing exhibits state-of-the-art performance at modest computational cost. In particular, it is shown to be far more robust than existing methods for spatially dependent multiple testing. We also apply the method to a data set from an fMRI experiment on spatial working memory, where it detects patterns that are much more biologically plausible than those detected by standard FDR-controlling methods. All code for FDR smoothing is publicly available in Python and R."
"An Abstract Interpretation-based Model of Tracing Just-In-Time Compilation Tracing just-in-time compilation is a popular compilation technique for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python and PHP. We provide a formal model of tracing JIT compilation of programs using abstract interpretation. Hot path detection corresponds to an abstraction of the trace semantics of the program. The optimization phase corresponds to a transform of the original program that preserves its trace semantics up to an observation modeled by some abstraction. We provide a generic framework to express dynamic optimizations and prove them correct. We instantiate it to prove the correctness of dynamic type specialization and constant variable folding. We show that our framework is more general than the model of tracing compilation introduced by Guo and Palsberg [2011] based on operational bisimulations."
"A family of lowered isothermal models We present a family of self-consistent, spherical, lowered isothermal models, consisting of one or more mass components, with parameterised prescriptions for the energy truncation and for the amount of radially biased pressure anisotropy. The models are particularly suited to describe the phase-space density of stars in tidally limited, mass-segregated star clusters in all stages of their life-cycle. The models extend a family of isotropic, single-mass models by Gomez-Leyton and Velazquez, of which the well-known Woolley, King and Wilson (in the non-rotating and isotropic limit) models are members. We derive analytic expressions for the density and velocity dispersion components in terms of potential and radius, and introduce a fast model solver in PYTHON (LIMEPY), that can be used for data fitting or for generating discrete samples."
"PySensors: A Python Package for Sparse Sensor Placement PySensors is a Python package for selecting and placing a sparse set of sensors for classification and reconstruction tasks. Specifically, PySensors implements algorithms for data-driven sparse sensor placement optimization for reconstruction (SSPOR) and sparse sensor placement optimization for classification (SSPOC). In this work we provide a brief description of the mathematical algorithms and theory for sparse sensor optimization, along with an overview and demonstration of the features implemented in PySensors (with code examples). We also include practical advice for user and a list of potential extensions to PySensors. Software is available at https://github.com/dynamicslab/pysensors."
"ShowerModel: A Python package for modelling cosmic-rayshowers, their light production and their detection Cosmic-ray observatories necessarily rely on Monte Carlo simulations for their design, calibration and analysis of their data. Detailed simulations are very demanding computationally. We present a python-based package called ShowerModel to model cosmic-ray showers, their light production and their detection by an array of telescopes. It is based on parameterizations of both Cherenkov and fluorescence emission in cosmic-ray induced air showers. The package permits the modelling of fluorescence telescopes, imaging air Cherenkov telescopes, wide-angle Cherenkov detectors or any hybrid design.   ShowerModel was conceived as a tool to speed up calculations that do not require a full simulation or that may serve to complement complex Monte Carlo studies and data analyses (e.g., as a cross-check). It can also be used for educational purposes."
"PyQUBO: Python Library for Mapping Combinatorial Optimization Problems to QUBO Form We present PyQUBO, an open-source, Python library for constructing quadratic unconstrained binary optimizations (QUBOs) from the objective functions and the constraints of optimization problems. PyQUBO enables users to prepare QUBOs or Ising models for various combinatorial optimization problems with ease thanks to the abstraction of expressions and the extensibility of the program. QUBOs and Ising models formulated using PyQUBO are solvable by Ising machines, including quantum annealing machines. We introduce the features of PyQUBO with applications in the number partitioning problem, knapsack problem, graph coloring problem, and integer factorization using a binary multiplier. Moreover, we demonstrate how PyQUBO can be applied to production-scale problems through integration with quantum annealing machines. Through its flexibility and ease of use, PyQUBO has the potential to make quantum annealing a more practical tool among researchers."
"PyRCN: Exploration and Application of ESNs As a family member of Recurrent Neural Networks and similar to Long-Short-Term Memory cells, Echo State Networks (ESNs) are capable of solving temporal tasks, but with a substantially easier training paradigm based on linear regression. However, optimizing hyper-parameters and efficiently implementing the training process might be somewhat overwhelming for the first-time users of ESNs. This paper aims to facilitate the understanding of ESNs in theory and practice. Treating ESNs as non-linear filters, we explain the effect of the hyper-parameters using familiar concepts such as impulse responses. Furthermore, the paper introduces the Python toolbox PyRCN (Python Reservoir Computing Network) for developing, training and analyzing ESNs on arbitrarily large datasets. The tool is based on widely-used scientific packages, such as numpy and scipy and offers an interface to scikit-learn. Example code and results for classification and regression tasks are provided."
"Easy asteroid phase curve fitting for the Python ecosystem: Pyedra A trending astronomical phenomenon to study is the variation in brightness of asteroids, caused by its rotation on its own axis, non-spherical shapes, changes of albedo along its surface and its position relative to the sun. The latter behaviour can be visualized on a ""Phase Curve"" (phase angle vs. reduced magnitude). To enable the comparison between several models proposed for this curve we present a Python package called Pyedra. Pyedra implements three phase-curve-models, and also providing capabilities for visualization as well as integration with external datasets. The package is fully documented and tested following a strict quality-assurance workflow, with a user-friendly programmatic interface. In future versions, we will include more models, and additional estimation of quantities derived from parameters like diameter, and types of albedo; as well as enabling correlation of information of physical and orbital parameters."
"Riroriro: Simulating gravitational waves and evaluating their detectability in Python Riroriro is a Python package to simulate the gravitational waveforms of binary mergers of black holes and/or neutron stars, and calculate several properties of these mergers and waveforms, specifically relating to their observability by gravitational wave detectors.   The gravitational waveform simulation of Riroriro is based upon the methods of Buskirk and Babiuc-Hamilton (2019), a paper which describes a computational implementation of an earlier theoretical gravitational waveform model by Huerta et al. (2017), using post-Newtonian expansions and an approximation called the implicit rotating source to simplify the Einstein field equations and simulate gravitational waves. Riroriro's calculation of signal-to-noise ratios (SNR) of gravitational wave events is based on the methods of Barrett et al. (2018), with the simpler gravitational wave model Findchirp (Allen et al. (2012)) being used for comparison and calibration in these calculations."
"Upgrade of Hardware Controls for the STAR Experiment at RHIC The STAR experiment has been delivering significant physics results for more than 20 years. Stable operation of the experiment was achieved by using a robust controls system based on the Experimental Physics and Industrial Control System (EPICS). Now an object-oriented approach with Python libraries, adapted for EPICS software, is going to replace the procedural-based EPICS C libraries previously used at STAR. Advantages of the new approach include stability of operation, code reduction and straightforward project documentation. The first two sections of this paper introduce the STAR experiment, give an overview of the EPICS architecture, and present the use of Python for controls software. Specific examples, as well as upgrades of user interfaces, are outlined in the following sections."
"KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle Computational notebooks have become the tool of choice for many data scientists and practitioners for performing analyses and disseminating results. Despite their increasing popularity, the research community cannot yet count on a large, curated dataset of computational notebooks. In this paper, we fill this gap by introducing KGTorrent, a dataset of Python Jupyter notebooks with rich metadata retrieved from Kaggle, a platform hosting data science competitions for learners and practitioners with any levels of expertise. We describe how we built KGTorrent, and provide instructions on how to use it and refresh the collection to keep it up to date. Our vision is that the research community will use KGTorrent to study how data scientists, especially practitioners, use Jupyter Notebook in the wild and identify potential shortcomings to inform the design of its future extensions."
"Text Normalization for Low-Resource Languages of Africa Training data for machine learning models can come from many different sources, which can be of dubious quality. For resource-rich languages like English, there is a lot of data available, so we can afford to throw out the dubious data. For low-resource languages where there is much less data available, we can't necessarily afford to throw out the dubious data, in case we end up with a training set which is too small to train a model. In this study, we examine the effects of text normalization and data set quality for a set of low-resource languages of Africa -- Afrikaans, Amharic, Hausa, Igbo, Malagasy, Somali, Swahili, and Zulu. We describe our text normalizer which we built in the Pynini framework, a Python library for finite state transducers, and our experiments in training language models for African languages using the Natural Language Toolkit (NLTK), an open-source Python library for NLP."
"PySDM v1: particle-based cloud modelling package for warm-rain microphysics and aqueous chemistry PySDM is an open-source Python package for simulating the dynamics of particles undergoing condensational and collisional growth, interacting with a fluid flow and subject to chemical composition changes. It is intended to serve as a building block for process-level as well as computational-fluid-dynamics simulation systems involving representation of a continuous phase (air) and a dispersed phase (aerosol), with PySDM being responsible for representation of the dispersed phase. The PySDM package core is a Pythonic high-performance implementation of the Super-Droplet Method (SDM) Monte-Carlo algorithm for representing collisional growth, hence the name. PySDM has two alternative parallel number-crunching backends available: multi-threaded CPU backend based on Numba and GPU-resident backend built on top of ThrustRTC. The usage examples are built on top of four simple atmospheric cloud modelling frameworks: box, adiabatic parcel, single-column and 2D prescribed flow kinematic models. In addition, the package ships with tutorial code depicting how PySDM can be used from Julia and Matlab."
"ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi This paper describes the ExKaldi-RT online automatic speech recognition (ASR) toolkit that is implemented based on the Kaldi ASR toolkit and Python language. ExKaldi-RT provides tools for building online recognition pipelines. While similar tools are available built on Kaldi, a key feature of ExKaldi-RT that it works on Python, which has an easy-to-use interface that allows online ASR system developers to develop original research, such as by applying neural network-based signal processing and by decoding model trained with deep learning frameworks. We performed benchmark experiments on the minimum LibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR performance in real-time recognition."
"Py-Feat: Python Facial Expression Analysis Toolbox Studying facial expressions is a notoriously difficult endeavor. Recent advances in the field of affective computing have yielded impressive progress in automatically detecting facial expressions from pictures and videos. However, much of this work has yet to be widely disseminated in social science domains such as psychology. Current state of the art models require considerable domain expertise that is not traditionally incorporated into social science training programs. Furthermore, there is a notable absence of user-friendly and open-source software that provides a comprehensive set of tools and functions that support facial expression research. In this paper, we introduce Py-Feat, an open-source Python toolbox that provides support for detecting, preprocessing, analyzing, and visualizing facial expression data. Py-Feat makes it easy for domain experts to disseminate and benchmark computer vision models and also for end users to quickly process, analyze, and visualize face expression data. We hope this platform will facilitate increased use of facial expression data in human behavior research."
"dlmontepython: A Python library for automation and analysis of Monte Carlo molecular simulations We present an open source Python 3 library aimed at practitioners of molecular simulation, especially Monte Carlo simulation. The aims of the library are to facilitate the generation of simulation data for a wide range of problems; and to support data analysis methods which enable one to make the most of previously generated data. The library contains a framework for automating the task of measuring target physical properties (e.g. density) over a range of thermodynamic parameters (e.g. temperature) calculated using a molecular simulation program, in particular the Monte Carlo program DL_MONTE. The library also supports analysis methods including block averaging, equilibration detection and histogram reweighting. Here we describe the library and provide examples to demonstrate its key functionality: we use the library to automatically calculate isotherms to a specified precision; and to calculate the surface tension and liquid-vapour coexistence properties of methane."
"A Python compressed low-$\ell$ Planck likelihood for temperature and polarization We present Planck-low-py, a binned low-$\ell$ temperature and E-mode polarization likelihood, as an option to facilitate ease of use of the Planck 2018 large-scale data in joint-probe analysis and forecasting. It is written in Python and compresses the $\ell<30$ temperature and polarization angular power spectra information from Planck into two log-normal bins in temperature and three in polarization. These angular scales constrain the optical depth to reionization and provide a lever arm to constrain the tilt of the primordial power spectrum. We show that cosmological constraints on $\Lambda$CDM model parameters using Planck-low-py are consistent with those derived with the full Commander and SimAll likelihoods from the Planck legacy release."
"MFDFA: Efficient Multifractal Detrended Fluctuation Analysis in Python Multifractal detrended fluctuation analysis (MFDFA) has become a central method to characterise the variability and uncertainty in empiric time series. Extracting the fluctuations on different temporal scales allows quantifying the strength and correlations in the underlying stochastic properties, their scaling behaviour, as well as the level of fractality. Several extensions to the fundamental method have been developed over the years, vastly enhancing the applicability of MFDFA, e.g. empirical mode decomposition for the study of long-range correlations and persistence. In this article we introduce an efficient, easy-to-use python library for MFDFA, incorporating the most common extensions and harnessing the most of multi-threaded processing for very fast calculations."
"DarkELF: A python package for dark matter scattering in dielectric targets We present a python package to calculate interaction rates of light dark matter in dielectric materials, including screening effects. The full response of the material is parametrized in the terms of the energy loss function (ELF) of material, which darkELF converts into differential scattering rates for both direct dark matter electron scattering and through the Migdal effect. In addition, darkELF can calculate the rate to produce phonons from sub-MeV dark matter scattering via the dark photon mediator, as well as the absorption rate for dark matter comprised of dark photons. The package includes precomputed ELFs for Al, $\mathrm{Al}_2\mathrm{O}_3$, GaAs, GaN, Ge, Si, $\mathrm{SiO}_2$, and ZnS, and allows the user to easily add their own ELF extractions for arbitrary materials."
"secml-malware: Pentesting Windows Malware Classifiers with Adversarial EXEmples in Python Machine learning has been increasingly used as a first line of defense for Windows malware detection. Recent work has however shown that learning-based malware detectors can be evaded by carefully-perturbed input malware samples, referred to as adversarial EXEmples, thus demanding for tools that can ease and automate the adversarial robustness evaluation of such detectors. To this end, we present secml-malware, the first Python library for computing adversarial attacks on Windows malware detectors. \secmlmalware implements state-of-the-art white-box and black-box attacks on Windows malware classifiers, by leveraging a set of feasible manipulations that can be applied to Windows programs while preserving their functionality. The library can be used to perform the penetration testing and assessment of the adversarial robustness of Windows malware detectors, and it can be easily extended to include novel attack strategies. Our library is available at https://github.com/pralab/secml_malware."
"Speeding up Python-based Lagrangian Fluid-Flow Particle Simulations via Dynamic Collection Data Structures Array-like collection data structures are widely established in Python's scientific computing-ecosystem for high-performance computations. The structure maps well to regular, gridded lattice structures that are common to computational problems in physics and geosciences. High performance is, however, only guaranteed for static computations with a fixed computational domain. We show that for dynamic computations within an actively changing computational domain, the array-like collections provided by NumPy and its derivatives are a bottleneck for large computations. In response, we describe the integration of naturally-dynamic collection data structures (e.g. double-linked lists) into NumPy simulations and \textit{ctypes}-based C-bindings. Our benchmarks verify and quantify the performance increase attributed to the change of the collection data structure. Our application scenario, a Lagrangian (oceanic) fluid-flow particle simulation within the \textit{Parcels} framework, demonstrates the speed-up yield in a realistic setting and demonstrates the novel capabilities that are facilitated by optimised collection data structures."
"Python and Malware: Developing Stealth and Evasive Malware Without Obfuscation With the continuous rise of malicious campaigns and the exploitation of new attack vectors, it is necessary to assess the efficacy of the defensive mechanisms used to detect them. To this end, the contribution of our work is twofold. First, it introduces a new method for obfuscating malicious code to bypass all static checks of multi-engine scanners, such as VirusTotal. Interestingly, our approach to generating the malicious executables is not based on introducing a new packer but on the augmentation of the capabilities of an existing and widely used tool for packaging Python, PyInstaller but can be used for all similar packaging tools. As we prove, the problem is deeper and inherent in almost all antivirus engines and not PyInstaller specific. Second, our work exposes significant issues of well-known sandboxes that allow malware to evade their checks. As a result, we show that stealth and evasive malware can be efficiently developed, bypassing with ease state of the art malware detection tools without raising any alert."
"LaboRecommender: A crazy-easy to use Python-based recommender system for laboratory tests Laboratory tests play a major role in clinical decision making because they are essential for the confirmation of diagnostics suspicions and influence medical decisions. The number of different laboratory tests available to physicians in our age has been expanding very rapidly due to the rapid advances in laboratory technology. To find the correct desired tests within this expanding plethora of elements, the Health Information System must provide a powerful search engine and the practitioner need to remember the exact name of the laboratory test to correctly select the bag of tests to order. Recommender systems are platforms which suggest appropriate items to a user after learning the users' behaviour. A neighbourhood-based collaborative filtering method was used to model the recommender system, where similar bags, clustered using nearest neighbours algorithm, are used to make recommendations of tests for each other similar bag of laboratory tests. The recommender system developed in this paper achieved 95.54 % in the mean average precision metric. A fully documented Python package named LaboRecommender was developed to implement the algorithm proposed in this paper"
"zeus: A Python implementation of Ensemble Slice Sampling for efficient Bayesian parameter inference We introduce zeus, a well-tested Python implementation of the Ensemble Slice Sampling (ESS) method for Bayesian parameter inference. ESS is a novel Markov chain Monte Carlo (MCMC) algorithm specifically designed to tackle the computational challenges posed by modern astronomical and cosmological analyses. In particular, the method requires no hand-tuning of any hyper-parameters, its performance is insensitive to linear correlations and it can scale up to 1000s of CPUs without any extra effort. Furthermore, its locally adaptive nature allows to sample efficiently even when strong non-linear correlations are present. Lastly, the method achieves a high performance even in strongly multimodal distributions in high dimensions. Compared to emcee, a popular MCMC sampler, zeus performs 9 and 29 times better in a cosmological and an exoplanet application respectively."
"PeriPy -- A High Performance OpenCL Peridynamics Package This paper presents a lightweight, open-source and high-performance python package for solving peridynamics problems in solid mechanics. The development of this solver is motivated by the need for fast analysis tools to achieve the large number of simulations required for `outer-loop' applications, including sensitivity analysis, uncertainty quantification and optimisation. Our python software toolbox utilises the heterogeneous nature of OpenCL so that it can be executed on any platform with CPU or GPU cores. We illustrate the package use through a range of industrially motivated examples, which should enable other researchers to build on and extend the solver for use in their own applications. Step improvements in execution speed and functionality over existing techniques are presented. A comparison between this solver and an existing OpenCL implementation in the literature is presented, tested on benchmarks with hundreds of thousands to tens of millions of nodes. We demonstrate the scalability of the solver on the GeForce RTX 2080 TiGPU from NVIDIA, and the memory-bound limitations are analysed. In all test cases, the implementation is between 1.4 and 10.0 times faster than a similar existing GPU implementation in the literature. In particular, this improvement has been achieved by utilising local memory on the GPU."
"SMUTHI: A python package for the simulation of light scattering by multiple particles near or between planar interfaces SMUTHI is a python package for the efficient and accurate simulation of electromagnetic scattering by one or multiple wavelength-scale objects in a planarly layered medium. The software combines the T-matrix method for individual particle scattering with the scattering matrix formalism for the propagation of the electromagnetic field through the planar interfaces. In this article, we briefly introduce the relevant theoretical concepts and present the main features of SMUTHI. Simulation results obtained for several benchmark configurations are validated against commercial software solutions. Owing to the generality of planarly layered geometries and the availability of different particle shapes and light sources, possible applications of SMUTHI include the study of discrete random media, meta-surfaces, photonic crystals and glasses, perforated membranes and plasmonic systems, to name a few relevant examples at visible and near-visible wavelengths."
"Finding Pythons in Unexpected Places We argue that novel (highly nonclassical) quantum extremal surfaces play a crucial role in reconstructing the black hole interior even for isolated, single-sided, non-evaporating black holes (i.e. with no auxiliary reservoir). Specifically, any code subspace where interior outgoing modes can be excited will have a quantum extremal surface in its maximally mixed state. We argue that as a result, reconstruction of interior outgoing modes is always exponentially complex. Our construction provides evidence in favor of a strong Python's lunch proposal: that nonminimal quantum extremal surfaces are the exclusive source of exponential complexity in the holographic dictionary. We also comment on the relevance of these quantum extremal surfaces to the geometrization of state dependence in the typicality arguments for firewalls."
"semopy 2: A Structural Equation Modeling Package with Random Effects in Python Structural Equation Modeling (SEM) is an umbrella term that includes numerous multivariate statistical techniques that are employed throughout a plethora of research areas, ranging from social to natural sciences. Until recently, SEM software was either commercial or restricted to niche languages, and the lack of SEM packages compatible with more mainstream programming languages was dire. To combat that, we introduced a Python package semopy 1 that surpassed other state-of-the-art software in terms of performance and estimation accuracy. Yet, it was lacking in functionality and its usage was burdened with unnecessary boilerplate code. Here, we introduce a complete overhaul of semopy that improves upon the previous results and comes with lots of new capabilities. Furthermore, we propose a novel SEM model that combines in itself a notion of random effects from linear mixed models (LMMs) to model numerous phenomena, such as spatial data, time series or population stratification in genetics."
"Automatic Assessment of the Design Quality of Python Programs with Personalized Feedback The assessment of program functionality can generally be accomplished with straight-forward unit tests. However, assessing the design quality of a program is a much more difficult and nuanced problem. Design quality is an important consideration since it affects the readability and maintainability of programs. Assessing design quality and giving personalized feedback is very time consuming task for instructors and teaching assistants. This limits the scale of giving personalized feedback to small class settings. Further, design quality is nuanced and is difficult to concisely express as a set of rules. For these reasons, we propose a neural network model to both automatically assess the design of a program and provide personalized feedback to guide students on how to make corrections. The model's effectiveness is evaluated on a corpus of student programs written in Python. The model has an accuracy rate from 83.67% to 94.27%, depending on the dataset, when predicting design scores as compared to historical instructor assessment. Finally, we present a study where students tried to improve the design of their programs based on the personalized feedback produced by the model. Students who participated in the study improved their program design scores by 19.58%."
"Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI In this paper, we describe an open source Python toolkit named Uncertainty Quantification 360 (UQ360) for the uncertainty quantification of AI models. The goal of this toolkit is twofold: first, to provide a broad range of capabilities to streamline as well as foster the common practices of quantifying, evaluating, improving, and communicating uncertainty in the AI application development lifecycle; second, to encourage further exploration of UQ's connections to other pillars of trustworthy AI such as fairness and transparency through the dissemination of latest research and education materials. Beyond the Python package (\url{https://github.com/IBM/UQ360}), we have developed an interactive experience (\url{http://uq360.mybluemix.net}) and guidance materials as educational tools to aid researchers and developers in producing and communicating high-quality uncertainties in an effective manner."
"PYROBOCOP : Python-based Robotic Control & Optimization Package for Manipulation and Collision Avoidance PYROBOCOP is a lightweight Python-based package for control and optimization of robotic systems described by nonlinear Differential Algebraic Equations (DAEs). In particular, the package can handle systems with contacts that are described by complementarity constraints and provides a general framework for specifying obstacle avoidance constraints. The package performs direct transcription of the DAEs into a set of nonlinear equations by performing orthogonal collocation on finite elements. The resulting optimization problem belongs to the class of Mathematical Programs with Complementarity Constraints (MPCCs). MPCCs fail to satisfy commonly assumed constraint qualifications and require special handling of the complementarity constraints in order for NonLinear Program (NLP) solvers to solve them effectively. PYROBOCOP provides automatic reformulation of the complementarity constraints that enables NLP solvers to perform optimization of robotic systems. The package is interfaced with ADOLC for obtaining sparse derivatives by automatic differentiation and IPOPT for performing optimization. We demonstrate the effectiveness of our approach in terms of speed and flexibility. We provide several numerical examples for several robotic systems with collision avoidance as well as contact constraints represented using complementarity constraints. We provide comparisons with other open source optimization packages like CasADi and Pyomo ."
"SemSegLoss: A python package of loss functions for semantic segmentation Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In recent years, various research papers proposed different loss functions used in case of biased data, sparse segmentation, and unbalanced dataset. In this paper, we introduce SemSegLoss, a python package consisting of some of the well-known loss functions widely used for image segmentation. It is developed with the intent to help researchers in the development of novel loss functions and perform an extensive set of experiments on model architectures for various applications. The ease-of-use and flexibility of the presented package have allowed reducing the development time and increased evaluation strategies of machine learning models for semantic segmentation. Furthermore, different applications that use image segmentation can use SemSegLoss because of the generality of its functions. This wide range of applications will lead to the development and growth of AI across all industries."
"PyGAD: An Intuitive Genetic Algorithm Python Library This paper introduces PyGAD, an open-source easy-to-use Python library for building the genetic algorithm. PyGAD supports a wide range of parameters to give the user control over everything in its life cycle. This includes, but is not limited to, population, gene value range, gene data type, parent selection, crossover, and mutation. PyGAD is designed as a general-purpose optimization library that allows the user to customize the fitness function. Its usage consists of 3 main steps: build the fitness function, create an instance of the pygad.GA class, and calling the pygad.GA.run() method. The library supports training deep learning models created either with PyGAD itself or with frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in active development to respond to the user's requested features and enhancement received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD comes with documentation https://pygad.readthedocs.io for further details and examples."
"WAX-ML: A Python library for machine learning and feedback loops on streaming data Wax is what you put on a surfboard to avoid slipping. It is an essential tool to go surfing... We introduce WAX-ML a research-oriented Python library providing tools to design powerful machine learning algorithms and feedback loops working on streaming data. It strives to complement JAX with tools dedicated to time series. WAX-ML makes JAX-based programs easy to use for end-users working with pandas and xarray for data manipulation. It provides a simple mechanism for implementing feedback loops, allows the implementation of online learning and reinforcement learning algorithms with functions, and makes them easy to integrate by end-users working with the object-oriented reinforcement learning framework from the Gym library. It is released with an Apache open-source license on GitHub at https://github.com/eserie/wax-ml."
"p$^\dagger$q: A tool for prototyping many-body methods for quantum chemistry p$^\dagger$q is a C++ accelerated Python library designed to generate equations for many-body quantum chemistry methods and to realize proof-of-concept implementations of these equations for rapid prototyping. Central to this library is a simple interface to define strings of second-quantized creation and annihilation operators and to bring these strings to normal order with respect to either the true vacuum state or the Fermi vacuum. Tensor contractions over fully-contracted strings can then be evaluated using standard Python functions ({\em e.g.}, \np's einsum). Given one- and two-electron integrals these features allow for the rapid implementation and assessment of a wide array of many-body quantum chemistry methods."
"mPyPl: Python Monadic Pipeline Library for Complex Functional Data Processing In this paper, we present a new Python library called mPyPl, which is intended to simplify complex data processing tasks using functional approach. This library defines operations on lazy data streams of named dictionaries represented as generators (so-called multi-field datastreams), and allows enriching those data streams with more 'fields' in the process of data preparation and feature extraction. Thus, most data preparation tasks can be expressed in the form of neat linear 'pipeline', similar in syntax to UNIX pipes, or |> functional composition operator in F#.   We define basic operations on multi-field data streams, which resemble classical monadic operations, and show similarity of the proposed approach to monads in functional programming. We also show how the library was used in complex deep learning tasks of event detection in video, and discuss different evaluation strategies that allow for different compromises in terms of memory and performance."
"pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks Extracting opinions from texts has gathered a lot of interest in the last years, as we are experiencing an unprecedented volume of user-generated content in social networks and other places. A problem that social researchers find in using opinion mining tools is that they are usually behind commercial APIs and unavailable for other languages than English. To address these issues, we present pysentimiento, a multilingual Python toolkit for Sentiment Analysis and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish and English in a black-box fashion, allowing researchers to easily access these techniques."
"PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale - a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging."
"QuaPy: A Python-Based Framework for Quantification QuaPy is an open-source framework for performing quantification (a.k.a. supervised prevalence estimation), written in Python. Quantification is the task of training quantifiers via supervised learning, where a quantifier is a predictor that estimates the relative frequencies (a.k.a. prevalence values) of the classes of interest in a sample of unlabelled data. While quantification can be trivially performed by applying a standard classifier to each unlabelled data item and counting how many data items have been assigned to each class, it has been shown that this ""classify and count"" method is outperformed by methods specifically designed for quantification. QuaPy provides implementations of a number of baseline methods and advanced quantification methods, of routines for quantification-oriented model selection, of several broadly accepted evaluation measures, and of robust evaluation protocols routinely used in the field. QuaPy also makes available datasets commonly used for testing quantifiers, and offers visualization tools for facilitating the analysis and interpretation of the results. The software is open-source and publicly available under a BSD-3 licence via https://github.com/HLT-ISTI/QuaPy, and can be installed via pip (https://pypi.org/project/QuaPy/)"
"Fast quantum circuit simulation using hardware accelerated general purpose libraries Quantum circuit simulators have a long tradition of exploiting massive hardware parallelism. Most of the times, parallelism has been supported by special purpose libraries tailored specifically for the quantum circuits. Quantum circuit simulators are integral part of quantum software stacks, which are mostly written in Python. Our focus has been on ease of use, implementation and maintainability within the Python ecosystem. We report the performance gains we obtained by using CuPy, a general purpose library (linear algebra) developed specifically for CUDA-based GPUs, to simulate quantum circuits. For supremacy circuits the speedup is around 2x, and for quantum multipliers almost 22x compared to state-of-the-art C++-based simulators."
"Catwoman: A transit modelling Python package for asymmetric light curves When exoplanets pass in front of their stars, they imprint a transit signature on the stellar light curve which to date has been assumed to be symmetric in time, owing to the planet being modelled as a circular area occulting the stellar surface. However, this signature might be asymmetric due to different temperature/pressure and/or chemical compositions in the different terminator regions of the transiting planet. catwoman is a Python package that allows to model these asymmetric transit lightcurves, calculating lightcurves for any radially symmetric stellar limb darkening law, and where planets are modelled as two semi-circles, of different radii, using the integration algorithm developed in arXiv:1507.08285 and implemented in the batman library, from which catwoman builds upon."
"The Lightweaver Framework for NLTE Radiative Transfer in Python Tools for computing detailed optically thick spectral line profiles out of local thermodynamic equilibrium have always been focused on speed, due to the large computational effort involved. With the Lightweaver framework, we have produced a more flexible, modular toolkit for building custom tools in a high-level language, Python, without sacrificing speed against the current state of the art. The goal of providing a more flexible method for constructing these complex simulations is to decrease the barrier to entry and allow more rapid exploration of the field.   In this paper we present an overview of the theory of optically thick NLTE radiative transfer, the numerical methods implemented in Lightweaver including the problems of time-dependent populations and charge-conservation, as well as an overview of the components most users will interact with, to demonstrate their flexibility."
"Toward Interlanguage Parallel Scripting for Distributed-Memory Scientific Computing Scripting languages such as Python and R have been widely adopted as tools for the productive development of scientific software because of the power and expressiveness of the languages and available libraries. However, deploying scripted applications on large-scale parallel computer systems such as the IBM Blue Gene/Q or Cray XE6 is a challenge because of issues including operating system limitations, interoperability challenges, parallel filesystem overheads due to the small file system accesses common in scripted approaches, and other issues. We present here a new approach to these problems in which the Swift scripting system is used to integrate high-level scripts written in Python, R, and Tcl, with native code developed in C, C++, and Fortran, by linking Swift to the library interfaces to the script interpreters. In this approach, Swift handles data management, movement, and marshaling among distributed-memory processes without direct user manipulation of low-level communication libraries such as MPI. We present a technique to efficiently launch scripted applications on large-scale supercomputers using a hierarchical programming model."
"Sramm: short read alignment mapping metrics Short Read Alignment Mapping Metrics (SRAMM): is an efficient and versatile command line tool providing additional short read mapping metrics, filtering, and graphs. Short read aligners report MAPing Quality (MAPQ), but these methods generally are neither standardized nor well described in literature or software manuals. Additionally, third party mapping quality programs are typically computationally intensive or designed for specific applications. SRAMM efficiently generates multiple different concept-based mapping scores to provide for an informative post alignment examination and filtering process of aligned short reads for various downstream applications. SRAMM is compatible with Python 2.6+ and Python 3.6+ on all operating systems. It works with any short read aligner that generates SAM/BAM/CRAM file outputs and reports 'AS' tags. It is freely available under the MIT license at http://github.com/achon/sramm."
"ADAPT : Awesome Domain Adaptation Python Toolbox ADAPT is an open-source python library providing the implementation of several domain adaptation methods. The library is suited for scikit-learn estimator object (object which implement fit and predict methods) and tensorflow models. Most of the implemented methods are developed in an estimator agnostic fashion, offering various possibilities adapted to multiple usage. The library offers three modules corresponding to the three principal strategies of domain adaptation: (i) feature-based containing methods performing feature transformation; (ii) instance-based with the implementation of reweighting techniques and (iii) parameter-based proposing methods to adapt pre-trained models to novel observations. A full documentation is proposed online https://adapt-python.github.io/adapt/ with gallery of examples. Besides, the library presents an high test coverage."
"Identification of Desired Pixels in an Image Using Grover's Quantum Search Algorithm Quantum Information Theory promises to speed up computation so is observed in real quantum computers as proved to its classical counterpart. This revolutionizes every field linked directly or indirectly with computation. Grover algorithm in quantum information gives quadratic speed up in unstructured database search. With the availability of public online resources for quantum computers like IBM, quantum image processing came into the picture for making use of quantum computers in the image processing field. Our research interest is to find all darker pixels in a 2x2 grayscale image using Grover's algorithm. We studied it in two different ways. In the first method, I ran Grover's algorithm on the python generated classical image. In the second method, I converted a python-generated 2x2 image into a quantum image and then ran Grover's algorithm to locate the darker pixels. As has been observed in complexity analysis, Grover's unstructured search has the O(2^{n}) while as for classical schemes O(2^{2n+2m}), where m and n denote the dimensions of the image."
"Fast Evaluation of Finite Element Weak Forms Using Python Tensor Contraction Packages In finite element calculations, the integral forms are usually evaluated using nested loops over elements, and over quadrature points. Many such forms (e.g. linear or multi-linear) can be expressed in a compact way, without the explicit loops, using a single tensor contraction expression by employing the Einstein summation convention. To automate this process and leverage existing high performance codes, we first introduce a notation allowing trivial differentiation of multi-linear finite element forms. Based on that we propose and describe a new transpiler from Einstein summation based expressions, augmented to allow defining multi-linear finite element weak forms, to regular tensor contraction expressions. The resulting expressions are compatible with a number of Python scientific computing packages, that implement, optimize and in some cases parallelize the general tensor contractions. We assess the performance of those packages, as well as the influence of operand memory layouts and tensor contraction paths optimizations on the elapsed time and memory requirements of the finite element form evaluations. We also compare the efficiency of the transpiled weak form implementations to the C-based functions available in the finite element package SfePy."
"IDRLnet: A Physics-Informed Neural Network Library Physics Informed Neural Network (PINN) is a scientific computing framework used to solve both forward and inverse problems modeled by Partial Differential Equations (PDEs). This paper introduces IDRLnet, a Python toolbox for modeling and solving problems through PINN systematically. IDRLnet constructs the framework for a wide range of PINN algorithms and applications. It provides a structured way to incorporate geometric objects, data sources, artificial neural networks, loss metrics, and optimizers within Python. Furthermore, it provides functionality to solve noisy inverse problems, variational minimization, and integral differential equations. New PINN variants can be integrated into the framework easily. Source code, tutorials, and documentation are available at \url{https://github.com/idrl-lab/idrlnet}."
"giotto-ph: A Python Library for High-Performance Computation of Persistent Homology of Vietoris-Rips Filtrations We introduce giotto-ph, a high-performance, open-source software package for the computation of Vietoris-Rips barcodes. giotto-ph is based on Morozov and Nigmetov's lockfree (multicore) implementation of Ulrich Bauer's Ripser package. It also contains a re-working of the GUDHI library's implementation of Boissonnat and Pritam's Edge Collapser, which can be used as a pre-processing step to dramatically reduce overall run-times in certain scenarios. Our contribution is twofold: on the one hand, we integrate existing state-of-the-art ideas coherently in a single library and provide Python bindings to the C++ code. On the other hand, we increase parallelization opportunities and improve overall performance by adopting more efficient data structures. Our persistent homology backend establishes a new state of the art, surpassing even GPU-accelerated implementations such as Ripser++ when using as few as 5-10 CPU cores. Furthermore, our implementation of Edge Collapser has fewer software dependencies and improved run-times relative to GUDHI's original implementation."
"Small-text: Active Learning for Text Classification in Python We present small-text, a simple modular active learning library, which offers pool-based active learning for text classification in Python. It comes with various pre-implemented state-of-the-art query strategies, including some which can leverage the GPU. Clearly defined interfaces allow to combine a multitude of such query strategies with different classifiers, thereby facilitating a quick mix and match, and enabling a rapid development of both active learning experiments and applications. To make various classifiers accessible in a consistent way, it integrates several well-known machine learning libraries, namely, scikit-learn, PyTorch, and huggingface transformers -- for which the latter integrations are available as optionally installable extensions. The library is available under the MIT License at https://github.com/webis-de/small-text."
"Analysis of complex circadian time series data using wavelets Experiments that compare rhythmic properties across different genetic alterations and entrainment conditions underlie some of the most important breakthroughs in circadian biology. A robust estimation of the rhythmic properties of the circadian signals goes hand in hand with these discoveries. Widely applied traditional signal analysis methods such as fitting cosine functions or Fourier transformations rely on the assumption that oscillation periods do not change over time. However, novel high-resolution recording techniques have shown that, most commonly, circadian signals exhibit time-dependent changes of periods and amplitudes which cannot be captured with the traditional approaches. In this chapter we introduce a method to determine time-dependent properties of oscillatory signals, using the novel open-source Python-based Biological Oscillations Analysis Toolkit (pyBOAT). We show with examples how to detect rhythms, compute and interpret high-resolution time-dependent spectral results, analyze the main oscillatory component and to subsequently determine these main components time-dependent instantaneous period, amplitude and phase. We introduce step-by-step how such an analysis can be done by means of the easy-to-use point-and-click graphical user interface (GUI) provided by pyBOAT or executed within a Python programming environment. Concepts are explained using simulated signals as well as experimentally obtained time series."
"Mobilkit: A Python Toolkit for Urban Resilience and Disaster Risk Management Analytics using High Frequency Human Mobility Data Increasingly available high-frequency location datasets derived from smartphones provide unprecedented insight into trajectories of human mobility. These datasets can play a significant and growing role in informing preparedness and response to natural disasters. However, limited tools exist to enable rapid analytics using mobility data, and tend not to be tailored specifically for disaster risk management. We present an open-source, Python-based toolkit designed to conduct replicable and scalable post-disaster analytics using GPS location data. Privacy, system capabilities, and potential expansions of \textit{Mobilkit} are discussed."
"gammaALPs: An open-source python package for computing photon-axion-like-particle oscillations in astrophysical environments Axions and axion-like particles (ALPs) are hypothetical particles that occur in extensions of the Standard Model and are candidates for cold dark matter. They could be detected through their oscillations into photons in the presence of external electromagnetic fields. gammaALPs is an open-source python framework that computes the oscillation probability between photons and axions/ALPs. In addition to solving the photon-ALP equations of motion, gammaALPs includes models for magnetic fields in different astrophysical environments such as jets of active galactic nuclei, intra-cluster and intergalactic media, and the Milky Way. Users are also able to easily incorporate their own custom magnetic-field models. We review the basic functionality and features of gammaALPs, which is heavily based on other open-source scientific packages such as numpy and scipy. Although focused on gamma-ray energies, gammaALPs can be easily extended to arbitrary photon energies."
"VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning Digital hardware is verified by comparing its behavior against a reference model on a range of randomly generated input signals. The random generation of the inputs hopes to achieve sufficient coverage of the different parts of the design. However, such coverage is often difficult to achieve, amounting to large verification efforts and delays. An alternative is to use Reinforcement Learning (RL) to generate the inputs by learning to prioritize those inputs which can more efficiently explore the design under test. In this work, we present VeRLPy an open-source library to allow RL-driven verification with limited additional engineering overhead. This contributes to two broad movements within the EDA community of (a) moving to open-source toolchains and (b) reducing barriers for development with Python support. We also demonstrate the use of VeRLPy for a few designs and establish its value over randomly generated input signals."
"galpy: A Python Library for Galactic Dynamics I describe the design, implementation, and usage of galpy, a Python package for galactic-dynamics calculations. At its core, galpy consists of a general framework for representing galactic potentials both in Python and in C (for accelerated computations); galpy functions, objects, and methods can generally take arbitrary combinations of these as arguments. Numerical orbit integration is supported with a variety of Runge-Kutta-type and symplectic integrators. For planar orbits, integration of the phase-space volume is also possible. galpy supports the calculation of action-angle coordinates and orbital frequencies for a given phase-space point for general spherical potentials, using state-of-the-art numerical approximations for axisymmetric potentials, and making use of a recent general approximation for any static potential. A number of different distribution functions (DFs) are also included in the current release; currently these consist of two-dimensional axisymmetric and non-axisymmetric disk DFs, a three-dimensional disk DF, and a DF framework for tidal streams. I provide several examples to illustrate the use of the code. I present a simple model for the Milky Way's gravitational potential consistent with the latest observations. I also numerically calculate the Oort functions for different tracer populations of stars and compare it to a new analytical approximation. Additionally, I characterize the response of a kinematically-warm disk to an elliptical m=2 perturbation in detail. Overall, galpy consists of about 54,000 lines, including 23,000 lines of code in the module, 11,000 lines of test code, and about 20,000 lines of documentation. The test suite covers 99.6% of the code.   galpy is available at http://github.com/jobovy/galpy with extensive documentation available at http://galpy.readthedocs.org/en/latest ."
"An Introduction to Programming for Bioscientists: A Python-based Primer Computing has revolutionized the biological sciences over the past several decades, such that virtually all contemporary research in the biosciences utilizes computer programs. The computational advances have come on many fronts, spurred by fundamental developments in hardware, software, and algorithms. These advances have influenced, and even engendered, a phenomenal array of bioscience fields, including molecular evolution and bioinformatics; genome-, proteome-, transcriptome- and metabolome-wide experimental studies; structural genomics; and atomistic simulations of cellular-scale molecular assemblies as large as ribosomes and intact viruses. In short, much of post-genomic biology is increasingly becoming a form of computational biology. The ability to design and write computer programs is among the most indispensable skills that a modern researcher can cultivate. Python has become a popular programming language in the biosciences, largely because (i) its straightforward semantics and clean syntax make it a readily accessible first language; (ii) it is expressive and well-suited to object-oriented programming, as well as other modern paradigms; and (iii) the many available libraries and third-party toolkits extend the functionality of the core language into virtually every biological domain (sequence and structure analyses, phylogenomics, workflow management systems, etc.). This primer offers a basic introduction to coding, via Python, and it includes concrete examples and exercises to illustrate the language's usage and capabilities; the main text culminates with a final project in structural bioinformatics. A suite of Supplemental Chapters is also provided. Starting with basic concepts, such as that of a 'variable', the Chapters methodically advance the reader to the point of writing a graphical user interface to compute the Hamming distance between two DNA sequences."
"GammaLib and ctools: A software framework for the analysis of astronomical gamma-ray data The field of gamma-ray astronomy has seen important progress during the last decade, yet there exists so far no common software framework for the scientific analysis of gamma-ray telescope data. We propose to fill this gap by means of the GammaLib software, a generic library that we have developed to support the analysis of gamma-ray event data. GammaLib has been written in C++ and all functionality is available in Python through an extension module. On top of this framework we have developed the ctools software package, a suite of software tools that enables building of flexible workflows for the analysis of Imaging Air Cherenkov Telescope event data. The ctools are inspired by science analysis software available for existing high-energy astronomy instruments, and they follow the modular ftools model developed by the High Energy Astrophysics Science Archive Research Center. The ctools have been written in Python and C++, and can be either used from the command line, via shell scripts, or directly from Python. In this paper we present the GammaLib and ctools software versions 1.0 that have been released end of 2015. GammaLib and ctools are ready for the science analysis of Imaging Air Cherenkov Telescope event data, and also support the analysis of Fermi-LAT data and the exploitation of the COMPTEL legacy data archive. We propose to use ctools as the Science Tools software for the Cherenkov Telescope Array Observatory."
"Vizic: A Jupyter-based Interactive Visualization Tool for Astronomical Catalogs The ever-growing datasets in observational astronomy have challenged scientists in many aspects, including an efficient and interactive data exploration and visualization. Many tools have been developed to confront this challenge. However, they usually focus on displaying the actual images or focus on visualizing patterns within catalogs in a predefined way. In this paper we introduce Vizic, a Python visualization library that builds the connection between images and catalogs through an interactive map of the sky region. Vizic visualizes catalog data over a custom background canvas using the shape, size and orientation of each object in the catalog. The displayed objects in the map are highly interactive and customizable comparing to those in the images. These objects can be filtered by or colored by their properties, such as redshift and magnitude. They also can be sub-selected using a lasso-like tool for further analysis using standard Python functions from inside a Jupyter notebook. Furthermore, Vizic allows custom overlays to be appended dynamically on top of the sky map. We have initially implemented several overlays, namely, Voronoi, Delaunay, Minimum Spanning Tree and HEALPix grid layers, which are helpful for visualizing large-scale structure. All these overlays can be generated, added or removed interactively with one line of code. The catalog data is stored in a non-relational database, and the interfaces were developed in JavaScript and Python to work within Jupyter Notebook, which allows to create custom widgets, user generated scripts to analyze and plot the data selected/displayed in the interactive map. This unique design makes Vizic a very powerful and flexible interactive analysis tool. Vizic can be adopted in variety of exercises, for example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations."
"A response-matrix-centred approach to presenting cross-section measurements The current canonical approach to publishing cross-section data is to unfold the reconstructed distributions. Detector effects like efficiency and smearing are undone mathematically, yielding distributions in true event properties. This is an ill-posed problem, as even small statistical variations in the reconstructed data can lead to large changes in the unfolded spectra.   This work presents an alternative or complementary approach: the response-matrix-centred forward-folding approach. It offers a convenient way to forward-fold model expectations in truth space to reconstructed quantities. These can then be compared to the data directly, similar to what is usually done with full detector simulations within the experimental collaborations. For this, the detector response (efficiency and smearing) is parametrised as a matrix. The effects of the detector on the measurement of a given model is simulated by simply multiplying the binned truth expectation values by this response matrix.   Systematic uncertainties in the detector response are handled by providing a set of matrices according to the prior distribution of the detector properties and marginalising over them. Background events can be included in the likelihood calculation by giving background events their own bins in truth space.   To facilitate a straight-forward use of response matrices, a new software framework has been developed: the Response Matrix Utilities (ReMU). ReMU is a Python package distributed via the Python Package Index. It only uses widely available, standard scientific Python libraries and does not depend on any custom experiment-specific software. It offers all methods needed to build response matrices from Monte Carlo data sets, use the response matrix to forward-fold truth-level model predictions, and compare the predictions to real data using Bayesian or frequentist statistical inference."
"Algorithms And Programs For Strong Gravitational Lensing In Kerr Space-time Including Polarization Active galactic nuclei (AGNs) and quasars are important astrophysical objects to understand. Recently, microlensing observations have constrained the size of the quasar X-ray emission region to be of the order of 10 gravitational radii of the central supermassive black hole. For distances within a few gravitational radii, light paths are strongly bent by the strong gravity field of the central black hole. If the central black hole has nonzero angular momentum (spin), a photon's polarization plane will be rotated by the gravitational Faraday effect. The observed X-ray flux and polarization will then be influenced significantly by the strong gravity field near the source. Consequently, linear gravitational lensing theory is inadequate for such extreme circumstances. We present simple algorithms computing strong lensing effects of Kerr black holes, including effects on polarization. Our algorithms are realized in a program ""KERTAP"" in two versions: MATLAB and Python. The key ingredients of KERTAP are: a graphic user interface, a {\it backward} ray-tracing algorithm, a polarization propagator dealing with gravitational Faraday rotation, and algorithms computing observables such as flux magnification and polarization angles. Our algorithms can be easily realized in other programming languages such as FORTRAN, C, and C++. The MATLAB version of KERTAP is parallelized using the MATLAB Parallel Computing Toolbox and the Distributed Computing Server. The Python code was sped up using Cython and supports full implementation of MPI using 'mpi4py' package. As an example, we investigate the inclination angle dependence of the observed polarization and the strong lensing magnification of AGN X-ray emission. We conclude that it is possible to perform complex numerical-relativity-related computations using interpreted languages such as MATLAB and Python."
"Monte Carlo Method for Calculating Oxygen Abundances and Their Uncertainties from Strong-Line Flux Measurements We present the open-source Python code pyMCZ that determines oxygen abundance and its distribution from strong emission lines in the standard metallicity calibrators, based on the original IDL code of Kewley & Dopita (2002) with updates from Kewley & Ellison (2008), and expanded to include more recently developed calibrators. The standard strong-line diagnostics have been used to estimate the oxygen abundance in the interstellar medium through various emission line ratios in many areas of astrophysics, including galaxy evolution and supernova host galaxy studies. We introduce a Python implementation of these methods that, through Monte Carlo sampling, better characterizes the statistical oxygen abundance confidence region including the effect due to the propagation of observational uncertainties. These uncertainties are likely to dominate the error budget in the case of distant galaxies, hosts of cosmic explosions. Given line flux measurements and their uncertainties, our code produces synthetic distributions for the oxygen abundance in up to 15 metallicity calibrators simultaneously, as well as for E(B-V), and estimates their median values and their 68% confidence regions. We test our code on emission line measurements from a sample of nearby supernova host galaxies (z < 0.15) and compare our metallicity results with those from previous methods. Our metallicity estimates are consistent with previous methods but yield smaller statistical uncertainties. Systematic uncertainties are not taken into account. We offer visualization tools to assess the spread of the oxygen abundance in the different calibrators, as well as the shape of the estimated oxygen abundance distribution in each calibrator, and develop robust metrics for determining the appropriate Monte Carlo sample size. The code is open access and open source and can be found at https://github.com/nyusngroup/pyMCZ (Abridged)"
"PYRO-NN: Python Reconstruction Operators in Neural Networks Purpose: Recently, several attempts were conducted to transfer deep learning to medical image reconstruction. An increasingly number of publications follow the concept of embedding the CT reconstruction as a known operator into a neural network. However, most of the approaches presented lack an efficient CT reconstruction framework fully integrated into deep learning environments. As a result, many approaches are forced to use workarounds for mathematically unambiguously solvable problems. Methods: PYRO-NN is a generalized framework to embed known operators into the prevalent deep learning framework Tensorflow. The current status includes state-of-the-art parallel-, fan- and cone-beam projectors and back-projectors accelerated with CUDA provided as Tensorflow layers. On top, the framework provides a high level Python API to conduct FBP and iterative reconstruction experiments with data from real CT systems. Results: The framework provides all necessary algorithms and tools to design end-to-end neural network pipelines with integrated CT reconstruction algorithms. The high level Python API allows a simple use of the layers as known from Tensorflow. To demonstrate the capabilities of the layers, the framework comes with three baseline experiments showing a cone-beam short scan FDK reconstruction, a CT reconstruction filter learning setup, and a TV regularized iterative reconstruction. All algorithms and tools are referenced to a scientific publication and are compared to existing non deep learning reconstruction frameworks. The framework is available as open-source software at \url{https://github.com/csyben/PYRO-NN}. Conclusions: PYRO-NN comes with the prevalent deep learning framework Tensorflow and allows to setup end-to-end trainable neural networks in the medical image reconstruction context. We believe that the framework will be a step towards reproducible research"
"Scikit-mobility: a Python library for the analysis, generation and risk assessment of mobility data The last decade has witnessed the emergence of massive mobility data sets, such as tracks generated by GPS devices, call detail records, and geo-tagged posts from social media platforms. These data sets have fostered a vast scientific production on various applications of mobility analysis, ranging from computational epidemiology to urban planning and transportation engineering. A strand of literature addresses data cleaning issues related to raw spatiotemporal trajectories, while the second line of research focuses on discovering the statistical ""laws"" that govern human movements. A significant effort has also been put on designing algorithms to generate synthetic trajectories able to reproduce, realistically, the laws of human mobility. Last but not least, a line of research addresses the crucial problem of privacy, proposing techniques to perform the re-identification of individuals in a database. A view on state of the art cannot avoid noticing that there is no statistical software that can support scientists and practitioners with all the aspects mentioned above of mobility data analysis. In this paper, we propose scikit-mobility, a Python library that has the ambition of providing an environment to reproduce existing research, analyze mobility data, and simulate human mobility habits. scikit-mobility is efficient and easy to use as it extends pandas, a popular Python library for data analysis. Moreover, scikit-mobility provides the user with many functionalities, from visualizing trajectories to generating synthetic data, from analyzing statistical patterns to assessing the privacy risk related to the analysis of mobility data sets."
"SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments."
"The young massive star cluster Westerlund 2 observed with MUSE. II. MUSEpack -- a Python package to analyze the kinematics of young star clusters We mapped the Galactic young massive star cluster Westerlund 2 (Wd2) with the integral field spectrograph MUSE (spatial resolution: 0.2arcsec/px, spectral resolution: $\Delta \lambda$ = 1.25A, wavelength range 4600-9350A) mounted on the VLT, as part of an on-going study to measure the stellar and gas kinematics of the cluster region. In this paper we present the fully reduced dataset and introduce our new Python package ""MUSEpack"", which we developed to measure stellar radial velocities with an absolute precision of 1-2km/s without the necessity of a spectral template library. This novel method uses the two-dimensional spectra and an atomic transition line library to create templates around strong absorption lines for each individual star. The code runs fully automatically on multi-core machines, which makes it possible to efficiently determine stellar radial velocities of a large number of stars with the necessary precision to measure the velocity dispersion of young star clusters. MUSEpack also provides an enhanced method for removing telluric lines in crowded fields without sky exposures and a Python wrapper for ESO's data reduction pipeline. We observed Wd2 with a total of 11 short and 5 long exposures to cover the bright nebular emission and OB stars, as well as the fainter pre-main sequence stars down to ~1M$_\odot$. The survey covers an area of ~11arcmin$^2$ (15.8pc$^2$). In total, we extracted 1,725 stellar spectra with a mean S/N>5 per pixel. A typical radial velocity (RV) uncertainty of 4.78km/s, 2.92km/s, and 1.1km/s is reached for stars with a mean S/N>10, S/N>20, S/N>50 per pixel, respectively. Depending on the number of spectral lines used to measure the RVs, it is possible to reach RV accuracies of 0.9km/s, 1.3km/s, and 2.2km/s with $\geq5$, 3-4, and 1-2 spectral lines, respectively. The combined statistical uncertainty on the radial velocity measurements is 1.10km/s."
"A novel fusion Python application of data mining techniques to evaluate airborne magnetic datasets A novel fusion python application of data mining techniques (DMT) was designed and implemented to locate, identify, and delineate the subsurface structural pattern (SSP) of source rocks for the features of interest underlain the study area. The techniques of machine learning tools (MLT) helped to define magnetic anomaly source (MAS) rock and the various depths of these subsurface source rock features. The principal objective is to use straightforward DMT to locate magnetic anomaly features of interest that host mineralization. The required geo-referenced radiometric data, which facilitated the delineation of SSP, were sufficiently covered by combining the application of the Oasis Montaj\c{opyright} 2014 source parameter imaging functions. Relevance basic filtering techniques of data reduction were used to improve the signal-to-noise (S/N) ratio and hence automatically determine depths to the various engrossed features from gridded geo-referenced airborne magnetic datasets before the DMT application was performed. Geological source rock models (GSRM) (i.e., rock contacts, dykes) served as the delineated features based on their structural index (SI) values. The anomalies were perpendicularly oriented, with few inconsequential nonvertical features, and all were generally aligned in NNE-SSW and NE-SW directions. The DMT approach showed that magnetic anomaly patterns (MAP) control the SSP and the ground surface stratigraphy (GSS) on a geological time-scale (GTS) by fusing the subsurface gravitational structural features (SGSF) in the area. The DMT facilitated the determination of depths to these subsurface geological source rock features with a maximum depth of approximately 1.277 km using a 3x3 window size to map the concealed features of interest."
"pymia: A Python package for data handling and evaluation in deep learning-based medical image analysis Background and Objective: Deep learning enables tremendous progress in medical image analysis. One driving force of this progress are open-source frameworks like TensorFlow and PyTorch. However, these frameworks rarely address issues specific to the domain of medical image analysis, such as 3-D data handling and distance metrics for evaluation. pymia, an open-source Python package, tries to address these issues by providing flexible data handling and evaluation independent of the deep learning framework.   Methods: The pymia package provides data handling and evaluation functionalities. The data handling allows flexible medical image handling in every commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise). Even data beyond images like demographics or clinical reports can easily be integrated into deep learning pipelines. The evaluation allows stand-alone result calculation and reporting, as well as performance monitoring during training using a vast amount of domain-specific metrics for segmentation, reconstruction, and regression.   Results: The pymia package is highly flexible, allows for fast prototyping, and reduces the burden of implementing data handling routines and evaluation methods. While data handling and evaluation are independent of the deep learning framework used, they can easily be integrated into TensorFlow and PyTorch pipelines. The developed package was successfully used in a variety of research projects for segmentation, reconstruction, and regression.   Conclusions: The pymia package fills the gap of current deep learning frameworks regarding data handling and evaluation in medical image analysis. It is available at https://github.com/rundherum/pymia and can directly be installed from the Python Package Index using pip install pymia."
"Characterization and Automatic Update of Deprecated Machine-Learning API Usages Due to the rise of AI applications, machine learning libraries have become far more accessible, with Python being the most common programming language to write them. Machine learning libraries tend to be updated periodically, which may deprecate existing APIs, making it necessary for developers to update their usages. However, updating usages of deprecated APIs are typically not a priority for developers, leading to widespread usages of deprecated APIs which expose library users to vulnerability issues. In this paper, we built a tool to automate these updates. We first conducted an empirical study to seek a better understanding on how updates of deprecated machine-learning API usages in Python can be done. The study involved a dataset of 112 deprecated APIs from Scikit-Learn, TensorFlow, and PyTorch. We found dimensions of deprecated API migration related to its update operation (i.e., the required operation to perform the migration), API mapping (i.e., the number of deprecated and its corresponding updated APIs),and context dependency (i.e., whether we need to consider surrounding contexts when performing the migration). Guided by the findings on our empirical study, we created MLCatchUp, a tool to automate the update of Python deprecated API usage that automatically infers the API migration transformation through comparison of the deprecated and updated API signatures. These transformations are expressed in a Domain Specific Language (DSL). We evaluated MLCatchUp using test dataset containing 258 files with 514 API usages that we collected from public GitHub repositories. In this evaluation, MLCatchUp achieves a precision of 86.19%. We further improve the precision of MLCatchUp by adding a feature that allows it to accept additional user input to specify the transformation constraints in the DSL for context-dependent API migration, where MLCatchUp achieves a precision of 93.58%."
"PINT: A Modern Software Package for Pulsar Timing Over the past few decades, the measurement precision of some pulsar-timing experiments has advanced from ~10 us to ~10 ns, revealing many subtle phenomena. Such high precision demands both careful data handling and sophisticated timing models to avoid systematic error. To achieve these goals, we present PINT (PINT Is Not Tempo3), a high-precision Python pulsar timing data analysis package, which is hosted on GitHub and available on Python Package Index (PyPI) as pint-pulsar. PINT is well-tested, validated, object-oriented, and modular, enabling interactive data analysis and providing an extensible and flexible development platform for timing applications. It utilizes well-debugged public Python packages (e.g., the NumPy and Astropy libraries) and modern software development schemes (e.g., version control and efficient development with git and GitHub) and a continually expanding test suite for improved reliability, accuracy, and reproducibility. PINT is developed and implemented without referring to, copying, or transcribing the code from other traditional pulsar timing software packages (e.g., TEMPO and TEMPO2) and therefore provides a robust tool for cross-checking timing analyses and simulating pulse arrival times. In this paper, we describe the design, usage, and validation of PINT, and we compare timing results between it and TEMPO and TEMPO2."
"Modernising the ESRF control system with GNU/Linux he ESRF control system is in the process of being modernised. The present contrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC, Motif and C. The new control system will be based on compact PCI, 100 MHz Ethernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main frontend operating system will be GNU/Linux running on Intel/x86 and Motorola/68k. Linux will also be used on handheld devices for mobile control. This poster describes how GNU/Linux is being used to modernise the control system and what problems have been encountered so far"
"Creating Annotation Tools with the Annotation Graph Toolkit The Annotation Graph Toolkit is a collection of software supporting the development of annotation tools based on the annotation graph model. The toolkit includes application programming interfaces for manipulating annotation graph data and for importing data from other formats. There are interfaces for the scripting languages Tcl and Python, a database interface, specialized graphical user interfaces for a variety of annotation tasks, and several sample applications. This paper describes all the toolkit components for the benefit of would-be application developers."
"Clarens Client and Server Applications Several applications have been implemented with access via the Clarens web service infrastructure, including virtual organization management, JetMET physics data analysis using relational databases, and Storage Resource Broker (SRB) access. This functionality is accessible transparently from Python scripts, the Root analysis framework and from Java applications and browser applets."
Remote-control and clustering of physical computations using the XML-RPC protocol and the open-Mosix system The applications of the remote control of physical simulations performed in clustered computers running under an open-Mosix system are presented. Results from the simulation of a 2-dimensional ferromagnetic system of spins in the Ising scheme are provided. Basic parameters of a simulated hysteresis loop like coercivity and exchange bias due to pinning of ferromagnetic spins are given. The paper describes in physicists terminology a cost effective solution which utilizes an XML-RPC protocol (Extensible Markup Language - Remote Procedure Calling) and standard C++ and Python languages.
"Firebird Database Backup by Serialized Database Table Dump This paper presents a simple data dump and load utility for Firebird databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load, for dumping and loading respectively, retrieves each database table using kinterbasdb and serializes the data using marshal module. This utility has two advantages over the standard Firebird database backup utility, gbak. Firstly, it is able to backup and restore single database tables which might help to recover corrupted databases. Secondly, the output is in text-coded format (from marshal module) making it more resilient than a compressed text backup, as in the case of using gbak."
"Automatically generating Feynman rules for improved lattice field theories Deriving the Feynman rules for lattice perturbation theory from actions and operators is complicated, especially when improvement terms are present. This physically important task is, however, suitable for automation. We describe a flexible algorithm for generating Feynman rules for a wide range of lattice field theories including gluons, relativistic fermions and heavy quarks. We also present an efficient implementation of this in a freely available, multi-platform programming language (\python), optimised to deal with a wide class of lattice field theories."
"Cosmic Microwave Background: Past, Future, and Present I explain the origin and evolution of anisotropies in the Cosmic Microwave Background (CMB) and argue that upcoming experiments will measure cosmological and fundamental parameters very accurately. Most of the paper focuses on present data, which strongly suggest that the universe is flat. Several arguments are given to prove that present data sets are not contaminated by systematics. New techniques to compare different experiments visually are introduced. These are illustrated for two years of the MSAM and Python experiments."
"Freeware solutions for spectropolarimetric data reduction Most of the solar physicists use very expensive software for data reduction and visualization. We present hereafter a reliable freeware solution based on the Python language. This is made possible by the association of the latter with a small set of additional libraries developed in the scientific community. It provides then a very powerful and economical alternative to other interactive data languages. Although it can also be used for any kind of post-processing of data, we demonstrate the capabities of such a set of freeware tools using THeMIS observations of the second solar spectrum."
"Plat_Forms -- a contest: The web development platform comparison ""Plat_Forms"" is a competition in which top-class teams of three programmers compete to implement the same requirements for a web-based system within 30 hours, each team using a different technology platform (Java EE, .NET, PHP, Perl, Python, or Ruby on Rails). The results will provide new insights into the real (rather than purported) pros, cons, and emergent properties of each platform. The evaluation will analyze many aspects of each solution, both external (usability, functionality, reliability, performance, etc.) and internal (structure, understandability, flexibility, etc.)."
"Limits, discovery and cut optimization for a Poisson process with uncertainty in background and signal efficiency: TRolke 2.0 A C++ class was written for the calculation of frequentist confidence intervals using the profile likelihood method. Seven combinations of Binomial, Gaussian, Poissonian and Binomial uncertainties are implemented. The package provides routines for the calculation of upper and lower limits, sensitivity and related properties. It also supports hypothesis tests which take uncertainties into account. It can be used in compiled C++ code, in Python or interactively via the ROOT analysis framework."
"Ezhil: A Tamil Programming Language Ezhil is a Tamil language based interpreted procedural programming language. Tamil keywords and grammar are chosen to make the native Tamil speaker write programs in the Ezhil system. Ezhil allows easy representation of computer program closer to the Tamil language logical constructs equivalent to the conditional, branch and loop statements in modern English based programming languages. Ezhil is a compact programming language aimed towards Tamil speaking novice computer users. Grammar for Ezhil and a few example programs are reported here, from the initial proof-of-concept implementation using the Python programming language1. To the best of our knowledge, Ezhil language is the first freely available Tamil programming language."
"Differently knotted symplectic surfaces in D^4 bounded by the same transverse knot In this paper we show that there are two symplectic surfaces in the 4-ball which bound the same transverse knot, have the same topology (as abstract surfaces), and are distinguished by the fundamental groups of their complements."
"The NumPy array: a structure for efficient numerical computation In the Python world, NumPy arrays are the standard representation for numerical data. Here, we show how these arrays enable efficient implementation of numerical computations in a high-level language. Overall, three techniques are applied to improve performance: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. We first present the NumPy array structure, then show how to use it for efficient computation, and finally how to share array data with other libraries."
"Reflex: Scientific Workflows for the ESO Pipelines The recently released Reflex scientific workflow environment supports the interactive execution of ESO VLT data reduction pipelines. Reflex is based upon the Kepler workflow engine, and provides components for organising the data, executing pipeline recipes based on the ESO Common Pipeline Library, invoking Python scripts, and constructing interaction loops. Reflex will greatly enhance the quick validation and reduction of the scientific data. In this paper we summarize the main features of Reflex, and demonstrate as an example its application to the reduction of echelle UVES data."
"Correcting pervasive errors in RNA crystallography through enumerative structure prediction Three-dimensional RNA models fitted into crystallographic density maps exhibit pervasive conformational ambiguities, geometric errors and steric clashes. To address these problems, we present enumerative real-space refinement assisted by electron density under Rosetta (ERRASER), coupled to Python-based hierarchical environment for integrated 'xtallography' (PHENIX) diffraction-based refinement. On 24 data sets, ERRASER automatically corrects the majority of MolProbity-assessed errors, improves the average Rfree factor, resolves functionally important discrepancies in noncanonical structure and refines low-resolution models to better match higher-resolution models."
"Trajectories of charged particles trapped in Earth's magnetic field I outline the theory of relativistic charged-particle motion in the magnetosphere in a way suitable for undergraduate courses. I discuss particle and guiding center motion, derive the three adiabatic invariants associated with them, and present particle trajectories in a dipolar field. I provide twelve computational exercises that can be used as classroom assignments or for self-study. Two of the exercises, drift-shell bifurcation and Speiser orbits, are adapted from active magnetospheric research. The Python code provided in the supplement can be used to replicate the trajectories and can be easily extended for different field geometries."
"Improving non-linear fits In this notes we describe an algorithm for non-linear fitting which incorporates some of the features of linear least squares into a general minimum $\chi^2$ fit and provide a pure Python implementation of the algorithm. It consists of the variable projection method (varpro), combined with a Newton optimizer and stabilized using the steepest descent with an adaptative step. The algorithm includes a term to account for Bayesian priors. We performed tests of the algorithm using simulated data. This method is suitable, for example, for fitting with sums of exponentials as often needed in Lattice Quantum Chromodynamics."
"QCDUtils This manual describes a set of utilities developed for Lattice QCD computations. They are collectively called QCDUtils. They comprise a set of Python programs each of them with a specific function: download gauge ensembles from the public NERSC repository, convert between formats, split files by time-slices, compile and run physics algorithms, generate visualizations in the form of VTK files, convert the visualizations into images, perform bootstrap analysis of results, fit the results of the analysis, and plot those results. These tools implement the typical workflow of most Lattice QCD computations and automate it by enforcing filename conventions: the output of one tool is read by the next tool in the workflow. This manual is organized as a series of autonomous recipes which can be combined together."
"Lucretia - a type system for objects in languages with reflection Object-oriented scripting languages such as JavaScript or Python gain in popularity due to their flexibility. Still, the growing code bases written in the languages call for methods that make possible to automatically control the properties of the programs that ensure their stability in the running time. We propose a type system, called Lucretia, that makes possible to control the object structure of languages with reflection. Subject reduction and soundness of the type system with respect to the semantics of the language is proved."
"What Makes Code Hard to Understand? What factors impact the comprehensibility of code? Previous research suggests that expectation-congruent programs should take less time to understand and be less prone to errors. We present an experiment in which participants with programming experience predict the exact output of ten small Python programs. We use subtle differences between program versions to demonstrate that seemingly insignificant notational changes can have profound effects on correctness and response times. Our results show that experience increases performance in most cases, but may hurt performance significantly when underlying assumptions about related code statements are violated."
"Invitation to Ezhil: A Tamil Programming Language for Early Computer-Science Education Ezhil is a Tamil programming language with support for imperative programming, with mixed use of Tamil and English identifiers and function-names. Ezhil programing system is targeted toward the K-12 (junior high-school) level Tamil speaking students, as an early introduction to thinking like a computer-scientist. We believe this 'numeracy' knowledge is easily transferred over from a native language (Tamil) to the pervasive English language programming systems, in Java, dot-Net, Ruby or Python. Ezhil is an effort to improve access to computing in the 21st Century."
"Automatic code generator for higher order integrators Some explicit algorithms for higher order symplectic integration of a large class of Hamilton's equations have recently been discussed by Mushtaq \emph{et. al}. Here we present a Python program for automatic numerical implementation of these algorithms for a given Hamiltonian, both for double precision and multiprecision computations. We provide examples of how to use this program, and illustrate behaviour of both the code generator and the generated solver module(s)."
Regression techniques for Portfolio Optimisation using MOSEK Regression is widely used by practioners across many disciplines. We reformulate the underlying optimisation problem as a second-order conic program providing the flexibility often needed in applications. Using examples from portfolio management and quantitative trading we solve regression problems with and without constraints. Several Python code fragments are given. The code and data are available online at http://www.github.com/tschm/MosekRegression.
"A General, Fast, and Robust Implementation of the Time-Optimal Path Parameterization Algorithm Finding the Time-Optimal Parameterization of a given Path (TOPP) subject to kinodynamic constraints is an essential component in many robotic theories and applications. The objective of this article is to provide a general, fast and robust implementation of this component. For this, we give a complete solution to the issue of dynamic singularities, which are the main cause of failure in existing implementations. We then present an open-source implementation of the algorithm in C++/Python and demonstrate its robustness and speed in various robotics settings."
"A low-cost mirror mount control system for optics setups We describe a flexible, simple to build, low-cost, and computer-controlled optical mirror actuator system we developed for undergraduate research laboratories. Geared motors for hobby robotics are controlled by an Arduino microcontroller board in combination with an H bridge to finely position the mirror mount actuators. We present a graphical user interface based on the free Python script language. The price of the fully controlled actuator system is thus only a small fraction of the price of any commercial system; however, it is quickly implementable due to the use of open hardware electronics. We show the performance of the system and give an outlook for future expansions and use in advanced optical setups."
"Easy Hyperparameter Search Using Optunity Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net."
"Theano-based Large-Scale Visual Recognition with Multiple GPUs In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date."
"Towards Interactive, Incremental Programming of ROS Nodes Writing software for controlling robots is a complex task, usually demanding command of many programming languages and requiring significant experimentation. We believe that a bottom-up development process that complements traditional component- and MDSD-based approaches can facilitate experimentation. We propose the use of an internal DSL providing both a tool to interactively create ROS nodes and a behaviour-replacement mechanism to interactively reshape existing ROS nodes by wrapping the external interfaces (the publish/subscribe topics), dynamically controlled using the Python command line interface."
"Tensor calculus with open-source software: the SageManifolds project The SageManifolds project aims at extending the mathematics software system Sage towards differential geometry and tensor calculus. Like Sage, SageManifolds is free, open-source and is based on the Python programming language. We discuss here some details of the implementation, which relies on Sage's parent/element framework, and present a concrete example of use."
"Testing MCMC code Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. We outline several strategies for testing the correctness of MCMC algorithms. Specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. We discuss strategies for both unit testing and integration testing. As a running example, we show how a Python implementation of Gibbs sampling for a mixture of Gaussians model can be tested."
"Replacing ANSI C with other modern programming languages Replacing ANSI C language with other modern programming languages such as Python or Java may be an actual debate topic in technical universities. Researchers whose primary interests are not in programming area seem to prefer modern and higher level languages. Keeping standard language ANSI C as a primary tool for engineers and for microcontrollers programming, robotics and data acquisition courses is another strong different opinion trend. Function oriented versus object oriented languages may be another highlighted topic in actual debates."
"The power conjugacy problem in Higman-Thompson groups An introduction to the universal algebra approach to Higman-Thompson groups (including Thompson's group $V$) is given, following a series of lectures by Graham Higman in 1973. In these talks, Higman outlined an algorithm for the conjugacy problem; which although essentially correct fails in certain cases, as we show here. A revised and complete version of the algorithm is written out explicitly. From this, we construct an algorithm for the power conjugacy problem in these groups. Python implementations of these algorithms can be found at [26]."
"Morphological Analyzer and Generator for Russian and Ukrainian Languages pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian languages. It uses large efficiently encoded lexi- cons built from OpenCorpora and LanguageTool data. A set of linguistically motivated rules is developed to enable morphological analysis and generation of out-of-vocabulary words observed in real-world documents. For Russian pymorphy2 provides state-of-the-arts morphological analysis quality. The analyzer is implemented in Python programming language with optional C++ extensions. Emphasis is put on ease of use, documentation and extensibility. The package is distributed under a permissive open-source license, encouraging its use in both academic and commercial setting."
"Comparative Studies of Six Programming Languages Comparison of programming languages is a common topic of discussion among software engineers. Multiple programming languages are designed, specified, and implemented every year in order to keep up with the changing programming paradigms, hardware evolution, etc. In this paper we present a comparative study between six programming languages: C++, PHP, C#, Java, Python, VB ; These languages are compared under the characteristics of reusability, reliability, portability, availability of compilers and tools, readability, efficiency, familiarity and expressiveness."
"Application of a single-board computer as a low cost pulse generator A BeagleBone Black (BBB) single-board open-source computer was implemented as a low-cost fully programmable pulse generator. The pulse generator makes use of the BBB Programmable Real-Time Unit (PRU) subsystem to achieve a deterministic temporal resolution of 5 ns, an RMS jitter of 290 ps and a timebase stability on the order of 10 ppm. A python based software framework has also been developed to simplify the usage of the pulse generator."
"Fast R-CNN This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
"FATS: Feature Analysis for Time Series In this paper, we present the FATS (Feature Analysis for Time Series) library. FATS is a Python library which facilitates and standardizes feature extraction for time series data. In particular, we focus on one application: feature extraction for astronomical light curve data, although the library is generalizable for other uses. We detail the methods and features implemented for light curve analysis, and present examples for its usage."
"Control Infrastructure for a Pulsed Ion Accelerator We report on updates to the accelerator controls for the Neutralized Drift Compression Experiment II, a pulsed induction-type accelerator for heavy ions. The control infrastructure is built around a LabVIEW interface combined with an Apache Cassandra backend for data archiving. Recent upgrades added the storing and retrieving of device settings into the database, as well as ZeroMQ as a message broker that replaces LabVIEW's shared variables. Converting to ZeroMQ also allows easy access via other programming languages, such as Python."
"Comment on 'The role of 3-D interactive visualization in blind surveys of HI in galaxies' Punzo et al. (2015) recently reported on the state of the art for visualisation of H I data cubes. I here briefly describe another program, FRELLED, specifically designed for dealing with H I data. Unlike many 3D viewers, FRELLED can handle astronomical world coordinates, easily and interactively mask and label specific volumes within the data, overlay optical data from the SDSS, generate contour plots and renzograms, make basic spectral profile measurements via an interface with MIRIAD, and can switch between viewing the data in 3D and 2D. The code is open source and can potentially be extended to include any astronomical function possible with Python, displaying the result in an interactive 3D environment."
Speech Controlled Quadruped The project which we have performed is based on voice recognition and we desire to create a four legged robot that can acknowledge the given instructions which are given through vocal commands and perform the tasks. The main processing unit of the robot will be Arduino Uno. We are using 8 servos for the movement of its legs while two servos will be required for each leg. The interface between a human and the robot is generated through Python programming and Eclipse software and it is implemented by using Bluetooth module HC 06.
"Extension complexity and realization spaces of hypersimplices The (n,k)-hypersimplex is the convex hull of all 0/1-vectors of length n with coordinate sum k. We explicitly determine the extension complexity of all hypersimplices as well as of certain classes of combinatorial hypersimplices. To that end, we investigate the projective realization spaces of hypersimplices and their (refined) rectangle covering numbers. Our proofs combine ideas from geometry and combinatorics and are partly computer assisted."
"Radioactive heat production of six geologically important nuclides Heat production rates for the geologically important nuclides ${}^{26}$Al, ${}^{40}$K, ${}^{60}$Fe, ${}^{232}$Th, ${}^{235}$U, and ${}^{238}$U are calculated on the basis of recent data on atomic and nuclear properties. The revised data differ by several per cent from some older values, but indicate that more recent analyses converge toward values with an accuracy sufficient for all common geoscience applications, although some possibilities for improvement still remain, especially in the case of ${}^{40}$K and with regard to the determination of half-lives. A Python script is provided for calculating heat production (https://github.com/trg818/radheat)."
"Web interface for reflectivity fitting The Liquids Reflectometer at Oak Ridge National Laboratory provides neutron reflectivity capability for an average of about 30 experiments each year. In recent years, there has been a large effort to streamline the data processing and analysis for the instrument. While much of the data reduction can be automated, data analysis remains something that needs to be done by scientists. For this purpose, we present a reflectivity fitting web interface that captures the process of setting up and executing fits while reducing the need for installing software or writing Python scripts."
"Algorithmic Programming Language Identification Motivated by the amount of code that goes unidentified on the web, we introduce a practical method for algorithmically identifying the programming language of source code. Our work is based on supervised learning and intelligent statistical features. We also explored, but abandoned, a grammatical approach. In testing, our implementation greatly outperforms that of an existing tool that relies on a Bayesian classifier. Code is written in Python and available under an MIT license."
"Simulation and Visualization of Chaos in a Driven Nonlinear Pendulum: An Aid to Introducing Chaotic Systems in Physics The presence of physical systems whose characteristics change in a seemingly erratic manner gives rise to the study of chaotic systems. The characteristics of these systems are due to their hypersensitivity to changes in initial conditions. In order to understand chaotic systems, some sort of simulation and visualization is pertinent. Consequently, in this work, we have simulated and graphically visualized chaos in a driven nonlinear pendulum as a means of introducing chaotic systems. The visualized results obtained which highlight the hypersensitivity of the pendulum to initial conditions can be used to effectively introduce the physics of chaotic system. The simulation and visualization programme is written in Python codes."
"Nonstandard techniques and nowhere differentiable functions I: A dense family of generalized blancmange functions We will give an elementary nonstandard proof that the family of generalized blancmange functions are nowhere differentiable. The proof follows from the intuitive characterization of differentiability at a point as almost $\delta$ affine along with the transfer of the functional equations these functions satisfy. We also give elementary nonstandard proofs of the uniform density of these functions among continuous functions. Finally, we discuss work done with the Python programming language in displaying these functions."
"Automatic synthesis of quantum circuits for point addition on ordinary binary elliptic curves Implementing the group arithmetic is a cost-critical task when designing quantum circuits for Shor's algorithm to solve the discrete logarithm problem. We introduce a tool for the automatic generation of addition circuits for ordinary binary elliptic curves, a prominent platform group for digital signatures. Our Python software generates circuit descriptions that, without increasing the number of qubits or T-depth, involve less than 39% of the number of T-gates in the best previous construction. The software also optimizes the (CNOT) depth for GF(2)-linear operations by means of suitable graph colorings."
"Combinatorial level densities by the real-time method Levels densities of independent-particle Hamiltonians can be calculated easily by using the real-time representation of the evolution operator together with the fast Fourier transform. We describe the method and implement it with a set of Python programs. Examples are provided for the total and partial levels densities of a heavy deformed nucleus (Dy-164). The partial level densities that may be calculated are the projected ones on neutron number, proton number, azimuthal angular momentum, and parity."
"A Graph Framework for Multimodal Medical Information Processing Multimodal medical information processing is currently the epicenter of intense interdisciplinary research, as proper data fusion may lead to more accurate diagnoses. Moreover, multimodality may disambiguate cases of co-morbidity. This paper presents a framework for retrieving, analyzing, and storing medical information as a multilayer graph, an abstract format suitable for data fusion and further processing. At the same time, this paper addresses the need for reliable medical information through co-author graph ranking. A use case pertaining to frailty based on Python and Neo4j serves as an illustration of the proposed framework."
"A New Architecture for Optimization Modeling Frameworks We propose a new architecture for optimization modeling frameworks in which solvers are expressed as computation graphs in a framework like TensorFlow rather than as standalone programs built on a low-level linear algebra interface. Our new architecture makes it easy for modeling frameworks to support high performance computational platforms like GPUs and distributed clusters, as well as to generate solvers specialized to individual problems. Our approach is particularly well adapted to first-order and indirect optimization algorithms. We introduce cvxflow, an open-source convex optimization modeling framework in Python based on the ideas in this paper, and show that it outperforms the state of the art."
"A storm is Coming: A Modern Probabilistic Model Checker We launch the new probabilistic model checker storm. It features the analysis of discrete- and continuous-time variants of both Markov chains and MDPs. It supports the PRISM and JANI modeling languages, probabilistic programs, dynamic fault trees and generalized stochastic Petri nets. It has a modular set-up in which solvers and symbolic engines can easily be exchanged. It offers a Python API for rapid prototyping by encapsulating storm's fast and scalable algorithms. Experiments on a variety of benchmarks show its competitive performance."
Student's opinions about System for automatic assessment of programming tasks Projekt Tomo In a previous paper a web service called Projekt Tomo intended to ease the process of learning programming for teachers and students has been described. Since the service received a very warm welcome from teachers and students alike we decided to collect additional information on the students' view of the service in order to improve it even further.   In the paper we briefly present our web service and a detailed analysis of the questionnaire handed out to the students of the highschool level programming course in Python.
"Numerical implementation of the multicomponent potential theory of adsorption in Python using the NIST Refprop database In this paper, we present a detailed numerical implementation of the multicomponent potential theory of adsorption which is among the most accurate gas mixtures adsorption models. The implementation uses the NIST Refprop database to describe fluid properties and applies to pure gases and mixtures in both subcritical and supercritical regimes. The limitations of the model and the issues encountered with its implementation are discussed. The adsorption isotherms of CH4 / CO2 mixture are modeled and parameterized as implementation examples."
"liquidSVM: A Fast and Versatile SVM package liquidSVM is a package written in C++ that provides SVM-type solvers for various classification and regression tasks. Because of a fully integrated hyper-parameter selection, very carefully implemented solvers, multi-threading and GPU support, and several built-in data decomposition strategies it provides unprecedented speed for small training sizes as well as for data sets of tens of millions of samples. Besides the C++ API and a command line interface, bindings to R, MATLAB, Java, Python, and Spark are available. We present a brief description of the package and report experimental comparisons to other SVM packages."
"Conan: a platform for complex network analysis Conan is a C++ library created for the accurate and efficient modelling, inference and analysis of complex networks. It implements the generation and modification of graphs according to several published models, as well as the unexpensive computation of global and local network properties. Other features include network inference and community detection. Furthermore, Conan provides a Python interface to facilitate the use of the library and its integration in currently existing applications.   Conan is available at http://github.com/rhz/conan/."
"Clustering Words by Projection Entropy We apply entropy agglomeration (EA), a recently introduced algorithm, to cluster the words of a literary text. EA is a greedy agglomerative procedure that minimizes projection entropy (PE), a function that can quantify the segmentedness of an element set. To apply it, the text is reduced to a feature allocation, a combinatorial object to represent the word occurences in the text's paragraphs. The experiment results demonstrate that EA, despite its reduction and simplicity, is useful in capturing significant relationships among the words in the text. This procedure was implemented in Python and published as a free software: REBUS."
"Automated lattice data generation The process of generating ensembles of gauge configurations (and measuring various observables over them) can be tedious and error-prone when done ""by hand"". In practice, most of this procedure can be automated with the use of a workflow manager. We discuss how this automation can be accomplished using Taxi, a minimal Python-based workflow manager built for generating lattice data. We present a case study demonstrating this technology."
"A fast C++ implementation of thermal functions We provide a small C++ library with Mathematica and Python interfaces for computing thermal functions, defined $$ J_\text{B/F}(y^2) \equiv \Re \int_0^\infty x^2 \log\left[1 \mp e^{-\sqrt{x^2 + y^2}} \right] \,\text{d}x, $$ which appear in finite-temperature quantum field theory and play a role in phase-transitions in the early Universe, including baryogenesis, electroweak symmetry breaking and the Higgs mechanism."
"CalcuList: a Functional Language Extended with Imperative Features CalcuList (Calculator with List manipulation), is an educational language for teaching functional programming extended with some imperative and side-effect features, which are enabled under explicit request by the programmer. In addition to strings and lists, the language natively supports json objects. The language adopts a Python-like syntax and enables interactive computation sessions with the user through a REPL (Read-Evaluate-Print-Loop) shell. The object code produced by a compilation is a program that will be eventually executed by the CalcuList Virtual Machine (CLVM)."
"Fisher Matrices and Confidence Ellipses: A Quick-Start Guide and Software Fisher matrices are used frequently in the analysis of combining cosmological constraints from various data sets. They encode the Gaussian uncertainties of multiple variables. They are simple to use, and I show how to get up and running with them quickly. Python software is also provided. I cover how to obtain confidence ellipses, add datasets, apply priors, marginalize, transform variables, and even calculate your own Fisher matrices. This treatment is not new, but I aim to provide a clear and concise reference guide. I also provide references and links to more sophisticated treatments and software."
Biophysics software for interdisciplinary education and research Biophysics is a subject that is spread over many disciplines and transcends the skills and knowledge of the individual student. This makes it challenging both to teach and to learn. Educational materials are described to aid in teaching undergraduates biophysics in an interdisciplinary manner. Projects have been devised on topics that range from x-ray diffraction to the Hodgkin Huxley equations. They are team-based and encourage collaboration. The projects make extensive use of software written in Python/Scipy which can be modified to explore a large range of possible phenomena. The software can also be used in lectures and in the teaching of more traditional biophysics courses.
"Computation of hyperspherical Bessel functions In this paper we present a fast and accurate numerical algorithm for the computation of hyperspherical Bessel functions of large order and real arguments. For the hyperspherical Bessel functions of closed type, no stable algorithm existed so far due to the lack of a backwards recurrence. We solved this problem by establishing a relation to Gegenbauer polynomials. All our algorithms are written in C and are publicly available at Github [https://github.com/lesgourg/class_public]. A Python wrapper is available upon request."
"An Elementary Proof That Rationally Isometric Quadratic Forms Are Isometric Let $R$ be a valuation ring with fraction field $K$ and $2\in R^\times$. We give an elementary proof of the following known result: Two unimodular quadratic forms over $R$ are isometric over $K$ if and only if they are isometric over $R$. Our proof does not use Witt's Cancelation Theorem and yields an explicit algorithm to construct an isometry over $R$ from a given isometry over $K$. The statement actually holds for hermitian forms over valuated involutary division rings, provided mild assumptions.   A python implementation of the algorithm derived from the proof can be found on the author's home page."
"Using Scripting Languages to Teach Programming Nowadays, scripting programming languages like Python, Perl and Ruby are widely used in system programming, scientific computing, etc. Although solving a particular problem in these languages requires less time, less programming effort, and less concepts to be taught to achieve the desired goal, still they are not used as teaching tools. Therefore, the use of scripting languages as a teaching vehicle for programming course is very promising. On the other hand, GUI programming, when performed with such languages, is easy and rewarding, since one sees the result of her work immediately. Thus, we are sure that scripting languages combined with GUI toolkits will be the next big thing in computer education."
"Which Sustainable Software Practices Do Scientists Find Most Useful? We studied scientists who attended two-day workshops on basic software skills to determine which tools and practices they found most useful. Our pre- and post-workshop surveys showed increases in self-reported familiarity, while our interviews showed that participants found learning Python more useful than learning the Unix shell, that they found pointers to further resources very valuable, and that background material---the ""why"" behind the skills---was also very valuable."
"A full-fledged micromagnetic code in less than 70 lines of NumPy We present a complete micromagnetic finite-difference code in less than 70 lines of Python. The code makes largely use of the NumPy library and computes the exchange field by finite differences and the demagnetization field with a fast convolution algorithm. Since the magnetization in finite-difference micromagnetics is represented by a multi-dimensional array and the NumPy library features a rich interface for this data structure, the presented code is a good starting point for the development of novel algorithms."
"Asymptotic connectivity for the network of RNA secondary structures Given an RNA sequence a, consider the network G = (V;E), where the set V of nodes consists of all secondary structures of a, and whose edge set E consists of all edges connecting two secondary structures whose base pair distance is 1. Define the network connectivity, or expected network degree, as the average number of edges incident to vertices of G. Using algebraic combinatorial methods, we prove that the asymptotic connectivity of length n homopolymer sequences is 0:473418 ? n. This raises the question of what other network properties are characteristic of the network of RNA secondary structures. Programs in Python, C and Mathematica are available at the web site http://bioinformatics.bc.edu/clotelab/ RNAexpNumNbors."
"Non-Metric Space Library Manual This document covers a library for fast similarity (k-NN)search. It describes only search methods and distances (spaces). Details about building, installing, Python bindings can be found online:https://github.com/searchivarius/nmslib/tree/v1.8/. Even though the library contains a variety of exact metric-space access methods, our main focus is on more generic and approximate search methods, in particular, on methods for non-metric spaces. NMSLIB is possibly the first library with a principled support for non-metric space searching."
"An efficient scheme for sampling fast dynamics at a low average data acquisition rate We introduce a temporal scheme for data sampling, based on a variable delay between two successive data acquisitions. The scheme is designed so as to reduce the average data flow rate, while still retaining the information on the data evolution on fast time scales. The practical implementation of the scheme is discussed and demonstrated in light scattering and microscopy experiments that probe the dynamics of colloidal suspensions using CMOS or CCD cameras as detectors."
Blackbox: A procedure for parallel optimization of expensive black-box functions This note provides a description of a procedure that is designed to efficiently optimize expensive black-box functions. It uses the response surface methodology by incorporating radial basis functions as the response model. A simple method based on a Latin hypercube is used for initial sampling. A modified version of CORS algorithm with space rescaling is used for the subsequent sampling. The procedure is able to scale on multicore processors by performing multiple function evaluations in parallel. The source code of the procedure is written in Python.
"Cell-veto Monte Carlo algorithm for long-range systems We present a rigorous efficient event-chain Monte Carlo algorithm for long-range interacting particle systems. Using a cell-veto scheme within the factorized Metropolis algorithm, we compute each single-particle move with a fixed number of operations. For slowly decaying potentials such as Coulomb interactions, screening line charges allow us to take into account periodic boundary conditions. We discuss the performance of the cell-veto Monte Carlo algorithm for general inverse-power-law potentials, and illustrate how it provides a new outlook on one of the prominent bottlenecks in large-scale atomistic Monte Carlo simulations."
"A Computational Framework for Automation of Point Defect Calculations A complete and rigorously validated open-source Python framework to automate point defect calculations using density functional theory has been developed. The framework provides an effective and efficient method for defect structure generation, and creation of simple yet customizable workflows to analyze defect calculations. The package provides the capability to compute widely-accepted correction schemes to overcome finite-size effects, including (1) potential alignment, (2) image-charge correction, and (3) band filling correction to shallow defects. Using Si, ZnO and In$_2$O$_3$ as test examples, we demonstrate the package capabilities and validate the methodology."
"Should I use TensorFlow Google's Machine Learning framework TensorFlow was open-sourced in November 2015 [1] and has since built a growing community around it. TensorFlow is supposed to be flexible for research purposes while also allowing its models to be deployed productively. This work is aimed towards people with experience in Machine Learning considering whether they should use TensorFlow in their environment. Several aspects of the framework important for such a decision are examined, such as the heterogenity, extensibility and its computation graph. A pure Python implementation of linear classification is compared with an implementation utilizing TensorFlow. I also contrast TensorFlow to other popular frameworks with respect to modeling capability, deployment and performance and give a brief description of the current adaption of the framework."
"A Syntactic Neural Model for General-Purpose Code Generation We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches."
"A bootstrap for the number of $\mathbb{F}_{q^r}$-rational points on a curve over $\mathbb{F}_q$ In this note we present a fast algorithm that finds for any $r$ the number $N_r$ of $\mathbb{F}_{q^r}$ rational points on a smooth absolutely irreducible curve $C$ defined over $\mathbb{F}_{q}$ assuming that we know $N_1,\cdots,N_g$, where $g$ is the genus of $C$. The proof of its validity is given in detail and its working are illustrated with several examples. In an Appendix we list the Python function in which we have implemented the algorithm together with other routines used in the examples."
"DATeS: A Highly-Extensible Data Assimilation Testing Suite v1.0 A flexible and highly-extensible data assimilation testing suite, named DATeS, is described in this paper. DATeS aims to offer a unified testing environment that allows researchers to compare different data assimilation methodologies and understand their performance in various settings. The core of DATeS is implemented in Python and takes advantage of its object-oriented capabilities. The main components of the package (the numerical models, the data assimilation algorithms, the linear algebra solvers, and the time discretization routines) are independent of each other, which offers great flexibility to configure data assimilation applications. DATeS can interface easily with large third-party numerical models written in Fortran or in C, and with a plethora of external solvers."
Models of Simply-connected Trivalent $2$-dimensional Stratifolds with an Implementation Code Trivalent $2$-stratifolds are a generalization of $2$-manifolds in that there are disjoint simple closed curves where three sheets meet. We develop operations on their associated labeled graphs that will effectively construct from a single vertex all graphs that represent $1$-connected $2$-stratifolds. We describe an implementation on Python of these operations and other previous results.
"BasisGen: automatic generation of operator bases BasisGen is a Python package for the automatic generation of bases of operators in effective field theories. It accepts any semisimple symmetry group and fields in any of its finite dimensional irreducible representations. It takes into account integration by parts redundancy and, optionally, the use of equations of motion. The implementation is based in well-known methods to generate and decompose representations using roots and weights, which allow for fast calculations, even with large numbers of fields and high-dimensional operators. BasisGen can also be used to do some representation-theoretic operations, such as finding the weight system of an irreducible representation from its highest weight or decomposing a tensor product of representations."
"Computing Extremely Accurate Quantiles Using t-Digests We present on-line algorithms for computing approximations of rank-based statistics that give high accuracy, particularly near the tails of a distribution, with very small sketches. Notably, the method allows a quantile $q$ to be computed with an accuracy relative to $\max(q, 1-q)$ rather than absolute accuracy as with most other methods. This new algorithm is robust with respect to skewed distributions or ordered datasets and allows separately computed summaries to be combined with no loss in accuracy.   An open-source Java implementation of this algorithm is available from the author. Independent implementations in Go and Python are also available."
"pyro: a framework for hydrodynamics explorations and prototyping pyro is a Python-based simulation framework designed for ease of implementation and exploration of hydrodynamics methods. It is built in a object-oriented fashion, allowing for the reuse of the core components and fast prototyping of new methods."
"The power disaggregation algorithms and their applications to demand dispatch We were interested in solving a power disaggregation problem which comes down to estimating the power consumption of each device given the total power consumption of the whole house. We started by looking at the Factorial Hierarchical Dirichlet Process - Hidden Semi-Markov Model. However, the inference method had a complexity which scales withthe number of observations. Thus, we developed an online algorithm based on particle filters. We applied the method to data from Pecan Street https://dataport.cloud/ using Python. We applied the disaggregation algorithm to the control techniques used in Demand Dispatch."
"Performance Analysis of Effective Symbolic Methods for Solving Band Matrix SLAEs This paper presents an experimental performance study of implementations of three symbolic algorithms for solving band matrix systems of linear algebraic equations with heptadiagonal, pentadiagonal, and tridiagonal coefficient matrices. The only assumption on the coefficient matrix in order for the algorithms to be stable is nonsingularity. These algorithms are implemented using the GiNaC library of C++ and the SymPy library of Python, considering five different data storing classes. Performance analysis of the implementations is done using the high-performance computing (HPC) platforms ""HybriLIT"" and ""Avitohol"". The experimental setup and the results from the conducted computations on the individual computer systems are presented and discussed. An analysis of the three algorithms is performed."
"Minimal Dominating Sets in a Tree: Counting, Enumeration, and Extremal Results A tree with $n$ vertices has at most $95^{n/13}$ minimal dominating sets. The growth constant $\lambda = \sqrt[13]{95} \approx 1.4194908$ is best possible. It is obtained in a semi-automatic way as a kind of ""dominant eigenvalue"" of a bilinear operation on sixtuples that is derived from the dynamic-programming recursion for computing the number of minimal dominating sets of a tree. We also derive an output-sensitive algorithm for listing all minimal dominating sets with linear set-up time and linear delay between successive solutions."
"EMTk -- The Emotion Mining Toolkit The Emotion Mining Toolkit (EMTk) is a suite of modules and datasets offering a comprehensive solution for mining sentiment and emotions from technical text contributed by developers on communication channels. The toolkit is written in Java, Python, and R, and is released under the MIT open source license. In this paper, we describe its architecture and the benchmark against the previous, standalone versions of our sentiment analysis tools. Results show large improvements in terms of speed."
"pyLLE: a Fast and User Friendly Lugiato-Lefever Equation Solver We present the development of pyLLE, a freely accessible and cross-platform Lugiato-Lefever equation solver programmed in Python and Julia and optimized for the simulation of microresonator frequency combs. Examples illustrating its operation, the simplicity of use, and performance against other programming language are presented. The documentation of the software can be found at https://gregmoille.github.io/pyLLE/"
"autoboot: A generator of bootstrap equations with global symmetry We introduce autoboot, a Mathematica program which automatically generates mixed-correlator bootstrap equations of an arbitrary number of scalar external operators, given the global symmetry group and the representations of the operators. The output is a Python program which uses Ohtsuki's cboot which in turn uses Simmons-Duffin's sdpb. In an appendix we also discuss a simple technique to significantly reduce the time to run sdpb, which we call hot-starting."
"On Computational Poisson Geometry I: Symbolic Foundations We present a computational toolkit for (local) Poisson-Nijenhuis calculus on manifolds. Our python module $\textsf{PoissonGeometry}$ implements our algorithms, and accompanies this paper. We include two examples of how our methods can be used, one for gauge transformations of Poisson bivectors in dimension 3, and a second one that determines parametric Poisson bivector fields in dimension 4."
"Multi-Gaussian fitting Algorithm to determine multi-band photometry and photometric redshifts of LABOCA and Herschel sources in proto-cluster environments This research focuses on identifying high redshift galaxies from LABOCA(LArge APEX BOlometer CAmera) and SPIRE(The Spectral and Photometric Imaging Receiver) maps towards proto-cluster candidates initially selected from the SPT (South pole telescope) survey. Based on the Multi-Gaussian fitting algorithm, we cross-match all significant LABOCA sources at SPIRE wavelengths based on their coordinates and signal to noise ratio to derive their photometry at 250, 350, 500 and 870 $\mu m$. We use this information to calculate a photometric redshift for SPT sources towards cluster fields. The code was developed in the Python programming environment."
"Multiregeneration for polynomial system solving We demonstrate our implementation of a continuation method as described in \cite{HR2015} for solving polynomials systems. Given a sequence of (multi)homogeneous polynomials, the software ""multiregeneration"" outputs the respective (multi)degree in a wide range of cases and partial multidegree in all others. We use Python for the file processing, while Bertini is needed for the continuation. Moreover, parallelization options and several strategies for solving structured polynomial systems are available."
"RASCAL: Towards automated spectral wavelength calibration Wavelength calibration is a routine and critical part of any spectral work-flow, but many astronomers still resort to matching detected peaks and emission lines by hand. We present RASCAL (RANSAC Assisted Spectral CALibration), a python library for automated wavelength calibration of astronomical spectrographs. RASCAL implements recent state-of-the-art methods for wavelength calibration and requires minimal input from a user. In this paper we discuss the implementation of the library and apply it to real-world calibration spectra."
"Using Cadabra for tensor computations in General Relativity Cadabra is an open access program ideally suited to complex tensor commutations in General Relativity. Tensor expressions are written in LaTeX while an enhanced version of Python is used to control the computations. This tutorial assumes no prior knowledge of Cadabra. It consists of a series of examples covering a range of topics from basic syntax such as declarations, functions, program control, component computations, input and output through to complete computations including a derivation of two of the BSSN equations from the ADM equations. Numerous exercises are included along with complete solutions. All of the source code for the examples, exercises and solutions are available on GitHub."
"A new algebraic and arithmetic framework for interval computations In this paper we propose some very promissing results in interval arithmetics which permit to build well-defined arithmetics including distributivity of multiplication and division according addition and substraction. Thus, it allows to build all algebraic operations and functions on intervals. This will avoid completely the wrapping effects and data dependance. Some simple applications for matrix eigenvalues calculations, inversion of symmetric matrices and finally optimization are exhibited in the object-oriented programming language python."
"-species and the enumeration of k-trees We study the class of graphs known as k-trees through the lens of Joyal's theory of combinatorial species (and an equivariant extension known as '$\Gamma$-species' which incorporates data about 'structural' group actions). This culminates in a system of recursive functional equations giving the generating function for unlabeled k-trees which allows for fast, efficient computation of their numbers. Enumerations up to k = 10 and n = 30 (for a k-tree with (n+k-1) vertices) are included in tables, and Sage code for the general computation is included in an appendix."
"Approaches to Interpreter Composition In this paper, we compose six different Python and Prolog VMs into 4 pairwise compositions: one using C interpreters; one running on the JVM; one using meta-tracing interpreters; and one using a C interpreter and a meta-tracing interpreter. We show that programs that cross the language barrier frequently execute faster in a meta-tracing composition, and that meta-tracing imposes a significantly lower overhead on composed programs relative to mono-language programs."
"Calculation of oscillation probabilities of atmospheric neutrinos using nuCraft NuCraft (nucraft.hepforge.org) is an open-source Python project that calculates neutrino oscillation probabilities for neutrinos from cosmic-ray interactions in the atmosphere for their propagation through Earth. The solution is obtained by numerically solving the Schr\""odinger equation. The code supports arbitrary numbers of neutrino flavors including additional sterile neutrinos, CP violation, arbitrary mass hierarchies, matter effects with a configurable Earth model, and takes into account the production height distribution of neutrinos in the Earth's atmosphere."
"Simulation of the relativistic electron dynamics and acceleration in a linearly-chirped laser pulse Theoretical investigations are presented, and their results are discussed, of the laser acceleration of a single electron by a chirped pulse. Fields of the pulse are modeled by simple plane-wave oscillations and a $\cos^2$ envelope. The dynamics emerge from analytic and numerical solutions to the relativistic Lorentz-Newton equations of motion of the electron in the fields of the pulse. All simulations have been carried out by independent Mathematica and Python codes, with identical results. Configurations of acceleration from a position of rest as well as from injection, axially and sideways, at initial relativistic speeds are studied."
"GraphState - a tool for graph identification and labelling We present python libraries for Feynman graphs manipulation. The key feature of these libraries is usage of generalization of graph representation offered by B. G. Nickel et al. In this approach graph is represented in some unique 'canonical' form that depends only on its combinatorial type. The uniqueness of graph representation gives an efficient way for isomorphism finding, searching for subgraphs and other graph manipulation tasks. Though offered libraries were originally designed for Feynman graphs, they might be useful for more general graph problems."
"Slice Sampling for Probabilistic Programming We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net."
GPU-accelerated micromagnetic simulations using cloud computing Highly-parallel graphics processing units (GPUs) can improve the speed of micromagnetic simulations significantly as compared to conventional computing using central processing units (CPUs). We present a strategy for performing GPU-accelerated micromagnetic simulations by utilizing cost-effective GPU access offered by cloud computing services with an open-source Python-based program for running the MuMax3 micromagnetics code remotely. We analyze the scaling and cost benefits of using cloud computing for micromagnetics.
"Frequency Distribution of Error Messages Which programming error messages are the most common? We investigate this question, motivated by writing error explanations for novices. We consider large data sets in Python and Java that include both syntax and run-time errors. In both data sets, after grouping essentially identical messages, the error message frequencies empirically resemble Zipf-Mandelbrot distributions. We use a maximum-likelihood approach to fit the distribution parameters. This gives one possible way to contrast languages or compilers quantitatively."
"Keep Me Around: Intron Retention Detection and Analysis We present a tool, keep me around (kma), a suite of python scripts and an R package that finds retained introns in RNA-Seq experiments and incorporates biological replicates to reduce the number of false positives when detecting retention events. kma uses the results of existing quantification tools that probabilistically assign multi-mapping reads, thus interfacing easily with transcript quantification pipelines. The data is represented in a convenient, database style format that allows for easy aggregation across introns, genes, samples, and conditions to allow for further exploratory analysis."
"HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest Neighbor Search Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale data processing and analytics, particularly for analyzing multimedia contents which are often of high dimensionality. Instead of using exact NN search, extensive research efforts have been focusing on approximate NN search algorithms. In this work, we present ""HDIdx"", an efficient high-dimensional indexing library for fast approximate NN search, which is open-source and written in Python. It offers a family of state-of-the-art algorithms that convert input high-dimensional vectors into compact binary codes, making them very efficient and scalable for NN search with very low space complexity."
"Technical Report of Participation in Higgs Boson Machine Learning Challenge This report entails the detailed description of the approach and methodologies taken as part of competing in the Higgs Boson Machine Learning Competition hosted by Kaggle Inc. and organized by CERN et al. It briefly describes the theoretical background of the problem and the motivation for taking part in the competition. Furthermore, the various machine learning models and algorithms analyzed and implemented during the 4 month period of participation are discussed and compared. Special attention is paid to the Deep Learning techniques and architectures implemented from scratch using Python and NumPy for this competition."
"GEMFsim: A Stochastic Simulator for the Generalized Epidemic Modeling Framework The recently proposed generalized epidemic modeling framework (GEMF) \cite{sahneh2013generalized} lays the groundwork for systematically constructing a broad spectrum of stochastic spreading processes over complex networks. This article builds an algorithm for exact, continuous-time numerical simulation of GEMF-based processes. Moreover the implementation of this algorithm, GEMFsim, is available in popular scientific programming platforms such as MATLAB, R, Python, and C; GEMFsim facilitates simulating stochastic spreading models that fit in GEMF framework. Using these simulations one can examine the accuracy of mean-field-type approximations that are commonly used for analytical study of spreading processes on complex networks."
"Deep Learning with Eigenvalue Decay Regularizer This paper extends our previous work on regularization of neural networks using Eigenvalue Decay by employing a soft approximation of the dominant eigenvalue in order to enable the calculation of its derivatives in relation to the synaptic weights, and therefore the application of back-propagation, which is a primary demand for deep learning. Moreover, we extend our previous theoretical analysis to deep neural networks and multiclass classification problems. Our method is implemented as an additional regularizer in Keras, a modular neural networks library written in Python, and evaluated in the benchmark data sets Reuters Newswire Topics Classification, IMDB database for binary sentiment classification, MNIST database of handwritten digits and CIFAR-10 data set for image classification."
"Fuchsia and master integrals for splitting functions from differential equations in QCD We report on the recent progress in reducing differential equations for Feynman master integrals to canonical form with the help of a method proposed by Roman Lee. For the first time, we present Fuchsia --- our open-source implementation of the Lee algorithm written in Python using mathematical routines of a free computer algebra system SageMath. We demonstrate Fuchsia by reducing differential equations for NLO contributions to splitting functions in QCD, which contain both loops and legs integrals."
"PyCells for an Open Semiconductor Industry In the modern semiconductor industry, automatic generation of parameterized and recurring layout structures plays an important role and should be present as a feature in Electronic Design Automation (EDA)-tools. Currently these layout generators are developed with a proprietary programming language and can be used with a specific EDA-tool. Therefore, the semiconductor companies find the development of the layout generators that can be used in all state of the art EDA-tools which support OpenAccess database appealing. The goal of this project is to develop computationally efficient layout generators with Python (PyCells), for ams AG technologies, that possess all the features of comprehensive layout generators."
"GPflow: A Gaussian process library using TensorFlow GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware."
"Extracting an accurate model for permittivity from experimental data : Hunting complex poles from the real line In this letter, we describe a very general procedure to obtain a causal fit of the permittivity of materials from experimental data with very few parameters. Unlike other closed forms proposed in the literature, the particularity of this approach lies in its independence towards the material or frequency range at stake. Many illustrative numerical examples are given and the accuracy of the fitting is compared to other expressions in the literature."
"Gradient Coding We propose a novel coding theoretic framework for mitigating stragglers in distributed learning. We show how carefully replicating data blocks and coding across gradients can provide tolerance to failures and stragglers for Synchronous Gradient Descent. We implement our schemes in python (using MPI) to run on Amazon EC2, and show how we compare against baseline approaches in running time and generalization error."
"TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning TF.Learn is a high-level Python module for distributed machine learning inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to simplify the process of creating, configuring, training, evaluating, and experimenting a machine learning model. TF.Learn integrates a wide range of state-of-art machine learning algorithms built on top of TensorFlow's low level APIs for small to large-scale supervised and unsupervised problems. This module focuses on bringing machine learning to non-specialists using a general-purpose high-level language as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment. Emphasis is put on ease of use, performance, documentation, and API consistency."
Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data Dimensionality reduction and matrix factorization techniques are important and useful machine-learning techniques in many fields. Nonnegative matrix factorization (NMF) is particularly useful for spectral analysis and image processing in astronomy. I present the vectorized update rules and an independent proof of their convergence for NMF with heteroscedastic measurements and missing data. I release a Python implementation of the rules and use an optical spectroscopic dataset of extragalactic sources as an example for demonstration. A future paper will present results of applying the technique to image processing of planetary disks.
"General Algorithmic Search In this paper we present a metaheuristic for global optimization called General Algorithmic Search (GAS). Specifically, GAS is a stochastic, single-objective method that evolves a swarm of agents in search of a global extremum. Numerical simulations with a sample of 31 test functions show that GAS outperforms Basin Hopping, Cuckoo Search, and Differential Evolution, especially in concurrent optimization, i.e., when several runs with different initial settings are executed and the first best wins. Python codes of all algorithms and complementary information are available online."
"NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017."
"PICsar2D: Public Release PICsar2D is a 2.5D relativistic, electromagnetic, particle in cell code designed for studying the pulsar magnetosphere. The source code and a suite of Python analysis routines can be downloaded from ""https://github.com/astromb/PICsar2D.git"". Additionally, the repository includes a step-by-step tutorial for compiling the code, running it, and analyzing the output. This article is devoted to several new algorithmic advances and numerical experiments. These include a new pair injection prescription at the pulsar surface, a comparison of different pair injection techniques, a discussion of particle trajectories near the pulsar Y-line, and performance optimization of the code."
"Mathematical aspect of the combinatorial game ""Mahjong"" We illustrate how one can use basic combinatorial theory and computer programming technique (Python) to analyze the combinatorial game: Mahjong. The results confirm some folklore concerning the game, and expose some unexpected results. Related results and possible future research in connection to artificial intelligence are mentioned. Readers interested in the subject may further develop the techniques to deepen the study of the game, or study other combinatorial games."
Example Setups of Navier-Stokes Equations with Control and Observation: Spatial Discretization and Representation via Linear-quadratic Matrix Coefficients We provide spatial discretizations of nonlinear incompressible Navier-Stokes equations with inputs and outputs in the form of matrices ready to use in any numerical linear algebra package. We discuss the assembling of the system operators and the realization of boundary conditions and inputs and outputs. We describe the two benchmark problems - the driven cavity and the cylinder wake - and provide the corresponding data. The use of the data is illustrated by numerous example setups. The test cases are provided as plain PYTHON or OCTAVE/MATLAB script files for immediate replication.
"Shenfun -- automating the spectral Galerkin method With the shenfun Python module (github.com/spectralDNS/shenfun) an effort is made towards automating the implementation of the spectral Galerkin method for simple tensor product domains, consisting of (currently) one non-periodic and any number of periodic directions. The user interface to shenfun is intentionally made very similar to FEniCS (fenicsproject.org). Partial Differential Equations are represented through weak variational forms and solved using efficient direct solvers where available. MPI decomposition is achieved through the {mpi4py-fft} module (bitbucket.org/mpi4py/mpi4py-fft), and all developed solver may, with no additional effort, be run on supercomputers using thousands of processors. Complete solvers are shown for the linear Poisson and biharmonic problems, as well as the nonlinear and time-dependent Ginzburg-Landau equation."
"Augmentor: An Image Augmentation Library for Machine Learning The generation of artificial data based on existing observations, known as data augmentation, is a technique used in machine learning to improve model accuracy, generalisation, and to control overfitting. Augmentor is a software package, available in both Python and Julia versions, that provides a high level API for the expansion of image data using a stochastic, pipeline-based approach which effectively allows for images to be sampled from a distribution of augmented images at runtime. Augmentor provides methods for most standard augmentation practices as well as several advanced features such as label-preserving, randomised elastic distortions, and provides many helper functions for typical augmentation tasks used in machine learning."
A fundamental frequency estimation method for tonal sounds inspired on bird song studies A fast implementation of fundamental frequency estimation is presented in this work. The algorithm is based on a frequency-domain approach. It was mainly develop for tonal sounds and used in Canary bird song analysis. The method was implemented but not restricted for this kind of data. It could be easily adapted for other proposes. Python libraries were used to develop a code with a simple algorithm to obtain fundamental frequency. A simple open source code is provided in the local university repository.
"GPflowOpt: A Bayesian Optimization Library using TensorFlow A novel Python framework for Bayesian optimization known as GPflowOpt is introduced. The package is based on the popular GPflow library for Gaussian processes, leveraging the benefits of TensorFlow including automatic differentiation, parallelization and GPU computations for Bayesian optimization. Design goals focus on a framework that is easy to extend with custom acquisition functions and models. The framework is thoroughly tested and well documented, and provides scalability. The current released version of GPflowOpt includes some standard single-objective acquisition functions, the state-of-the-art max-value entropy search, as well as a Bayesian multi-objective approach. Finally, it permits easy use of custom modeling strategies implemented in GPflow."
"Rasa: Open Source Language Understanding and Dialogue Management We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/"
"BoltzTraP2, a program for interpolating band structures and calculating semi-classical transport coefficients BoltzTraP2 is a software package for calculating a smoothed Fourier expression of periodic functions and the Onsager transport coefficients for extended systems using the linearized Boltzmann transport equation. It uses only the band and $k$-dependent quasi-particle energies, as well as the intra-band optical matrix elements and scattering rates, as input. The code can be used via a command-line interface and/or as a Python module. It is tested and illustrated on a simple parabolic band example as well as silicon. The positive Seebeck coefficient of lithium is reproduced in an example of going beyond the constant relaxation time approximation."
"Feature Extraction and Feature Selection: Reducing Data Complexity with Apache Spark Feature extraction and feature selection are the first tasks in pre-processing of input logs in order to detect cyber security threats and attacks while utilizing machine learning. When it comes to the analysis of heterogeneous data derived from different sources, these tasks are found to be time-consuming and difficult to be managed efficiently. In this paper, we present an approach for handling feature extraction and feature selection for security analytics of heterogeneous data derived from different network sensors. The approach is implemented in Apache Spark, using its python API, named pyspark."
"DeepMind Control Suite The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at https://www.github.com/deepmind/dm_control . A video summary of all tasks is available at http://youtu.be/rAai4QzcYbs ."
"Selective review of offline change point detection methods This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures."
"CALLISTO Spectrometer at IISER-Pune A CALLISTO spectrometer to monitor solar radio transient emissions from $\approx 0.8-1.6~R_{\odot}$ (above photosphere) is installed at IISER, Pune, India (longitude $73^{\circ} 55'$ E and latitude $18^{\circ}31'$ N). In this paper, we illustrate the instrumental details (log-periodic dipole antenna and the receiver system) along with the recorded solar radio bursts and radio frequency interferences produced by the thunderstorms in the frequency range 45-870 MHz. We also developed the image processing pipelines using `sunpy' and in-house developed python library called `pycallisto'."
"Evolution of the propagator matrix method and its implementation in seismology In this paper, we review development of an algorithm that is referred to in seismology as the Haskell matrix method, the Thomson-Haskell matrix method, or the propagator matrix method. The roots of this algorithm and main developments are examined to offer a better understanding of its essential features. The underlying theory is highlighted by removing specific expressions and manipulations that often shroud the common method involved. Also, I discuss implementations of the algorithm in Python, with a reference to source code. These implementations calculate dispersion curves for guided waves."
"SpeechPy - A Library for Speech Processing and Recognition SpeechPy is an open source Python package that contains speech preprocessing techniques, speech features, and important post-processing operations. It provides most frequent used speech features including MFCCs and filterbank energies alongside with the log-energy of filter-banks. The aim of the package is to provide researchers with a simple tool for speech feature extraction and processing purposes in applications such as Automatic Speech Recognition and Speaker Verification."
"Fast Implementation of a Bayesian Unsupervised Segmentation Algorithm In a recent paper, we have proposed an unsupervised algorithm for audio signal segmentation entirely based on Bayesian methods. In its first implementation, however, the method showed poor computational performance. In this paper we address this question by describing a fast parallel implementation using the Cython library for Python; we use open GSL methods for standard mathematical functions, and the OpenMP framework for parallelization. We also offer a detailed analysis on the sensibility of the algorithm to its different parameters, and show its application to real-life subacquatic signals obtained off the brazilian South coast. Our code and data are available freely on github."
"Approximation of quantum control correction scheme using deep neural networks We study the functional relationship between quantum control pulses in the idealized case and the pulses in the presence of an unwanted drift. We show that a class of artificial neural networks called LSTM is able to model this functional relationship with high efficiency, and hence the correction scheme required to counterbalance the effect of the drift. Our solution allows studying the mapping from quantum control pulses to system dynamics and then analysing the robustness of the latter against local variations in the control profile."
Simulation of microphonic effects in high $Q_L$ TESLA cavities during CW operations This document describes a new package to compute high performance simulations of a module of superconducting accelerating cavities from the LLRF controller perspective. The reason to make a dedicated C++/Python package is to simulate all the effects that arise during Continuous Wave (CW) operations at different timescales to speed-up the LLRF controller design. In particular the speed of the sampling rate of the ADCs used in a LLRF control system (some MHz) are $10^4$ - $10^5$ times faster than typical mechanical resonances and microphonics frequencies.
"Von Neumann regularity, split epicness and elementary cellular automata We show that a cellular automaton on a mixing subshift of finite type is a Von Neumann regular element in the semigroup of cellular automata if and only if it is split epic onto its image in the category of sofic shifts and block maps. It follows from [S.-T\""orm\""a, 2015] that Von Neumann regularity is decidable condition, and we decide it for all elementary CA."
"A Channel-based Exact Inference Algorithm for Bayesian Networks This paper describes a new algorithm for exact Bayesian inference that is based on a recently proposed compositional semantics of Bayesian networks in terms of channels. The paper concentrates on the ideas behind this algorithm, involving a linearisation (`stretching') of the Bayesian network, followed by a combination of forward state transformation and backward predicate transformation, while evidence is accumulated along the way. The performance of a prototype implementation of the algorithm in Python is briefly compared to a standard implementation (pgmpy): first results show competitive performance."
"Open Astronomy Catalogs API We announce the public release of the application program interface (API) for the Open Astronomy Catalogs (OACs), the OACAPI. The OACs serve near-complete collections of supernova, tidal disruption, kilonova, and fast stars data (including photometry, spectra, radio, and X-ray observations) via a user-friendly web interface that displays the data interactively and offers full data downloads. The OACAPI, by contrast, enables users to specifically download particular pieces of the OAC dataset via a flexible programmatic syntax, either via URL GET requests, or via a module within the astroquery Python package."
"Sesame: a 2-dimensional solar cell modeling tool This work introduces a new software package `Sesame' for the numerical computation of classical semiconductor equations. It supports 1 and 2-dimensional systems and provides tools to easily implement extended defects such as grain boundaries or sample surfaces. Sesame has been designed to facilitate fast exploration of the system parameter space and to visualize local charge transport properties. Sesame is distributed as a Python package or as a standalone GUI application, and is available at https://pages.nist.gov/sesame/ ."
"An Evolving Solar Data Environment The rapid growth of solar data is driving changes in the typical workflow and algorithmic approach to solar data analysis. We present recently deployed tools to aid this evolution and layout the path for future development. The majority of space-based datasets including those from the multi-petabyte Solar Dynamics Observatory and the Hinode and Interface Region Imaging Spectrograph (IRIS) missions are made available to the community through a common API with support in IDL (via SolarSoft), Python/SunPy and other emerging languages. Stellar astronomers may find the IRIS data particularly useful for research into stellar chromospheres and for interpreting UV spectra."
An algebraic semi-automated proof of the fundamental identity of Jordan algebras The fundamental identity of quadratic Jordan algebras $Q_{Q_a b} = Q_aQ_bQ_a$ is commonly proven as a consequence of MacDonalds theorem or using more analytic methods. In this short note we give a self-contained purely algebraic proof using just a few easily proven identities and a Python script that follows a simple randomised logic to reduce expressions of Jordan operators.
"UniParse: A universal graph-based parsing toolkit This paper describes the design and use of the graph-based parsing framework and toolkit UniParse, released as an open-source python software package. UniParse as a framework novelly streamlines research prototyping, development and evaluation of graph-based dependency parsing architectures. UniParse does this by enabling highly efficient, sufficiently independent, easily readable, and easily extensible implementations for all dependency parser components. We distribute the toolkit with ready-made configurations as re-implementations of all current state-of-the-art first-order graph-based parsers, including even more efficient Cython implementations of both encoders and decoders, as well as the required specialised loss functions."
"Scikit-Multiflow: A Multi-output Streaming Framework Scikit-multiflow is a multi-output/multi-label and stream data mining framework for the Python programming language. Conceived to serve as a platform to encourage democratization of stream learning research, it provides multiple state of the art methods for stream learning, stream generators and evaluators. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles and quality is enforced by complying with PEP8 guidelines and using continuous integration and automatic testing. The source code is publicly available at https://github.com/scikit-multiflow/scikit-multiflow."
"GuiTeNet: A graphical user interface for tensor networks We introduce a graphical user interface for constructing arbitrary tensor networks and specifying common operations like contractions or splitting, denoted GuiTeNet. Tensors are represented as nodes with attached legs, corresponding to the ordered dimensions of the tensor. GuiTeNet visualizes the current network, and instantly generates Python/NumPy source code for the hitherto sequence of user actions. Support for additional programming languages is planned for the future. We discuss the elementary operations on tensor networks used by GuiTeNet, together with high-level optimization strategies. The software runs directly in web browsers and is available online at http://guitenet.org."
"DarkCapPy: Dark Matter Capture and Annihilation DarkCapPy is a Python 3/Jupyter package for calculating rates associated with dark matter capture in the Earth, annihilation into light mediators, and the subsequent observable decay of the light mediators near the surface of the Earth. The package includes a calculation of the Sommerfeld enhancement at the center of the Earth and the timescale for capture--annihilation equilibrium. The code is open source and can be modified for other compact astronomical objects and mediator spins."
OptFROG - Analytic signal spectrograms with optimized time-frequency resolution A Python package for the calculation of spectrograms with optimized time and frequency resolution for application in the analysis of numerical simulations on ultrashort pulse propagation is presented. Gabor's uncertainty principle prevents both resolutions from being optimal simultaneously for a given window function employed in the underlying short-time Fourier analysis. Our aim is to yield a time-frequency representation of the input signal with marginals that represent the original intensities per unit time and frequency similarly well. As use-case we demonstrate the implemented functionality for the analysis of simulations on ultrashort pulse propagation in a nonlinear waveguide.
"MiTMoJCo: Microscopic Tunneling Model for Josephson Contacts MiTMoJCo (Microscopic Tunneling Model for Josephson Contacts) is C code which aims to assist modeling of superconducting Josephson contacts based on the microscopic tunneling theory. The code offers implementation of a computationally demanding part of this calculation, that is evaluation of superconducting pair and quasiparticle tunnel currents from the given tunnel current amplitudes (TCAs) which characterize the junction material. MiTMoJCo comes with a library of pre-calculated TCAs for frequently used Nb-AlOx-Nb and Nb-AlN-NbN junctions, a Python module for developing custom TCAs, supplementary optimum filtration module for extraction of a constant component of a sinusoidal signal and examples of modeling few common cases of superconducting Josephson contacts."
"Real Time System for Facial Analysis In this paper we describe the anatomy of a real-time facial analysis system. The system recognizes the age, gender and facial expression from users in appearing in front of the camera. All components are based on convolutional neural networks, whose accuracy we study on commonly used training and evaluation sets. A key contribution of the work is the description of the interplay between processing threads for frame grabbing, face detection and the three types of recognition. The python code for executing the system uses common libraries--keras/tensorflow, opencv and dlib--and is available for download."
"Inter-Rater: Software for analysis of inter-rater reliability by permutating pairs of multiple users Inter-Rater quantifies the reliability between multiple raters who evaluate a group of subjects. It calculates the group quantity, Fleiss kappa, and it improves on existing software by keeping information about each user and quantifying how each user agreed with the rest of the group. This is accomplished through permutations of user pairs. The software was written in Python, can be run in Linux, and the code is deposited in Zenodo and GitHub. This software can be used for evaluation of inter-rater reliability in systematic reviews, medical diagnosis algorithms, education applications, and others."
"Generating Comments From Source Code with CCGs Good comments help developers understand software faster and provide better maintenance. However, comments are often missing, generally inaccurate, or out of date. Many of these problems can be avoided by automatic comment generation. This paper presents a method to generate informative comments directly from the source code using general-purpose techniques from natural language processing. We generate comments using an existing natural language model that couples words with their individual logical meaning and grammar rules, allowing comment generation to proceed by search from declarative descriptions of program text. We evaluate our algorithm on several classic algorithms implemented in Python."
"Pyro: Deep Universal Probabilistic Programming Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs."
"Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package Vector space embedding models like word2vec, GloVe, fastText, and ELMo are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings. Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups."
"Bayesim: a tool for adaptive grid model fitting with Bayesian inference Bayesian inference is a widely used and powerful analytical technique in fields such as astronomy and particle physics but has historically been underutilized in some other disciplines including semiconductor devices. In this work, we introduce Bayesim, a Python package that utilizes adaptive grid sampling to efficiently generate a probability distribution over multiple input parameters to a forward model using a collection of experimental measurements. We discuss the implementation choices made in the code, showcase two examples in photovoltaics, and discuss general prerequisites for the approach to apply to other systems."
Monte Carlo event generator for the production and decay of string resonances in proton-proton collisions We describe a Monte Carlo event generator for the production and decay of first and second string resonances through 2 $\rightarrow$ 2 partonic and also 2-parton $\rightarrow$ $\gamma$-parton scatterings in proton-proton collisions - STRINGS version 1.00. This generator is also capable of producing QCD diparton processes. STRINGS is written in Python 2 and can be interfaced to common hadronization programs using the Les Houches Accord.
"Beyond Pham's algorithm for joint diagonalization The approximate joint diagonalization of a set of matrices consists in finding a basis in which these matrices are as diagonal as possible. This problem naturally appears in several statistical learning tasks such as blind signal separation. We consider the diagonalization criterion studied in a seminal paper by Pham (2001), and propose a new quasi-Newton method for its optimization. Through numerical experiments on simulated and real datasets, we show that the proposed method outper-forms Pham's algorithm. An open source Python package is released."
"A generic coordinate descent solver for nonsmooth convex optimization We present a generic coordinate descent solver for the minimization of a nonsmooth convex objective with structure. The method can deal in particular with problems with linear constraints. The implementation makes use of efficient residual updates and automatically determines which dual variables should be duplicated. A list of basic functional atoms is pre-compiled for efficiency and a modelling language in Python allows the user to combine them at run time. So, the algorithm can be used to solve a large variety of problems including Lasso, sparse multinomial logistic regression, linear and quadratic programs."
"Compression with wildcards: Abstract simplicial complexes Despite the more handy terminology of abstract simplicial complexes SC, in its core this article is about antitone Boolean functions. Given the maximal faces (=facets) of SC, our main algorithm, called Facets-To-Faces, outputs SC in a compressed format. The degree of compression of Facets-To-Faces, which is programmed in high-level Mathematica code, compares favorably to both the Mathematica command BooleanConvert, and to the BDD's provided by Python. A novel way to calculate the face-numbers from the facets is also presented. Both algorithms can be parallelized and are applicable (e.g.) to reliability analysis, combinatorial topology, and frequent set mining."
"Robust Deep Gaussian Processes This report provides an in-depth overview over the implications and novelty Generalized Variational Inference (GVI) (Knoblauch et al., 2019) brings to Deep Gaussian Processes (DGPs) (Damianou & Lawrence, 2013). Specifically, robustness to model misspecification as well as principled alternatives for uncertainty quantification are motivated with an information-geometric view. These modifications have clear interpretations and can be implemented in less than 100 lines of Python code. Most importantly, the corresponding empirical results show that DGPs can greatly benefit from the presented enhancements."
"Multi-Level Mesa Multi-level Mesa is an extension to support the Python based Agents Based Model (ABM) library Mesa. Multi-level Mesa provides ABM infrastructure to allow for the inclusion of complex networks, which have modules (groups) and hierarchies (layers) of agents. This approach allows for users to define and simulate multi-layered adaptions of complex networks. This study reviews other multi-level libraries currently in the field, describes the main functions and classes of the Multi-level Mesa, and describes its implementation and impact in numerous varieties using the seminal ABM - Sugarscape. Multi-level Mesa and Sugarscape examples are available on GitHub at https://github.com/tpike3/multilevel_mesa and https://github.com/tpike3/SugarScape."
"Quadcubic interpolation: a four-dimensional spline method We present a local interpolation method in four dimensions utilising cubic splines. An extension of the three-dimensional tricubic method, the interpolated function has C$^1$ continuity and its partial derivatives are analytically accessible. The specific example of application of this work to a time-varying three-dimensional magnetic field is given, but this method would work equally well for a time-independent four-dimensional field. Implementations of both of these methods in the Python programming language are also available to download."
"GaborNet: Gabor filters with learnable parameters in deep convolutional neural networks The article describes a system for image recognition using deep convolutional neural networks. Modified network architecture is proposed that focuses on improving convergence and reducing training complexity. The filters in the first layer of the network are constrained to fit the Gabor function. The parameters of Gabor functions are learnable and are updated by standard backpropagation techniques. The system was implemented on Python, tested on several datasets and outperformed the common convolutional networks."
"The Scikit-HEP Project The Scikit-HEP project is a community-driven and community-oriented effort with the aim of providing Particle Physics at large with a Python scientific toolset containing core and common tools. The project builds on five pillars that embrace the major topics involved in a physicist's analysis work: datasets, data aggregations, modelling, simulation and visualisation. The vision is to build a user and developer community engaging collaboration across experiments, to emulate scikit-learn's unified interface with Astropy's embrace of third-party packages, and to improve discoverability of relevant tools."
"Reduced Ideals in Pure Cubic Fields Reduced ideals have been defined in the context of integer rings in quadratic number fields, and they are closely tied to the continued fraction algorithm. The notion of this type of ideal extends naturally to number fields of higher degree. In the case of pure cubic fields, generated by cube roots of integers, a convenient integral basis provides a means for identifying reduced ideals in these fields. We define integer sequences whose terms are in correspondence with some of these ideals, suggesting a generalization of continued fractions."
"The Largest Contained Quadrilateral and the Smallest Enclosing Parallelogram of a Convex Polygon We present a linear-time algorithm for finding the quadrilateral of largest area contained in a convex polygon, and we show that it is closely related to an old algorithm for the smallest enclosing parallelogram of a convex polygon."
"A Beginner's Guide to Working with Astronomical Data This elementary review covers the basics of working with astronomical data, notably with images, spectra and higher-level (catalog) data. The basic concepts and tools are presented using both application software (DS9 and TOPCAT) and Python. The level of presentation is suitable for undergraduate students, but should also be accessible to advanced high school students."
"Neural network-based anomaly detection for high-resolution X-ray spectroscopy We propose an anomaly detection technique for high-resolution X-ray spectroscopy. The method is based on the neural network architecture variational autoencoder, and requires only {\it normal} samples for training. We implement the network using Python taking account of the effect of Poisson statistics carefully, and deonstrate the concept with simulated high-resolution X-ray spectral datasets of one-temperature, two-temperature and non-equilibrium plasma. Our proposed technique would assist scientists in finding important information that would otherwise be missed due to the unmanageable amount of data taken with future X-ray observatories."
"MUSICNTWRK: data tools for music theory, analysis and composition We present the API for MUSICNTWRK, a python library for pitch class set and rhythmic sequences classification and manipulation, the generation of networks in generalized music and sound spaces, deep learning algorithms for timbre recognition, and the sonification of arbitrary data. The software is freely available under GPL 3.0 and can be downloaded at www.musicntwrk.com or installed as a PyPi project (pip install musicntwrk)."
"Visual Backpropagation We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem."
"PyRep: Bringing V-REP to Deep Robot Learning PyRep is a toolkit for robot learning research, built on top of the virtual robotics experimentation platform (V-REP). Through a series of modifications and additions, we have created a tailored version of V-REP built with robot learning in mind. The new PyRep toolkit offers three improvements: (1) a simple and flexible API for robot control and scene manipulation, (2) a new rendering engine, and (3) speed boosts upwards of 10,000x in comparison to the previous Python Remote API. With these improvements, we believe PyRep is the ideal toolkit to facilitate rapid prototyping of learning algorithms in the areas of reinforcement learning, imitation learning, state estimation, mapping, and computer vision."
"Toward a Procedural Fruit Tree Rendering Framework for Image Analysis We propose a procedural fruit tree rendering framework, based on Blender and Python scripts allowing to generate quickly labeled dataset (i.e. including ground truth semantic segmentation). It is designed to train image analysis deep learning methods (e.g. in a robotic fruit harvesting context), where real labeled training datasets are usually scarce and existing synthetic ones are too specialized. Moreover, the framework includes the possibility to introduce parametrized variations in the model (e.g. lightning conditions, background), producing a dataset with embedded Domain Randomization aspect."
"A new approach (extra vertex) and generalization of Shoelace Algorithm usage in convex polygon (Point-in-Polygon) In this paper we aim to bring new approach into usage of Shoelace Algorithm for area calculation in convex polygons on Cartesian coordinate system, with concentration on point in polygon concept. Generalization of usage of the concept will be proposed for line segment and polygons. Testing of new method will be done using Python language. Results of tests show that the new approach is more effective than the current one."
"The Nested_fit data analysis program We present here Nested_fit, a Bayesian data analysis code developed for investigations of atomic spectra and other physical data. It is based on the nested sampling algorithm with the implementation of an upgraded lawn mower robot method for finding new live points. For a given data set and a chosen model, the program provides the Bayesian evidence, for the comparison of different hypotheses/models, and the different parameter probability distributions. A large database of spectral profiles is already available (Gaussian, Lorentz, Voigt, Log-normal, etc.) and additional ones can easily added. It is written in Fortran, for an optimized parallel computation, and it is accompanied by a Python library for the results visualization."
"BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and HyperBand, and a comprehensive analysis of the optimization process and its outcomes."
"Extensive Online Shock Model Database We present a new database of fully radiative shock models calculated with the shock and photoionization code MAPPINGS V. The database architecture is built to contain diverse shock grids comprising of multiple shock parameters. It can be easily accessible through the MySQL protocol. Intensities of spectral lines from infrared to X-rays are stored along with other useful outputs such as the ionic fractions/temperature, integrated densities, etc. A web page was created in other to explore interactively the database as it evolves with time. Examples of its usage is given using the Python language."
"ICurry FlatCurry is a well-established intermediate representation of Curry programs used in compilers that translate Curry code into Prolog and Haskell code. Some FlatCurry constructs have no direct translation into imperative code. These constructs must be each handled differently when translating Curry code into C, C++ and Python code. We introduce a new representation of Curry programs, called ICurry, and derive a translation from all FlatCurry constructs into ICurry. We present the syntax of ICurry and the translation from FlatCurry to ICurry. We present a model of functional logic computations as graph rewriting, show how this model can be implemented in a low-level imperative language, and describe the translation from ICurry to this model."
"sktime: A Unified Interface for Machine Learning with Time Series We present sktime -- a new scikit-learn compatible Python library with a unified interface for machine learning with time series. Time series data gives rise to various distinct but closely related learning tasks, such as forecasting and time series classification, many of which can be solved by reducing them to related simpler tasks. We discuss the main rationale for creating a unified interface, including reduction, as well as the design of sktime's core API, supported by a clear overview of common time series tasks and reduction approaches."
"Movie Recommender Systems: Implementation and Performance Evaluation Over the years, explosive growth in the number of items in the catalog of e-commerce businesses, such as Amazon, Netflix, Pandora, etc., have warranted the development of recommender systems to guide consumers towards their desired products based on their preferences and tastes. Some of the popular approaches for building recommender systems, for mining user, derived input datasets, are: content-based systems, collaborative filtering, latent-factor systems using Singular Value Decomposition (SVD), and Restricted Boltzmann Machines (RBM). In this project, user-user collaborative filtering, item-item collaborative filtering, content-based recommendation, SVD, and neural networks were chosen for implementation in Python to predict the user ratings of unwatched movies for each user, and their performances were evaluated and compared."
"ZMCintegral-v5: Support for Integrations with the Scanning of Large Parameter Grids on Multi-GPUs In this updated vesion of ZMCintegral, we have added the functionality of integrations with parameter scan on distributed Graphics Processing Units(GPUs). Given a large parameter grid (up to 10^{10} parameter points to be scanned), the code will evaluate integrations for each parameter grid value. To ensure the evaluation speed, this new functionality employs a direct Monte Carlo method for the integraion. The Python API is kept the same as the previous ones and users have a full flexibility to define their own integrands. The performance of this new functionality is tested for both one node and multi-nodes conditions."
"Work Stealing Simulator We present in this paper a Work Stealing lightweight PYTHON simulator. Our simulator is used to execute an application (list of tasks with or without dependencies), on a multiple processors platform linked by specific topology. We first give an overview of the different variants of the work stealing algorithm, then we present the architecture of our light Work Stealing simulator. Its architecture facilitates the development of other types of applications and other topologies for interconnecting the processors. We present the use cases of the simulator and the different types of results."
"Introducing an Explicit Symplectic Integration Scheme for Riemannian Manifold Hamiltonian Monte Carlo We introduce a recent symplectic integration scheme derived for solving physically motivated systems with non-separable Hamiltonians. We show its relevance to Riemannian manifold Hamiltonian Monte Carlo (RMHMC) and provide an alternative to the currently used generalised leapfrog symplectic integrator, which relies on solving multiple fixed point iterations to convergence. Via this approach, we are able to reduce the number of higher-order derivative calculations per leapfrog step. We explore the implications of this integrator and demonstrate its efficacy in reducing the computational burden of RMHMC. Our code is provided in a new open-source Python package, hamiltorch."
"Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW."
"Proximal Adam: Robust Adaptive Update Scheme for Constrained Optimization We implement the adaptive step size scheme from the optimization methods AdaGrad and Adam in a novel variant of the Proximal Gradient Method (PGM). Our algorithm, dubbed AdaProx, avoids the need for explicit computation of the Lipschitz constants or additional line searches and thus reduces per-iteration cost. In test cases for Constrained Matrix Factorization we demonstrate the advantages of AdaProx in fidelity and performance over PGM, while still allowing for arbitrary penalty functions. The python implementation of the algorithm presented here is available as an open-source package at https://github.com/pmelchior/proxmin."
"Detect Toxic Content to Improve Online Conversations Social media is filled with toxic content. The aim of this paper is to build a model that can detect insincere questions. We use the 'Quora Insincere Questions Classification' dataset for our analysis. The dataset is composed of sincere and insincere questions, with the majority of sincere questions. The dataset is processed and analyzed using Python and its libraries such as sklearn, numpy, pandas, keras etc. The dataset is converted to vector form using word embeddings such as GloVe, Wiki-news and TF-IDF. The imbalance in the dataset is handled by resampling techniques. We train and compare various machine learning and deep learning models to come up with the best results. Models discussed include SVM, Naive Bayes, GRU and LSTM."
"pyannote.audio: neural building blocks for speaker diarization We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them."
"TensorTrace: an application to contract tensor networks Tensor network methods are a conceptually elegant framework for encoding complicated datasets, where high-order tensors are approximated as networks of low-order tensors. In practice, however, the numeric implementation of tensor network algorithms is often a labor-intensive and error-prone task, even for experienced researchers in this area. \emph{TensorTrace} is application designed to alleviate the burden of contracting tensor networks: it provides a graphic drawing interface specifically tailored for the construction of tensor network diagrams, from which the code for their optimal contraction can then be automatically generated (in the users choice of the MATLAB, Python or Julia languages). \emph{TensorTrace} is freely available at \url{https://www.tensortrace.com} with versions for Windows, Mac and Ubuntu."
"Transfer Learning Toolkit: Primers and Benchmarks The transfer learning toolkit wraps the codes of 17 transfer learning models and provides integrated interfaces, allowing users to use those models by calling a simple function. It is easy for primary researchers to use this toolkit and to choose proper models for real-world applications. The toolkit is written in Python and distributed under MIT open source license. In this paper, the current state of this toolkit is described and the necessary environment setting and usage are introduced."
